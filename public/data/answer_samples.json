[
  {
    "site": "math",
    "question_title": "Norms Induced by Inner Products and the Parallelogram Law",
    "question_body": "Let $ V $ be a normed vector space (over $\\mathbb{R}$, say, for simplicity) with norm $ \\lVert\\cdot\\rVert$.\nIt's not hard to show that if $\\lVert \\cdot \\rVert = \\sqrt{\\langle \\cdot, \\cdot \\rangle}$ for some (real) inner product $\\langle \\cdot, \\cdot \\rangle$, then the parallelogram equality\n$$ 2\\lVert u\\rVert^2 + 2\\lVert v\\rVert^2 = \\lVert u + v\\rVert^2 + \\lVert u - v\\rVert^2 $$\nholds for all pairs $u, v \\in V$.\nI'm having difficulty with the converse.  Assuming the parallelogram identity, I'm able to convince myself that the inner product should be\n$$ \\langle u, v \\rangle = \\frac{\\lVert u\\rVert^2 + \\lVert v\\rVert^2 - \\lVert u - v\\rVert^2}{2} = \\frac{\\lVert u + v\\rVert^2 - \\lVert u\\rVert^2 - \\lVert v\\rVert^2}{2} = \\frac{\\lVert u + v\\rVert^2 - \\lVert u - v\\rVert^2}{4} $$\nI cannot seem to get that $\\langle \\lambda u,v \\rangle = \\lambda \\langle u,v \\rangle$ for $\\lambda \\in \\mathbb{R}$.  How would one go about proving this?\n",
    "answer_body": "Since this question is asked often enough, let me add a detailed solution. I'm not quite following Arturo's outline, though. The main difference is that I'm not re-proving the Cauchy-Schwarz inequality (Step 4 in Arturo's outline) but rather use the fact that multiplication by scalars and addition of vectors as well as the norm are continuous, which is a bit easier to prove.\nSo, assume that the norm $\\|\\cdot\\|$ satisfies the parallelogram law\n$$2 \\Vert x \\Vert^2 + 2\\Vert y \\Vert^2 = \\Vert x + y \\Vert^2 + \\Vert x - y \\Vert^2$$\nfor all $x,y \\in V$ and put\n$$\\langle x, y \\rangle = \\frac{1}{4} \\left( \\Vert x + y \\Vert^2 - \\Vert x - y \\Vert^2\\right).$$ We're dealing with real vector spaces and defer the treatment of the complex case to Step 4 below. \nStep 0. $\\langle x, y \\rangle = \\langle y, x\\rangle$ and $\\Vert x \\Vert = \\sqrt{\\langle x, x\\rangle}$.\nObvious.\nStep 1. The function $(x,y) \\mapsto \\langle x,y \\rangle$ is continuous with respect to $\\Vert \\cdot \\Vert$.\nContinuity with respect to the norm $\\Vert \\cdot\\Vert$ follows from the fact that addition and negation are $\\Vert \\cdot \\Vert$-continuous, that the norm itself is continuous and that sums and compositions of continuous functions are continuous.\nRemark. This continuity property of the (putative) scalar product will only be used at the very end of step 3. Until then the solution consists of purely algebraic steps.\nStep 2. We have $\\langle x + y, z \\rangle = \\langle x, z \\rangle + \\langle y, z\\rangle$.\nBy the parallelogram law we have\n$$2\\Vert x + z \\Vert^2 + 2\\Vert y \\Vert^2  = \\Vert x + y + z \\Vert^2 + \\Vert x - y + z\\Vert^2 .$$\nThis gives \n$$\\begin{align*}\n\\Vert x + y + z \\Vert^2 & = 2\\Vert x + z \\Vert^2 + 2\\Vert y \\Vert^2 - \\Vert x - y + z \\Vert^2 \\\\\n& = 2\\Vert y + z \\Vert^2 + 2\\Vert x \\Vert^2 - \\Vert y - x + z \\Vert^2\n\\end{align*}$$\nwhere the second formula follows from the first by exchanging $x$ and $y$. Since $A = B$ and $A = C$ imply $A = \\frac{1}{2} (B + C)$ we get\n$$\\Vert x + y + z \\Vert^2 = \\Vert x \\Vert^2 + \\Vert y \\Vert^2 + \\Vert x + z \\Vert^2 + \\Vert y + z \\Vert^2 - \\frac{1}{2}\\Vert x - y + z \\Vert^2 - \\frac{1}{2}\\Vert y - x + z \\Vert^2.$$\nReplacing $z$ by $-z$ in the last equation gives\n$$\\Vert x + y - z \\Vert^2 = \\Vert x \\Vert^2 + \\Vert y \\Vert^2 + \\Vert x - z \\Vert^2 + \\Vert y - z \\Vert^2 - \\frac{1}{2}\\Vert x - y - z \\Vert^2 - \\frac{1}{2}\\Vert y - x - z \\Vert^2.$$\nApplying $\\Vert w \\Vert = \\Vert - w\\Vert$ to the two negative terms in the last  equation we get\n$$\\begin{align*}\\langle x + y, z \\rangle & = \\frac{1}{4}\\left(\\Vert x + y + z \\Vert^2 - \\Vert x + y - z \\Vert^2\\right) \\\\\n& = \\frac{1}{4}\\left(\\Vert x + z \\Vert^2 - \\Vert x - z \\Vert^2\\right) + \n\\frac{1}{4}\\left(\\Vert y + z \\Vert^2 - \\Vert y - z \\Vert^2\\right) \\\\\n& = \\langle x, z \\rangle + \\langle y, z \\rangle\n\\end{align*}$$\nas desired.\nStep 3. $\\langle \\lambda x, y \\rangle = \\lambda \\langle x, y \\rangle$ for all $\\lambda \\in \\mathbb{R}$.\nThis clearly holds for $\\lambda = -1$ and by step 2 and induction we have $\\langle \\lambda x, y \\rangle = \\lambda \\langle x, y \\rangle$ for all $\\lambda \\in \\mathbb{N}$, thus for all $\\lambda \\in \\mathbb{Z}$. If $\\lambda = \\frac{p}{q}$ with $p,q \\in \\mathbb{Z}, q \\neq 0$ we get with $x' = \\dfrac{x}{q}$ that\n$$q \\langle \\lambda x, y \\rangle = q\\langle p x', y \\rangle = p \\langle q x', y \\rangle = p\\langle x,y \\rangle,$$\nso dividing this by $q$ gives\n$$\\langle \\lambda x , y \\rangle = \\lambda \\langle x, y \\rangle \\qquad\\text{for all } \\lambda \\in \\mathbb{Q}.$$\nWe have just seen that for fixed $x,y$ the continuous function $\\displaystyle t \\mapsto \\frac{1}{t} \\langle t x,y \\rangle$ defined on $\\mathbb{R} \\smallsetminus \\{0\\}$ is equal to $\\langle x,y \\rangle$ for all $t \\in \\mathbb{Q} \\smallsetminus \\{0\\}$, thus equality holds for all $t \\in \\mathbb{R} \\smallsetminus \\{0\\}$. The case $\\lambda = 0$ being trivial, we're done.\nStep 4. The complex case.\nDefine $\\displaystyle \\langle x, y \\rangle =\\frac{1}{4} \\sum_{k =0}^{3} i^{k} \\Vert x +i^k y\\Vert^2$, observe that $\\langle ix,y \\rangle = i \\langle x, y \\rangle$ and $\\langle x, y \\rangle = \\overline{\\langle y, x \\rangle}$ and apply the case of real scalars twice (to the real and imaginary parts of $\\langle \\cdot, \\cdot \\rangle$).\nAddendum. In fact we can weaken requirements of Jordan von Neumann theorem to\n$$\n2\\Vert x\\Vert^2+2\\Vert y\\Vert^2\\leq\\Vert x+y\\Vert^2+\\Vert x-y\\Vert^2\n$$\nIndeed after substitution $x\\to\\frac{1}{2}(x+y)$, $y\\to\\frac{1}{2}(x-y)$ and simplifications we get\n$$\n\\Vert x+y\\Vert^2+\\Vert x-y\\Vert^2\\leq 2\\Vert x\\Vert^2+2\\Vert y\\Vert^2\n$$\nwhich together with previous inequality gives the equality.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Since this question is asked often enough, let me add a detailed solution."
      },
      {
        "type": "Comparison",
        "text": "I'm not quite following Arturo's outline, though. The main difference is that I'm not re-proving the Cauchy-Schwarz inequality (Step 4 in Arturo's outline) but rather use the fact that multiplication by scalars and addition of vectors as well as the norm are continuous, which is a bit easier to prove."
      },
      {
        "type": "Assumption",
        "text": "So, assume that the norm $\\|\\cdot\\|$ satisfies the parallelogram law..."
      },
      {
        "type": "Procedure",
        "text": "Step 0. $\\langle x, y \\rangle = \\langle y, x\\rangle$ and $\\Vert x \\Vert = \\sqrt{\\langle x, x\\rangle}.\\nObvious."
      },
      {
        "type": "Elaboration",
        "text": "Step 1. The function $(x,y) \\mapsto \\langle x,y \\rangle$ is continuous with respect to $\\Vert \\cdot \\Vert$..."
      },
      {
        "type": "Example",
        "text": "Step 2. We have $\\langle x + y, z \\rangle = \\langle x, z \\rangle + \\langle y, z\\rangle$..."
      },
      {
        "type": "Elaboration",
        "text": "Step 3. $\\langle \\lambda x, y \\rangle = \\lambda \\langle x, y \\rangle$ for all $\\lambda \\in \\mathbb{R$..."
      },
      {
        "type": "Elaboration",
        "text": "Step 4. The complex case..."
      },
      {
        "type": "Elaboration",
        "text": "Addendum. In fact we can weaken requirements of Jordan von Neumann theorem..."
      }
    ]
  },
  {
    "site": "math",
    "question_title": "In (relatively) simple words: What is an inverse limit?",
    "question_body": "I am a set theorist in my orientation, and while I did take a few courses that brushed upon categorical and algebraic constructions, one has always eluded me.\nThe inverse limit. I tried to ask one of the guys in my office, and despite a very shady explanation he ended up muttering that \"you usually take an already known construction.\"\nThe Wikipedia article presents two approaches, the algebraic and the categorical. While the categorical is extremely vague for me, the algebraic one is too general and the intuition remains hidden underneath the text in a place I cannot find it.\nSince I am not too familiar with categories, the explanation most people would try to give me which is categorical in nature seems to confuse me - as I keep asking this question over and over every now and then.\nCould anyone explain to me in non-categorical terms what is the idea behind an inverse limit? (I am roughly familiar with its friend \"direct limit\", if that helps)\n(While editing, I can say that the answers given so far are very interesting, and I have read them thoroughly, although I need to give it quite some thinking before I can comment on all of them right now.)\n",
    "answer_body": "I like George Bergman's explanation (beginning in section 7.4 of his Invitation to General Algebra and Universal Constructions).\nWe start with a motivating example. \nSuppose you are interested in solving $x^2=-1$ in $\\mathbb{Z}$. Of course, there are no solutions, but let's ignore that annoying reality for a moment.\nWe use the notation $\\mathbb{Z}_n$ for $\\mathbb Z / n \\mathbb Z$.\nThe equation has a solution in the ring $\\mathbb{Z}_5$ (in fact, two: both $2$ and $3$, which are the same up to sign). So we want to find a solution to $x^2=-1$ in $\\mathbb{Z}$ which satisfies $x\\equiv 2 \\pmod{5}$. \nAn integer that is congruent to $2$ modulo $5$ is of the form $5y+2$, so we can rewrite our original equation as $(5y+2)^2 = -1$, and expand to get\n$25y^2 + 20y = -5$.\nThat means $20y\\equiv -5\\pmod{25}$, or $4y\\equiv -1\\pmod{5}$, which has the unique solution $y\\equiv 1\\pmod{5}$. Substituting back we determine $x$ modulo $25$:\n$$x = 5y+2 \\equiv 5\\cdot 1 + 2  = 7 \\pmod{25}.$$\nContinue this way: putting $x=25z+7$ into $x^2=-1$ we conclude $z\\equiv 2 \\pmod{5}$, so $x\\equiv 57\\pmod{125}$. \nUsing Hensel's Lemma, we can continue this indefinitely. What we deduce is that there is a sequence of residues, \n$$x_1\\in\\mathbb{Z}_5,\\quad x_2\\in\\mathbb{Z}_{25},\\quad \\ldots, x_{i}\\in\\mathbb{Z}_{5^i},\\ldots$$\neach of which satisfies $x^2=-1$ in the appropriate ring, and which are \"consistent\", in the sense that each $x_{i+1}$ is a lifting of $x_i$ under the natural homomorphisms\n$$\\cdots \\stackrel{f_{i+1}}{\\longrightarrow} \\mathbb{Z}_{5^{i+1}} \\stackrel{f_i}{\\longrightarrow} \\mathbb{Z}_{5^i} \\stackrel{f_{i-1}}{\\longrightarrow}\\cdots\\stackrel{f_2}{\\longrightarrow} \\mathbb{Z}_{5^2}\\stackrel{f_1}{\\longrightarrow} \\mathbb{Z}_5.$$\nTake the set of all strings $(\\ldots,x_i,\\ldots,x_2,x_1)$ such that $x_i\\in\\mathbb{Z}_{5^i}$ and $f_i(x_{i+1}) = x_i$, $i=1,2,\\ldots$. This is a ring under componentwise operations. What we did above shows that in this ring, you do have a square root of $-1$. \n\nAdded. Bergman here inserts the quote, \"If the fool will persist in his folly, he will become wise.\" We obtained the sequence by stubbornly looking for a solution to an equation that has no solution, by looking at putative approximations, first modulo 5, then modulo 25, then modulo 125, etc. We foolishly kept going even though there was no solution to be found. In the end, we get a \"full description\" of what that object must look like; since we don't have a ready-made object that satisfies this condition, then we simply take this \"full description\" and use that description as if it were an object itself. By insisting in our folly of looking for a solution, we have become wise by introducing an entirely new object that is a solution.\nThis is much along the lines of taking a Cauchy sequence of rationals, which \"describes\" a limit point, and using the entire Cauchy sequence to represent this limit point, even if that limit point does not exist in our original set. \n\nThis ring is the $5$-adic integers; since an integer is completely determined by its remainders modulo the powers of $5$, this ring contains an isomorphic copy of $\\mathbb{Z}$.\nEssentially, we are taking successive approximations to a putative answer to the original equation, by first solving it modulo $5$, then solving it modulo $25$ in a way that is consistent with our solution modulo $5$; then solving it modulo $125$ in a way that is consistent with out solution modulo $25$, etc.\nThe ring of $5$-adic integers projects onto each $\\mathbb{Z}_{5^i}$ via the projections; because the elements of the $5$-adic integers are consistent sequences, these projections commute with our original maps $f_i$. So the projections are compatible with the $f_i$ in the sense that for all $i$, $f_i\\circ\\pi_{i+1} = \\pi_{i}$, where $\\pi_k$ is the projection onto the $k$th coordinate from the $5$-adics.\nMoreover, the ring of $5$-adic integers is universal for this property: given any ring $R$ with homomorphisms $r_i\\colon R\\to\\mathbb{Z}_{5^i}$ such that $f_i\\circ r_{i+1} = r_i$, for any $a\\in R$ the tuple of images $(\\ldots, r_i(a),\\ldots, r_2(a),r_1(a))$ defines an element in the $5$-adics. The $5$-adics are the inverse limit of the system of maps\n$$\\cdots\\stackrel{f_{i+1}}{\\longrightarrow}\\mathbb{Z}_{5^{i+1}}\\stackrel{f_i}{\\longrightarrow}\\mathbb{Z}_{5^i}\\stackrel{f_{i-1}}{\\longrightarrow}\\cdots\\stackrel{f_2}{\\longrightarrow}\\mathbb{Z}_{5^2}\\stackrel{f_1}{\\longrightarrow}\\mathbb{Z}_5.$$\nSo the elements of the inverse limit are \"consistent sequences\" of partial approximations, and the inverse limit is a way of taking all these \"partial approximations\" and combine them into a \"target object.\"\nMore generally, assume that you have a system of, say, rings, $\\{R_i\\}$, indexed by an directed set $(I,\\leq)$ (so that for all $i,j\\in I$ there exists $k\\in I$ such that $i,j\\leq k$), and a system of maps $f_{rs}\\colon R_s\\to R_r$ whenever $r\\leq s$ which are \"consistent\" (if $r\\leq s\\leq t$, then $f_{rs}\\circ f_{st} = f_{rt}$), and let's assume that the $f_{rs}$ are surjective, as they were in the example of the $5$-adics. Then you can think of the $R_i$ as being \"successive approximations\" (with a higher indexed $R_i$ as being a \"finer\" or \"better\" approximation than the lower indexed one). The directedness of the index set guarantees that given any two approximations, even if they are not directly comparable to one another, you can combine them into an approximation which is finer (better) than each of them (if $i,j$ are incomparable, then find a $k$ with $i,j\\leq k$). The inverse limit is a way to combine all of these approximations into an object in a consistent manner.\nIf you imagine your maps as going right to left, you have a branching tree that is getting \"thinner\" as you move left, and the inverse limit is the combination of all branches occurring \"at infinity\". \n\nAdded. The example of the $p$-adic integers may be a bit misleading because our directed set is totally ordered and all maps are surjective. In the more general case, you can think of every chain in the directed set as a \"line of approximation\"; the directed property ensures that any finite number of \"lines of approximation\" will meet in \"finite time\", but you may need to go all the way to \"infinity\" to really put all the lines of approximation together. The inverse limit takes care of this. \nIf the directed set has no maximal elements, but the structure maps are not surjective, it turns out that no element that is not in the image will matter; essentially, that element never shows up in a net of \"successive approximations\", so it never forms part of a \"consistent system of approximations\" (which is what the elements of the inverse limit are). \n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Citation",
        "text": "I like George Bergman's explanation (beginning in section 7.4 of his Invitation to General Algebra and Universal Constructions)."
      },
      {
        "type": "Example",
        "text": "We start with a motivating example. Suppose you are interested in solving $x^2=-1$ in $\\mathbb{Z}$. Of course, there are no solutions, but let's ignore that annoying reality for a moment."
      },
      {
        "type": "Procedure",
        "text": "We use the notation $\\mathbb{Z}_n$ for $\\mathbb Z / n \\mathbb Z$. The equation has a solution in the ring $\\mathbb{Z}_5$ (in fact, two: both $2$ and $3$, which are the same up to sign). So we want to find a solution to $x^2=-1$ in $\\mathbb{Z}$ which satisfies $x\\equiv 2 \\pmod{5}."
      },
      {
        "type": "Elaboration",
        "text": "An integer that is congruent to $2$ modulo $5$ is of the form $5y+2$, so we can rewrite our original equation as $(5y+2)^2 = -1$, and expand to get $25y^2 + 20y = -5$."
      },
      {
        "type": "Procedure",
        "text": "That means $20y\\equiv -5\\pmod{25}$, or $4y\\equiv -1\\pmod{5}$, which has the unique solution $y\\equiv 1\\pmod{5}$. Substituting back we determine $x$ modulo $25$: $$x = 5y+2 \\equiv 5\\cdot 1 + 2  = 7 \\pmod{25}.$$ Continue this way..."
      },
      {
        "type": "Elaboration",
        "text": "Using Hensel's Lemma, we can continue this indefinitely. What we deduce is that there is a sequence of residues..."
      },
      {
        "type": "Story",
        "text": "Added. Bergman here inserts the quote, 'If the fool will persist in his folly, he will become wise.' We obtained the sequence by stubbornly looking for a solution to an equation that has no solution..."
      },
      {
        "type": "Elaboration",
        "text": "This ring is the $5$-adic integers; since an integer is completely determined by its remainders modulo the powers of $5$, this ring contains an isomorphic copy of $\\mathbb{Z$."
      },
      {
        "type": "Implication",
        "text": "Essentially, we are taking successive approximations to a putative answer to the original equation, by first solving it modulo $5$, then solving it modulo $25$ in a way that is consistent with our solution modulo $5$..."
      },
      {
        "type": "Elaboration",
        "text": "The ring of $5$-adic integers projects onto each $\\mathbb{Z}_{5^i}$ via the projections; because the elements of the $5$-adic integers are consistent sequences, these projections commute with our original maps $f_i$."
      },
      {
        "type": "Implication",
        "text": "Moreover, the ring of $5$-adic integers is universal for this property: given any ring $R$ with homomorphisms $r_i\\colon R\\to\\mathbb{Z}_{5^i}$ such that $f_i\\circ r_{i+1} = r_i$, for any $a\\in R$..."
      },
      {
        "type": "Elaboration",
        "text": "More generally, assume that you have a system of, say, rings, $\\{R_i\\}$, indexed by an directed set $(I,\\leq)$..."
      },
      {
        "type": "Elaboration",
        "text": "If you imagine your maps as going right to left, you have a branching tree that is getting 'thinner' as you move left, and the inverse limit is the combination of all branches occurring 'at infinity'."
      },
      {
        "type": "Story",
        "text": "Added. The example of the $p$-adic integers may be a bit misleading because our directed set is totally ordered and all maps are surjective..."
      }
    ]
  },
  {
    "site": "math",
    "question_title": "Why do we care about dual spaces?",
    "question_body": "When I first took linear algebra, we never learned about dual spaces. Today in lecture we discussed them and I understand what they are, but I don't really understand why we want to study them within linear algebra.\nI was wondering if anyone knew a nice intuitive motivation for the study of dual spaces and whether or not they \"show up\" as often as other concepts in linear algebra? Is their usefulness something that just becomes more apparent as you learn more math and see them arise in different settings?\n\nEdit\nI understand that dual spaces show up in functional analysis and multilinear algebra, but I still don't really understand the intuition/motivation behind their definition in the standard topics covered in a linear algebra course. (Hopefully, this clarifies my question)\n",
    "answer_body": "Let $V$ be a vector space (over any field, but we can take it to be $\\mathbb R$ if you like,\nand for concreteness I will take the field to be $\\mathbb R$ from now on;\neverything is just as interesting in that case).  Certainly one of the interesting concepts\nin linear algebra is that of a hyperplane in $V$.\nFor example, if $V = \\mathbb R^n$, then a hyperplane is just the solution set to an equation\nof the form\n$$a_1 x_1 + \\cdots + a_n x_n = b,$$\nfor some $a_i$ not all zero and some $b$.\nRecall that solving such equations (or simultaneous sets of such equations) is one\nof the basic motivations for developing linear algebra.\nNow remember that when a vector space is not given to you as $\\mathbb R^n$,\nit doesn't normally have a canonical basis, so we don't have a canonical way\nto write its elements down via coordinates, and so we can't describe hyperplanes\nby explicit equations like above.  (Or better, we can, but only after choosing\ncoordinates, and this is not canonical.)\nHow can we canonically describe hyperplanes in $V$?\nFor this we need a conceptual interpretation of the above equation.  And here linear\nfunctionals come to the rescue.  More precisely, the map\n$$\\begin{align*}\n\\ell: \\mathbb{R}^n &\\rightarrow \\mathbb{R} \\\\\n(x_1,\\ldots,x_n) &\\mapsto a_1 x_1 + \\cdots + a_n x_n\n\\end{align*}$$\nis a linear functional on $\\mathbb R^n$, and so the above equation for the\nhyperplane can be written as\n$$\\ell(v) = b,$$\nwhere $v = (x_1,\\ldots,x_n).$\nMore generally, if $V$ is any vector space, and $\\ell: V \\to \\mathbb R$ is any\nnon-zero linear functional (i.e. non-zero element of the dual space), then\nfor any $b \\in \\mathbb R,$ the set\n$$\\{v \\, | \\, \\ell(v) = b\\}$$\nis a hyperplane in $V$, and all hyperplanes in $V$ arise this way.\nSo this gives a reasonable justification for introducing the elements of the dual\nspace to $V$; they generalize the notion of linear equation in several variables\nfrom the case of $\\mathbb R^n$ to the case of an arbitrary vector space.\nNow you might ask: why do we make them a vector space themselves?  Why do we want\nto add them to one another, or multiply them by scalars?\nThere are lots of reasons for this; here is one: Remember how important it is,\nwhen you solve systems of linear equations, to add equations together, or\nto multiply them by scalars (here I am referring to all the steps you typically\nmake when performing Gaussian elimination on a collection of simultaneous linear\nequations)?  Well, under the dictionary above between linear equations\nand linear functionals, these processes correspond precisely to adding together\nlinear functionals, or multiplying them by scalars.  If you ponder this for a bit,\nyou can hopefully convince yourself that making the set of linear\nfunctionals a vector space is a pretty natural thing to do.\nSummary: just as concrete vectors $(x_1,\\ldots,x_n) \\in \\mathbb R^n$ are naturally\ngeneralized to elements of vector spaces, concrete linear expressions\n$a_1 x_1 + \\ldots + a_n x_n$ in $x_1,\\ldots, x_n$ are naturally generalized to linear functionals.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Let $V$ be a vector space (over any field, but we can take it to be $\\mathbb R$ if you like, and for concreteness I will take the field to be $\\mathbb R$ from now on; everything is just as interesting in that case). Certainly one of the interesting concepts in linear algebra is that of a hyperplane in $V$."
      },
      {
        "type": "Example",
        "text": "For example, if $V = \\mathbb R^n$, then a hyperplane is just the solution set to an equation of the form $a_1 x_1 + \\cdots + a_n x_n = b$, for some $a_i$ not all zero and some $b$."
      },
      {
        "type": "Context",
        "text": "Recall that solving such equations (or simultaneous sets of such equations) is one of the basic motivations for developing linear algebra."
      },
      {
        "type": "Elaboration",
        "text": "Now remember that when a vector space is not given to you as $\\mathbb R^n$, it doesn't normally have a canonical basis, so we don't have a canonical way to write its elements down via coordinates, and so we can't describe hyperplanes by explicit equations like above. (Or better, we can, but only after choosing coordinates, and this is not canonical.)"
      },
      {
        "type": "Procedure",
        "text": "How can we canonically describe hyperplanes in $V? For this we need a conceptual interpretation of the above equation. And here linear functionals come to the rescue."
      },
      {
        "type": "Elaboration",
        "text": "More generally, if $V$ is any vector space, and $\\ell: V \\to \\mathbb R$ is any non-zero linear functional (i.e. non-zero element of the dual space), then for any $b \\in \\mathbb R$, the set $\\{v \\, | \\, \\ell(v) = b\\}$ is a hyperplane in $V, and all hyperplanes in $V$ arise this way."
      },
      {
        "type": "Implication",
        "text": "So this gives a reasonable justification for introducing the elements of the dual space to $V; they generalize the notion of linear equation in several variables from the case of $\\mathbb R^n$ to the case of an arbitrary vector space."
      },
      {
        "type": "Question",
        "text": "Now you might ask: why do we make them a vector space themselves? Why do we want to add them to one another, or multiply them by scalars?"
      },
      {
        "type": "Elaboration",
        "text": "There are lots of reasons for this; here is one: Remember how important it is, when you solve systems of linear equations, to add equations together, or to multiply them by scalars (here I am referring to all the steps you typically make when performing Gaussian elimination on a collection of simultaneous linear equations)? Well, under the dictionary above between linear equations and linear functionals, these processes correspond precisely to adding together linear functionals, or multiplying them by scalars."
      },
      {
        "type": "Implication",
        "text": "If you ponder this for a bit, you can hopefully convince yourself that making the set of linear functionals a vector space is a pretty natural thing to do."
      },
      {
        "type": "Summary",
        "text": "Summary: just as concrete vectors $(x_1,\\ldots,x_n) \\in \\mathbb R^n$ are naturally generalized to elements of vector spaces, concrete linear expressions $a_1 x_1 + \\ldots + a_n x_n$ in $x_1,\\ldots, x_n$ are naturally generalized to linear functionals."
      }
    ]
  },
  {
    "site": "math",
    "question_title": "Why are rings called rings?",
    "question_body": "I've done some search in Internet and other sources about this question. Why the name ring to this particular object? Just curiosity.\nThanks.\n",
    "answer_body": "The name \"ring\" is derived from Hilbert's term \"Zahlring\" (number ring), introduced in his Zahlbericht for certain rings of algebraic integers. As for why Hilbert chose the name \"ring\", I recall reading speculations that it may have to do with cyclical (ring-shaped) behavior of powers of algebraic integers. Namely, if $\\:\\alpha\\:$ is an algebraic integer of degree $\\rm\\:n\\:$  then $\\:\\alpha^n\\:$ is a $\\rm\\:\\mathbb Z$-linear combination of lower powers of $\\rm\\:\\alpha\\:,\\:$ thus so too are all higher powers of $\\rm\\:\\alpha\\:.\\:$ Hence all powers cycle back onto  $\\rm\\:1,\\:\\alpha,\\:,\\ldots,\\alpha^{n-1}\\:,\\:$ i.e. $\\rm\\:\\mathbb Z[\\alpha]\\:$ is a finitely generated $\\:\\mathbb Z$-module. Possibly also the motivation for the name had to do more specifically with rings of cyclotomic integers. However, as plausible as that may seem, I don't recall the existence of any historical documents that provide solid evidence in support of such speculations.\nBeware that one has to be very careful when reading such older literature. Some authors mistakenly read modern notions into terms which have no such denotation in their original usage. To provide some context I recommend reading Lemmermeyer and Schappacher's Introduction to the English Edition of Hilbert\u2019s Zahlbericht. Below is a pertinent excerpt.\n\nBelow is an excerpt from Leo Corry's Modern algebra and the rise of mathematical structures, p. 149.\n\n\n\nBelow are a couple typical examples of said speculative etymology of the term \"ring\" via the \"circling  back\" nature of integral dependence, from Harvey Cohn's Advanced Number Theory, p. 49.\n\n$\\quad$The designation of the letter $\\mathfrak D$ for the integral domain has some historical importance going back to Gauss's work on quadratic forms. Gauss $\\left(1800\\right)$ noted that for certain quadratic forms $Ax^2+Bxy+Cy^2$ the discriminant need not be square-free, although $A$, $B$, $C$ are relatively prime. For example, $x^2-45y^2$ has $D=4\\cdot45$. The $4$ was ignored for the reason that $4|D$ necessarily by virtue of Gauss's requirement that $B$ be even, but the factor of $3^2$ in $D$ caused Gauss to refer to the form as one of \"order $3$.\" Eventually, the forms corresponding to a value of $D$ were called an \"order\" (Ordnung). Dedekind retained this word for what is here called an \"integral domain.\"\n$\\quad$The term \"ring\" is a contraction of \"Zahlring\" introduced by Hilbert $\\left(1892\\right)$ to denote (in our present context) the ring generated by the rational integers and a quadratic integer $\\eta$ defined by $$\\eta^2+B\\eta+C=0.$$ It would seem that module $\\left[1,\\eta\\right]$ is called a Zahlring because $\\eta^2$ equals $-B\\eta-C$ \"circling directly back\" to an element of $\\left[1,\\eta\\right]$ . This word has been maintained today. Incidentally, every Zahlring is an integral domain and the converse is true for quadratic fields.\n\nand from Rotman's Advanced Modern Algebra, p. 81.\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "The name 'ring' is derived from Hilbert's term 'Zahlring' (number ring), introduced in his Zahlbericht for certain rings of algebraic integers."
      },
      {
        "type": "Elaboration",
        "text": "As for why Hilbert chose the name 'ring', I recall reading speculations that it may have to do with cyclical (ring-shaped) behavior of powers of algebraic integers."
      },
      {
        "type": "Implication",
        "text": "Namely, if \u03b1 is an algebraic integer of degree n then \u03b1^n is a Z-linear combination of lower powers of \u03b1, thus so too are all higher powers of \u03b1."
      },
      {
        "type": "Implication",
        "text": "Hence all powers cycle back onto 1, \u03b1, ..., \u03b1^(n-1), i.e., Z[\u03b1] is a finitely generated Z-module."
      },
      {
        "type": "Qualification",
        "text": "Possibly also the motivation for the name had to do more specifically with rings of cyclotomic integers."
      },
      {
        "type": "Caveat",
        "text": "However, as plausible as that may seem, I don't recall the existence of any historical documents that provide solid evidence in support of such speculations."
      },
      {
        "type": "Caveat",
        "text": "Beware that one has to be very careful when reading such older literature. Some authors mistakenly read modern notions into terms which have no such denotation in their original usage."
      },
      {
        "type": "Context",
        "text": "To provide some context I recommend reading Lemmermeyer and Schappacher's Introduction to the English Edition of Hilbert\u2019s Zahlbericht."
      },
      {
        "type": "Citation",
        "text": "Below is an excerpt from Leo Corry's Modern algebra and the rise of mathematical structures, p. 149."
      },
      {
        "type": "Example",
        "text": "Below are a couple typical examples of said speculative etymology of the term 'ring' via the 'circling back' nature of integral dependence, from Harvey Cohn's Advanced Number Theory, p. 49."
      },
      {
        "type": "Historical Reference",
        "text": "The designation of the letter \ud835\udd3b for the integral domain has some historical importance going back to Gauss's work on quadratic forms."
      },
      {
        "type": "Definition",
        "text": "The term 'ring' is a contraction of 'Zahlring' introduced by Hilbert (1892) to denote (in our present context) the ring generated by the rational integers and a quadratic integer \u03b7 defined by \u03b7^2 + B\u03b7 + C = 0."
      },
      {
        "type": "Elaboration",
        "text": "It would seem that module [1, \u03b7] is called a Zahlring because \u03b7^2 equals -B\u03b7-C 'circling directly back' to an element of [1, \u03b7]."
      },
      {
        "type": "Implication",
        "text": "Incidentally, every Zahlring is an integral domain and the converse is true for quadratic fields."
      },
      {
        "type": "Citation",
        "text": "And from Rotman's Advanced Modern Algebra, p. 81."
      }
    ]
  },
  {
    "site": "math",
    "question_title": "Do complex numbers really exist?",
    "question_body": "Complex numbers involve the square root of negative one, and most non-mathematicians find it hard to accept that such a number is meaningful. In contrast, they feel that real numbers have an obvious and intuitive meaning. What's the best way to explain to a non-mathematician that complex numbers are necessary and meaningful, in the same way that real numbers are?\nThis is not a Platonic question about the reality of mathematics, or whether abstractions are as real as physical entities, but an attempt to bridge a comprehension gap that many people experience when encountering complex numbers for the first time. The wording, although provocative, is deliberately designed to match the way that many people actually ask this question.\n",
    "answer_body": "There are a few good answers to this question, depending on the audience. I've used all of these on occasion.\nA way to solve polynomials\nWe came up with equations like $x - 5 = 0$, what is $x$?, and the naturals solved them (easily). Then we asked, \"wait, what about $x + 5 = 0$?\" So we invented negative numbers. Then we asked \"wait, what about $2x = 1$?\" So we invented rational numbers. Then we asked \"wait, what about $x^2 = 2$?\" so we invented irrational numbers.\nFinally, we asked, \"wait, what about $x^2 = -1$?\" This is the only question that was left, so we decided to invent the \"imaginary\" numbers to solve it. All the other numbers, at some point, didn't exist and didn't seem \"real\", but now they're fine. Now that we have imaginary numbers, we can solve every polynomial, so it makes sense that that's the last place to stop.\nPairs of numbers\nThis explanation goes the route of redefinition. Tell the listener to forget everything he or she knows about imaginary numbers. You're defining a new number system, only now there are always pairs of numbers. Why? For fun. Then go through explaining how addition/multiplication work. Try and find a good \"realistic\" use of pairs of numbers (many exist).\nThen, show that in this system,  $(0,1) * (0,1) = (-1,0)$, in other words, we've defined a new system, under which it makes sense to say that $\\sqrt{-1} = i$, when $i=(0,1)$. And that's really all there is to imaginary numbers: a definition of a new number system, which makes sense to use in most places. And under that system, there is an answer to $\\sqrt{-1}$.\nThe historical explanation\nExplain the history of the imaginary numbers. Showing that mathematicians also fought against them for a long time helps people understand the mathematical process, i.e., that it's all definitions in the end.\nI'm a little rusty, but I think there were certain equations that kept having parts of them which used $\\sqrt{-1}$, and the mathematicians kept  throwing out the equations since there is no such thing.\nThen, one mathematician decided to just \"roll with it\", and kept working, and found out that all those square roots cancelled each other out.\nAmazingly, the answer that was left was the correct answer (he was working on finding roots of polynomials, I think). Which lead him to think that there was a valid reason to use $\\sqrt{-1}$, even if it took a long time to understand it.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "There are a few good answers to this question, depending on the audience. I've used all of these on occasion."
      },
      {
        "type": "Procedure",
        "text": "A way to solve polynomials: We came up with equations like $x - 5 = 0$, what is $x$?, and the naturals solved them (easily). Then we asked, 'wait, what about $x + 5 = 0$?' So we invented negative numbers. Then we asked 'wait, what about $2x = 1$?' So we invented rational numbers. Then we asked 'wait, what about $x^2 = 2$?' so we invented irrational numbers. Finally, we asked, 'wait, what about $x^2 = -1$?' This is the only question that was left, so we decided to invent the 'imaginary' numbers to solve it. All the other numbers, at some point, didn't exist and didn't seem 'real', but now they're fine. Now that we have imaginary numbers, we can solve every polynomial, so it makes sense that that's the last place to stop."
      },
      {
        "type": "Analogy",
        "text": "Pairs of numbers: This explanation goes the route of redefinition. Tell the listener to forget everything he or she knows about imaginary numbers. You're defining a new number system, only now there are always pairs of numbers. Why? For fun. Then go through explaining how addition/multiplication work. Try and find a good 'realistic' use of pairs of numbers (many exist). Then, show that in this system, $(0,1) * (0,1) = (-1,0)$, in other words, we've defined a new system, under which it makes sense to say that $\\sqrt{-1} = i$, when $i=(0,1)'. And that's really all there is to imaginary numbers: a definition of a new number system, which makes sense to use in most places. And under that system, there is an answer to $\\sqrt{-1}$."
      },
      {
        "type": "Historical Reference",
        "text": "The historical explanation: Explain the history of the imaginary numbers. Showing that mathematicians also fought against them for a long time helps people understand the mathematical process, i.e., that it's all definitions in the end. I'm a little rusty, but I think there were certain equations that kept having parts of them which used $\\sqrt{-1}$, and the mathematicians kept throwing out the equations since there is no such thing. Then, one mathematician decided to just 'roll with it', and kept working, and found out that all those square roots cancelled each other out. Amazingly, the answer that was left was the correct answer (he was working on finding roots of polynomials, I think). Which lead him to think that there was a valid reason to use $\\sqrt{-1}$, even if it took a long time to understand it."
      }
    ]
  },
  {
    "site": "philosophy",
    "question_title": "Why does the universe obey scientific laws?",
    "question_body": "As far as anyone is aware, the universe consistently acts according to predictable laws (and scientific inquiry exists to determine those laws). Is there any metaphysical reason for this? Is such a question even answerable?\nEDIT: I think my question was misunderstood, so I'll try to clarify. I know about the mathematics question, but this question is, why is the universe consistent? It's related to the problem of induction: just because all hitherto observed emeralds are green doesn't necessarily mean that all emeralds are green. Yet, those who have hypothesized that emeralds are green have (thus far) been found to be correct. In other words, as far as anyone can tell, the universe is consistent to the point where much of its behavior is predictable using known laws and statistics. Is there any philosophical discussion regarding why that appears to be the case?\nFURTHER EDIT: The question is more fundamental than the simple, 'why are all emeralds green', to which the answer is obviously, 'because if it wasn't green, we wouldn't call it an emerald', and once I formulate the question better, I think that the answer becomes obvious. Let's use an actual law, F=ma. We've checked rocks, we've checked feathers; we've checked slow moving objects and fast moving objects, and yet, lo and behold, the law always seems to be true, and it's stayed true for at least a few hundred years (but we can reasonably assume that it was just as true a millennium ago). Now, I ask the metaphysical (in the most literal sense) question: why is this law always true? Why does the universe behave so consistently?\n",
    "answer_body": "I say it's not that the universe acts according to \"scientific laws,\" but rather that these laws are a tool for people to use to quantify how the universe works.\nIn other words, the universe works how it works.  Not according to any laws or conventions, but because \"that's how it is.\"  The fact that the observable universe is consistent allows us to create scientific laws/theories to describe (and predict) it's behavior, but there is no prescriptive power in these labels.\nEdit: I previously called Scientific Laws outdated terminology.  Turns out I was incorrect.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Claim",
        "text": "I say it's not that the universe acts according to 'scientific laws,' but rather that these laws are a tool for people to use to quantify how the universe works."
      },
      {
        "type": "Elaboration",
        "text": "In other words, the universe works how it works. Not according to any laws or conventions, but because 'that's how it is.'"
      },
      {
        "type": "Implication",
        "text": "The fact that the observable universe is consistent allows us to create scientific laws/theories to describe (and predict) its behavior, but there is no prescriptive power in these labels."
      },
      {
        "type": "Qualification",
        "text": "Edit: I previously called Scientific Laws outdated terminology. Turns out I was incorrect."
      }
    ]
  },
  {
    "site": "philosophy",
    "question_title": "Is atheism just another form of dogma?",
    "question_body": "I was raised Catholic, and part of my motivation for leaving Catholicism was my dislike of any kind of enforced structure. However, to my surprise, it seems like Atheism is also dogmatic. Is this intentional? Is atheism supposed to be only anti-religious or is it more against dogma itself?\n",
    "answer_body": "Nomenclature\nYou don't explain why you get the impression that atheism is dogmatic, but it appears that you mix terms which are not interchangeable.\n\nIf you leave catholicism because of your dislike of any kind of enforced structure, it means you're antireligious and probably anticlericalist.\nYou didn't say anything about your faith, or lack thereof. It's possible to leave catholicism or any other organized religion while still believing in some kind of god, possibly a different one than the one described in your previous religion.\nIf you still have faith, you're a theist. If you think that the existence of god is not known or unknowable, you're agnostic. If you don't believe in god, you're an atheist. From the short description you wrote, it's not possible to determine which category you belong to.\n\nIs atheism dogmatic?\nFirst, atheists aren't an homogenous group. The vast majority of them is silent, sometimes for security reasons. They often have nothing more in common than simply not believing in god. A famous description is:\n\n\"Atheism is a religion like not collecting stamps is a hobby.\"\n\nSome atheists are certain that there isn't any god, some will try to convince theists that they are wrong. They still don't belong to any official institution or blindly follow principles, though.\nSome atheists will try to use science (e.g. Russell's teapot) to \"prove\" that there is no god. This might be dogmatic, but science at least has the advantage of being falsifiable. As noted by @FrankHubeny in the comments, science never considers models to be 100% correct when describing the universe: they are merely \"good enough\" for the time being and probably will be replaced in the future.\nFinally, the \"Church of the Flying Spaghetti Monster\" has been created as a joke, but could be considered as dogmatic for some hardcore followers. By its own definition, though:  \n\n\"the only dogma allowed in the Church of the Flying Spaghetti Monster\n  is the rejection of dogma\"\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": []
  },
  {
    "site": "philosophy",
    "question_title": "Why should one accept trivial claims without evidence?",
    "question_body": "There are certain claims that I accept as obviously true without (much) evidence. For example:\n\nMost people don't like to be hit on the head with a hammer.\nDonald Trump ate dinner some time last week.\nThere has yet to be a whale on the moon.\n\nI don't need empirical evidence, before accepting these claims; but if someone challenged me for some, I could not come up with any (although I could come up with arguments in support of them).\nAlso, these claims are inconsequential. If I'm wrong about such claims, it doesn't appear to be affecting anything in the world. Why?\n",
    "answer_body": "First, because they are \"inconsequential\". Nothing hangs on it for you, there is no need to act on them and accept the consequences also, it is a \"cheap\", easily swayable \"acceptance\". But this still leaves the question as to why accept rather than reject, as easily, perhaps at random. \nWhich brings us to the second because: they are not accepted without much evidence, there is plenty of evidence for them, in fact. It is just not processed into the conclusion consciously. They are inductive surmises from long ordinary experience condensed to a point where the inference made is subconscious. That people dislike being hit comes from observing people's reactions when they are, one's own empathy with the resulting pain, imagining it done to oneself, background knowledge about damage it might cause, etc. Same with people eating dinners, at least once in a while, or background knowledge about the whales and the moon, implied by mere familiarity with what the words mean. When such surmises are reflected upon their origin is colloquially labeled as \"common sense\".\nPeirce thought a lot about what scholastics called logica utens, practical \"implicit\" logic. I'll quote his opinion on another famously accepted \"inconsequential\" claim, the one Descartes accepted so much as to believe it the indubitable foundation of all reasoning, upon which he tries to build chains of inferences leading to equally \"indubitable\" conclusions. It is his cogito, anticipated, to an extent, by Augustine. According to Peirce, \"I think, therefore I am\" is not an indubitable foundation, certain \"in itself\", but an instinctive surmise from a multitude of other less abstract ordinary acceptances. It is their intertwining with it, like strands in a cable, which holds it in place as \"indubitable\":\n\n\"There are, however, cases in which we are conscious\n  that a belief has been determined by another given belief, but are not conscious that it\n  proceeds on any general principle. Such is St. Augustine's \"cogito, ergo sum.\" Such a\n  process should be called, not a reasoning, but an acritical inference.\n[...] Descartes thought this \"tr\u00e8s-clair\"; but it is a fundamental mistake to suppose\n  that an idea which stands isolated can be otherwise than perfectly blind. He professes\n  to doubt the testimony of his memory; and in that case all that is left is a vague\n  indescribable idea. There is no warrant for putting it into the first person singular. \"I\n  think\" begs the question. \"There is an idea: therefore, I am,\" it may be contended\n  represents a compulsion of thought; but it is not a rational compulsion. \n[...] Philosophy ought to... trust rather to the multitude and variety of its arguments than to the\n  conclusiveness of any one. Its reasoning should not form a chain which is no stronger\n  than its weakest link, but a cable whose fibers may be ever so slender, provided they\n  are sufficiently numerous and intimately connected.\" [quoted from Collected Papers of C. S. Peirce]\n\nAnother philosopher who famously reflected on our countless applications of common sense was Moore. In his Defence of Common Sense he gives many other examples of easily accepted truisms, not necessarily inconsequential, now called Moorean certainties:\n\n\"I begin, then, with my list of truisms, every one of which (in my own opinion) I know, with certainty, to be true... There exists at present a living human body, which is my body. This body was born at a certain time in the past, and has existed continuously ever since, though not without undergoing changes; it was, for instance, much smaller when it was born, and for some time afterwards, than it is now. Ever since it was born, it has been either in contact with or not far from the surface of the earth; and, at every moment since it was born, there have also existed many other things, having shape and size in three dimensions... \n[...] Among the things which have, in this sense, formed part of its environment (i.e. have been either in contact with it, or at some distance from it, however great) there have, at every moment since its birth, been large numbers of other living human bodies... and many of these bodies have already died and ceased to exist. But the earth had existed also for many years before my body was born; and for many of these years, also, large numbers of human bodies had, at every moment, been alive upon it; and many of these bodies had died and ceased to exist before it was born\".\n\nLater he distilled it into a famous \"hand argument\":\"\"I can prove now, for instance, that two human hands\nexist. How?. By holding up my two hands, and saying, as I\nmake a certain gesture with the right hand \"Here is one\nhand\" and adding, as I make a certain gesture with the left\n\"and here is another.\"\". Wittgenstein was so taken by Moore's musings that he devoted his last work, On Certainty, to them. And he found some of Moore's truisms to be so consequential as to dub them hinge propositions, around which all other empirical reasoning revolves:\n\n\"That is to say, the questions that we raise and our doubts depend on the fact that some propositions are exempt from doubt, are as it were like hinges on which those turn.\nBut it isn't that the situation is like this: We just can't investigate everything, and for that reason we are forced to rest content with assumption. If I want the door to turn, the hinges must stay put.\" [OC 341,343]\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "The text discusses the acceptance of inconsequential claims and the reasons behind why people tend to accept them."
      },
      {
        "type": "Elaboration",
        "text": "The first reason for accepting inconsequential claims is that they are considered inconsequential, meaning there is no pressure to act on them or face consequences. This makes acceptance easy and swayable."
      },
      {
        "type": "Elaboration",
        "text": "The second reason for accepting inconsequential claims is that there is plenty of evidence for them, although this evidence is not consciously processed into the conclusion. These claims are often inductive surmises from long ordinary experiences, condensed into subconscious inferences."
      },
      {
        "type": "Example",
        "text": "Observing people's reactions to being hit, empathizing with resulting pain, and considering potential damage are examples of how common sense beliefs are formed."
      },
      {
        "type": "Citation",
        "text": "The text references Peirce's thoughts on practical 'implicit' logic and Descartes' acceptance of the cogito as an inconsequential claim."
      },
      {
        "type": "Quote",
        "text": "Peirce argues that 'I think, therefore I am' is not an indubitable foundation but an instinctive surmise from various ordinary acceptances, intertwined to form a seemingly indubitable belief."
      },
      {
        "type": "Comparison",
        "text": "Moore's Defense of Common Sense provides examples of easily accepted truisms, which are now known as Moorean certainties."
      },
      {
        "type": "Example",
        "text": "Moore's 'hand argument' is given as an example of proving the existence of two human hands, illustrating a common sense belief."
      },
      {
        "type": "Citation",
        "text": "Wittgenstein's work 'On Certainty' is mentioned, where he explores Moore's truisms and identifies some as hinge propositions."
      }
    ]
  },
  {
    "site": "philosophy",
    "question_title": "Does humanism's rejection of God necesitate relativism?",
    "question_body": "I had the following discussion on Programmers.SE:\n\n@Peter Turner, Which is a good example of how religion warps morality, leading people to imagine their concerns are moral when they are profoundly immoral. \u2013 TRiG\n@TRiG, Which is a good example of making an appeal to absolute truth where you would normally appeal to relativism. The problem is, you don't listen to our arguments and you assume they're all appeals to something you don't believe in. Whereas, Catholics almost always argue for morality in a non-dogmatic rational way. \u2013 Peter Turner\n@Peter, I don't think I've ever appealed to \"relativism\". I'm familiar with Catholic \"Natural Law\" argument, and I know it's nonsense. Humanism is not relativism. But perhaps discussion on these lines would be better on the philosophy site. \u2013 TRiG\n\nThis discussion made me question whether I was really correct in assuming that humanism's rejection of God necessitates a belief in moral relativism. I found the following definition of humanism, which seems to back up my thinking:\n\nHumanism's exclusion of God necessitates moral relativism. Humanist John Dewey (1859-1952), co-author and signer of the Humanist Manifesto 1 (1933), declared, \"There is no God and there is no soul. Hence, there are no needs for the props of traditional religion. With dogma and creed excluded, then immutable truth is also dead and buried. There is no room for fixed, natural law or moral absolutes.\" Humanists believe one should do, as one feels is right. \n\nIs there any evidence to the contrary? \n",
    "answer_body": "No, humanism does not, in any way, necessitate relativism. They are completely different philosophies. In fact, I'm not sure that I can even imagine a way in which humanism would imply relativism.\nOne thing to get straight at the outset is a definition of \"humanism\". There are at least two major ideologies that typically fall under the umbrella of humanism. First is secular humanism, which is probably the one you're thinking of when you talk about \"humanism\". Secular humanism tends to justify action in terms of human reason, ethics, and justice. And in doing so, it naturally denounces religious dogma, superstition, and other things that could be considered \"pseudo-science\". However, it does not prescribe a particular code or system of ethics, and many philosophers who fall into the \"humanist\" camp have strongly believed in universal moral standards, exactly the opposite of relativism. A few examples of such thinkers might be: Immanuel Kant, John Stuart Mill, and John Rawls. Certainly, secular humanism can be seen as incompatible with a strong religious faith, on which basis you may take objection to it, but it certainly is not incompatible with objectivism (with a little O), the idea that there is a universal system of ethics accessible to all.\nAnother common branch of humanism is religious humanism, which actually attempts to integrate humanism with religious ideals. Religious humanism is fairly straight-forward: it places the focus on the human being, affirming the individual dignity and worth of all people, just like the name na\u00efvely implies. Certainly this could be compatible with religious teachings. S\u00f8ren Kierkegaard is a famous \"Christian Existential Humanist\".\nBeyond these two specific disciplines, you'll also see \"humanism\" applied generically to any philosophy that places the primary focus on human beings, as opposed to society at large or organized religion. But again, the idea that different cultures have different ideas about morality and ethical principles, and that those ideals are equally as valid as any other culture's ideals, is not implied by this line of thinking. Neither is the relativist tenant that there is no absolute truth or validity. In fact, a humanist could argue that human intuition, reason, and moral virtue are themselves absolute truths!\n\nBut beyond that, it appears that your confusion (and the confusion of whomever wrote the article you are quoting) stems mainly from the notion that atheism (or the rejection of organized religion) implies relativism, absent or beyond its humanist affiliations. That is also quite incorrect (albeit quite a common fallacy that religious scholars succumb to). \nThe flaw lies in thinking that the only possible source for an objective morality is from [a] God. While that's certainly one possible source, and even a good source, it's far from the only possible source. Humanists, as I hinted above, would argue that human rationality, intuition, and logic are the sources for morality, and since all humans share these faculties, such a moral framework would be objectively shared across all humanity.\nThere is an entire world of philosophers who don't necessarily believe in the existence of a supreme being, yet believe in objective morality. The one who makes this position most persuasively is probably Shelly Kagan, a contemporary moral philosopher. And other philosophers have even argued (this is Plato's famous Euthyphro dilemma) that objective morality cannot be based on God, even if both are granted to exist.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Claim",
        "text": "No, humanism does not, in any way, necessitate relativism. They are completely different philosophies. In fact, I'm not sure that I can even imagine a way in which humanism would imply relativism."
      },
      {
        "type": "Definition",
        "text": "One thing to get straight at the outset is a definition of 'humanism'. There are at least two major ideologies that typically fall under the umbrella of humanism."
      },
      {
        "type": "Example",
        "text": "First is secular humanism, which tends to justify action in terms of human reason, ethics, and justice, denouncing religious dogma and superstition. It does not prescribe a particular code of ethics and some humanist philosophers believe in universal moral standards."
      },
      {
        "type": "Example",
        "text": "Religious humanism integrates humanism with religious ideals, focusing on human dignity and worth. S\u00f8ren Kierkegaard is a famous 'Christian Existential Humanist'."
      },
      {
        "type": "Elaboration",
        "text": "Beyond specific disciplines, 'humanism' can refer to any philosophy focusing on human beings. However, this does not imply cultural relativism or the denial of absolute truth."
      },
      {
        "type": "Claim",
        "text": "Atheism or the rejection of organized religion does not imply relativism, even beyond humanist affiliations. The idea that objective morality can only come from God is a fallacy."
      },
      {
        "type": "Elaboration",
        "text": "Humanists argue that human rationality, intuition, and logic are sources of morality, leading to an objectively shared moral framework across humanity."
      },
      {
        "type": "Example",
        "text": "Shelly Kagan, a contemporary moral philosopher, argues for objective morality without belief in a supreme being."
      },
      {
        "type": "Example",
        "text": "Plato's Euthyphro dilemma questions the basis of objective morality on God's existence."
      }
    ]
  },
  {
    "site": "philosophy",
    "question_title": "What factors led to the widespread popularity of nihilism in the 21st century?",
    "question_body": "I used to think that nihilism is popular because of the uprising of positivism in science and scientism in the past century, but I'm no longer sure. I know some also blame post-modernism. What is the core issue that made room for the popularity of nihilism in the last few decades?\nI think atheism has some connection to nihilism; in my opinion they came from the same source in the last century.\nI am thinking about two definitions of nihilism:\n\nPostmodern nihilism - the postmodern position that says \"there's no one truth\", which leads to a nihilism that denies objectivity, and is more popular today because, well, postmodern is quite popular today as it's a new school in philosophy.\n\nExistential nihilism - the existential crisis of \"there's no meaning to life, we are nothing compared to the billions of years of the universe\", which leads to a nihilism that denies any sort of hope and aspiration for a meaning.\n\n\n",
    "answer_body": "While I'm not entirely convinced of the premises of the question, in general people seek out philosophies that address conditions of life as they experience it.  In the marketplace of ideas, a philosophy may thrive not as much because of its connection with deeper truth, but because of its connection with present conundrums.\nIn light of that, I'd submit that part of the reason for the rise in the existentialist family of philosophies --existentialism, nihilism, absurdism, and so forth --is the rise of globalization.  In a world with a diversity of culture and beliefs, it becomes more difficult for people to accept received beliefs without questioning them.\nI'd agree that there is a relationship here with scientism, although I would see that more as an attempted alternative to nihilism than as an extension of it.  \n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "While I'm not entirely convinced of the premises of the question, in general people seek out philosophies that address conditions of life as they experience it."
      },
      {
        "type": "Implication",
        "text": "In the marketplace of ideas, a philosophy may thrive not as much because of its connection with deeper truth, but because of its connection with present conundrums."
      },
      {
        "type": "Implication",
        "text": "In light of that, I'd submit that part of the reason for the rise in the existentialist family of philosophies --existentialism, nihilism, absurdism, and so forth --is the rise of globalization."
      },
      {
        "type": "Implication",
        "text": "In a world with a diversity of culture and beliefs, it becomes more difficult for people to accept received beliefs without questioning them."
      },
      {
        "type": "Comparison",
        "text": "I'd agree that there is a relationship here with scientism, although I would see that more as an attempted alternative to nihilism than as an extension of it."
      }
    ]
  },
  {
    "site": "ai",
    "question_title": "How can an AI train itself if no one is telling it if its answer is correct or wrong?",
    "question_body": "I am a programmer but not in the field of AI. A question constantly confuses me is that how can an AI be trained if we human beings are not telling it its calculation is correct?\nFor example, news usually said something like \"company A has a large human face database so that it can train its facial recognition program more efficiently\". What the piece of news doesn't mention is whether a human engineer needs to tell the AI program each of the program's recognition result is accurate or not.\nAre there any engineers who are constantly telling an AI what it produced it correct or wrong? If no, how can an AI determine if the result it produces is correct or wrong?\n",
    "answer_body": "By \"company A has a large human face database so that it can train its facial recognition program more efficiently\" the article probably means that there is a training dataset $S$ of the form \n$$\nS = \\{ (\\mathbf{x}_1, y_1), \\dots,(\\mathbf{x}_N, y_N) \\}\n$$\nwhere $\\mathbf{x}_i$ is an image of the face of the $i$th human and $y_i$ (which is often called a label, class or target) is e.g. the name of the $i$th human. So, the programmer provides a supervisory signal (the label) for the AI to learn. The programmer also specifies the function that determines the error the AI program is making, based on the answer of the AI model and $y_i$. \nThis way of learning is called supervised learning (SL). However, there are other ways of training an AI. For example, there is unsupervised learning (UL), where the AI needs to find patterns in the data by aggregating objects based on some similarity measure, which is specified by the programmer. There's also reinforcement learning (RL), where the programmer specifies only certain reinforcement signals, that is, the programmer tells the AI which moves or results are \"good\" and which ones are \"bad\" to achieve its goal, by giving to the AI, respectively, a positive or negative reward. You can also combine these three approaches and there are other variations. \n\nAre there any engineers who are constantly telling an AI what it produced it correct or wrong?\n\nYes, in the case of SL. In the case of RL, the programmer also needs to provide the reinforcement signal, but it doesn't need to explicitly tell the AI which action it needs to take. In UL, the programmer needs to specify the way the AI needs to aggregate the objects, so, in this case, the programmer is also involved in the learning process.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "By 'company A has a large human face database so that it can train its facial recognition program more efficiently' the article probably means that there is a training dataset $S$ of the form $S = \\{ (\\mathbf{x}_1, y_1), \\dots,(\\mathbf{x}_N, y_N) \\}$ where $\\mathbf{x}_i$ is an image of the face of the $i$th human and $y_i$ is the label or class associated with that image."
      },
      {
        "type": "Elaboration",
        "text": "The programmer provides a supervisory signal (the label) for the AI to learn and specifies the function that determines the error the AI program is making based on the answer of the AI model and $y_i. This way of learning is called supervised learning (SL)."
      },
      {
        "type": "Comparison",
        "text": "There are other ways of training an AI, such as unsupervised learning (UL) where the AI finds patterns in the data by aggregating objects based on similarity measures specified by the programmer, and reinforcement learning (RL) where the programmer provides reinforcement signals to guide the AI's actions towards a goal."
      },
      {
        "type": "Question",
        "text": "Are there any engineers who are constantly telling an AI what it produced is correct or wrong?"
      },
      {
        "type": "Claim",
        "text": "In supervised learning (SL), engineers provide feedback to the AI on the correctness of its outputs."
      },
      {
        "type": "Elaboration",
        "text": "In reinforcement learning (RL), engineers also provide reinforcement signals but do not explicitly instruct the AI on which actions to take. In unsupervised learning (UL), engineers specify how the AI should aggregate objects, thus being involved in the learning process."
      }
    ]
  },
  {
    "site": "ai",
    "question_title": "Why do you not see dropout layers on reinforcement learning examples?",
    "question_body": "I've been looking at reinforcement learning, and specifically playing around with creating my own environments to use with the OpenAI Gym AI. I am using agents from the stable_baselines project to test with it.\nOne thing I've noticed in virtually all RL examples is that there never seems to be any dropout layers in any of the networks. Why is this?\nI have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.\nBut is there a reason why you don't see dropout layers in RL networks? Is there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?\nOr is it deemed that the chaotic nature of the environment itself should provide enough different combinations of outcomes that you don't need to have dropout layers?\n",
    "answer_body": "Dropout essentially introduces a bit more variance. In supervised learning settings, this indeed often helps to reduce overfitting (although I believe there dropout is also already becoming less.. fashionable in recent years than in the few years before that; I'm not 100% sure though, it's not my primary area of expertise).\nIn Reinforcement Learning, additional variance is not really what we're looking for. There already tends to be a large amount of variance in the learning signals that we get, and this variance already tends to be a major issue for learning stability and/or learning speed. For example:\n\nRandomness in action selection leads to variance in returns that we observe\nThere may be randomness inherent to the environment itself, leading to extra variance in our observations (some environments are nondeterministic)\nUnlike Supervised Learning settings, in Reinforcement Learning we often actually use our own predictions as a part of our loss function / training signal. For example, in temporal-difference learning (like Q-learning / DQN), the target that we update towards looks like $r + \\max_{a'} Q(s', a')$. In that term, only the $r$ is a ground-truth observation (like we would use in supervised learning), and the other term is our own prediction. During a learning process, those latter parts (our own predictions) are changing over time. This is a \"moving target'' problem, which can be viewed as additional variance in our learning signals.\n\nMany important parts of Deep RL algorithms (without which our training processes empirically turn out to destabilize and break down) are very much tailored towards reducing that variance. For example, Target Networks in DQN were introduced specifically to reduce the moving target problem. From this point of view, it's not surprising that if we were to add more artificial variance through other means again (such as dropout), that this would hurt performance / destabilize learning.\n\n\nIs there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?\n\nIn the majority of current (Deep) Reinforcement Learning research, overfitting is indeed not viewed as a problem. The vast majority of RL research consists of training in one environment (for example Cartpole, or Breakout, or one particular level in Pacman, or navigating in one specific maze, etc.), and either constantly evaluating performance during that learning process, or evaluating performance after such a learning process in the same environment.\nIf we were to compare that evaluation methodology to what happens in supervised learning... we are basically evaluating performance on the training set*. In supervised learning, this would be absolutely unacceptable, but in RL it is very much treated as acceptable and more rule than exception. Some say this is simply a problem in current RL research, something that needs to change. It could also be argued that it's not necessarily a problem; if we really are able to train the agent in precisely the same environment that we wish to deploy it in later... well, then what's the problem with it overfitting to that environment? \nSo, when we're using the evaluation methodology described above, indeed we are overfitting to one specific environment, but overfitting is good rather than bad according to our evaluation criteria. It is clear that this methodology does not lead to agents that can generalize well though; if you consistently train an agent to navigate in one particular maze, it will likely be unable to navigate a different maze after training.\n*Note: the truth, in my opinion, is slightly more nuanced than that we are really \"evaluating on the training set\" in RL. See, for example, this nice thread of tweets: https://twitter.com/nanjiang_cs/status/1049682399980908544\n\n\nI have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.\n\nNote that your evaluation methodology described here indeed no longer fits the more \"common\" evaluation methodology. You have a problem with concept drift, with nonstationarity in the environment. This means overfitting may be a problem for you. \nStill, I'm not sure if dropout would help (it's still additional variance which may hurt). First and foremost, you'd want to make sure that there's some way to keep track of the time / month in your inputs, such that you'll at least have a chance of learning a policy that adapts itself over time. If you have a clear, solid boundary between \"training phase\" and \"evaluation phase\", and you know that concept drift occurs across that boundary (you know that your environment behaves differently in the training phase from the evaluation phase)... you really don't have much hope of learning a policy only from experience in the training phase that still performs well in the evaluation phase. I suspect you'll have to get rid of that clear, solid boundary. You'll want to keep learning throughout the evaluation phase as well. This enables your learning algorithm to actually collect experience in the changed environment, and adapt to it.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Dropout essentially introduces a bit more variance. In supervised learning settings, this indeed often helps to reduce overfitting."
      },
      {
        "type": "Comparison",
        "text": "In Reinforcement Learning, additional variance is not really what we're looking for compared to supervised learning settings."
      },
      {
        "type": "Example",
        "text": "Randomness in action selection leads to variance in returns that we observe."
      },
      {
        "type": "Elaboration",
        "text": "Many important parts of Deep RL algorithms are tailored towards reducing variance to stabilize learning processes."
      },
      {
        "type": "Question",
        "text": "Is there another mechanism to deal with overfitting in RL examples where overfitting is not viewed as a problem?"
      },
      {
        "type": "Claim",
        "text": "In the majority of current (Deep) Reinforcement Learning research, overfitting is not viewed as a problem."
      },
      {
        "type": "Implication",
        "text": "Overfitting can be considered acceptable in RL if the agent is trained and deployed in the same environment."
      },
      {
        "type": "Context",
        "text": "Evaluation in RL often involves training and testing in the same environment, unlike supervised learning."
      },
      {
        "type": "Example",
        "text": "Training an agent in one specific maze may lead to poor performance in a different maze due to overfitting."
      },
      {
        "type": "Claim",
        "text": "Evaluation methodology in RL may lead to overfitting to one specific environment, which aligns with the evaluation criteria."
      },
      {
        "type": "Elaboration",
        "text": "Concept drift and nonstationarity in the environment can lead to overfitting becoming a problem."
      },
      {
        "type": "Qualification",
        "text": "Using dropout to address overfitting may introduce additional variance, potentially harming performance."
      },
      {
        "type": "Procedure",
        "text": "To address concept drift, ensure the learning algorithm can adapt over time and across different phases."
      }
    ]
  },
  {
    "site": "ai",
    "question_title": "Why do CNN's sometimes make highly confident mistakes, and how can one combat this problem?",
    "question_body": "I trained a simple CNN on the MNIST database of handwritten digits to 99% accuracy. I'm feeding in a bunch of handwritten digits, and non-digits from a document.\nI want the CNN to report errors, so I set a threshold of 90% certainty below which my algorithm assumes that what it's looking at is not a digit. \nMy problem is that the CNN is 100% certain of many incorrect guesses. In the example below, the CNN reports 100% certainty that it's a 0. How do I make it report failure?\n\nMy thoughts on this:\nMaybe the CNN is not really 100% certain that this is a zero. Maybe it just thinks that it can't be anything else, and it's being forced to choose (because of normalisation on the output vector). Is there any way I can get insight into what the CNN \"thought\" before I forced it to choose?\nPS: I'm using Keras on Tensorflow with Python.\nEdit\nBecause someone asked. Here is the context of my problem:\nThis came from me applying a heuristic algorithm for segmentation of sequences of connected digits. In the image above, the left part is actually a 4, and the right is the curve bit of a 2 without the base. The algorithm is supposed to step through segment cuts, and when it finds a confident match, remove that cut and continue moving along the sequence. It works really well for some cases, but of course it's totally reliant on being able to tell if what it's looking at is not a good match for a digit. Here's an example of where it kind of did okay.\n\nMy next best option is to do inference on all permutations and maximise combined score. That's more expensive.\n",
    "answer_body": "The concept you are looking for is called epistemic uncertainty, also known as model uncertainty. You want the model to produce meaningful calibrated probabilities that quantify the real confidence of the model.\nThis is generally not possible with simple neural networks as they simply do not have this property, for this you need a Bayesian Neural Network (BNN). This kind of network learns a distribution of weights instead of scalar or point-wise weights, which then allow to encode model uncertainty, as then the distribution of the output is calibrated and has the properties you want.\nThis problem is also called out of distribution (OOD) detection, and again it can be done with BNNs, but unfortunately training a full BNN is untractable, so we use approximations.\nAs a reference, one of these approximations is Deep Ensembles, which train several instances of a model in the same dataset and then average the softmax probabilities, and has good out of distribution detection properties. Check the paper here, in particular section 3.5 which shows results for OOD based on entropy of the ensemble probabilities.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "The concept you are looking for is called epistemic uncertainty, also known as model uncertainty. You want the model to produce meaningful calibrated probabilities that quantify the real confidence of the model."
      },
      {
        "type": "Comparison",
        "text": "This is generally not possible with simple neural networks as they simply do not have this property, for this you need a Bayesian Neural Network (BNN)."
      },
      {
        "type": "Elaboration",
        "text": "This kind of network learns a distribution of weights instead of scalar or point-wise weights, which then allow to encode model uncertainty, as then the distribution of the output is calibrated and has the properties you want."
      },
      {
        "type": "Context",
        "text": "This problem is also called out of distribution (OOD) detection, and again it can be done with BNNs, but unfortunately training a full BNN is untractable, so we use approximations."
      },
      {
        "type": "Example",
        "text": "As a reference, one of these approximations is Deep Ensembles, which train several instances of a model in the same dataset and then average the softmax probabilities, and has good out of distribution detection properties."
      },
      {
        "type": "Citation",
        "text": "Check the paper here, in particular section 3.5 which shows results for OOD based on entropy of the ensemble probabilities."
      }
    ]
  },
  {
    "site": "ai",
    "question_title": "How does the (decoder-only) transformer architecture work?",
    "question_body": "How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?\n",
    "answer_body": "Introduction\nLarge-language models (LLMs) have gained tons of popularity lately with the releases of ChatGPT, GPT-4, Bard, and more. All these LLMs are based on the transformer neural network architecture. The transformer architecture was first introduced in the paper \"Attention is All You Need\" by Google Brain in 2017. LLMs/GPT models use a variant of this architecture called de' decoder-only transformer'.\nThe most popular variety of transformers are currently these GPT models. The only purpose of these models is to receive a prompt (an input) and predict the next token/word that comes after this input. Nothing more, nothing less.\nNote: Not all large-language models use a transformer architecture. However, models such as GPT-3, ChatGPT, GPT-4 & LaMDa use the (decoder-only) transformer architecture.\nOverview of the (decoder-only) Transformer model\nIt is key first to understand the input and output of a transformer:\n\nThe input is a prompt (often referred to as context) fed into the transformer as a whole. There is no recurrence.\nThe output depends on the goal of the model. For GPT models, the output is a probability distribution of the next token/word that comes after the prompt. It outputs one prediction for the complete input.\n\nNext, it is essential to understand the key components that make up the decoder-only transformer architecture:\n\nThe embedding: the input of the transformer model is a prompt. This prompt needs to be embedded into something that the model can use.\nThe block(s): This is the main source of complexity. Each block contains a masked multi-head attention submodule, a feedforward network, and several layer normalization operations. Blocks are put in sequence to make the model deeper.\nThe output: the output of the last block is fed through one more linear layer to obtain the final output of the model (a classification, a next word/token etc.)\n\nThe following visualization gives an overview of the transformer architecture.\n\nSelf-Attention Mechanism\nSelf-attention makes the transformer powerful. The intuition of self-attention is that the mechanism allows the model to focus on (attend to) the most relevant parts of the input. A single self-attention mechanism is called a head.\nThe head works as follows. First, the input is fed into three separate linear layers. Two of those (the queries Q and the keys K) are multiplied, scaled, and turned into a probability distribution using a softmax activation function. Think of this probability distribution as describing which indices matter most for the output (i.e. which words in the prompt matter for the next word to be predicted). Finally, the output is multiplied with values V. This thus gives V * the importance of each of the tokens in V. A key observation is that the learnable parameters in the head are the three linear layers.\nThe following figure gives an overview of the operations done in a head and an overview of how multi-head attention works.\n\nMulti-Head Attention\nMulti-head attention is nothing more than several individual heads stacked on top of one another. The input to all heads is equivalent. However, each head has its own weights. After forwarding the input through all the heads, the output of the heads is concatenated and passed through a linear layer which brings the dimensionality back to the dimension of the initial input.\nMasked Self-Attention\nIn the decoder-only transformer, masked self-attention is nothing more than sequence padding. The 'masking' term is a left-over of the original encoder-decoder transformer module in which the encoder could see the whole (original language) sentence, and the decoder could only see the first part of the sentence which was already translated. As such, they called it 'masking'.\nBlock\nEach block contains a multi-head attention submodule, a feedforward network, 2 layer-normalization operations, and 2 skip connections.\nThe feedforward network is simply a multi-layer perceptron. In the original paper, the proposed feedforward module consisted of (1) a fully connected layer; (2) a ReLU activation; (3) another fully connected layer; and (4) a dropout layer.\nThe 'add & norm' blocks get the output from the multi-head attention/feedforward submodule and add it to the input into those modules. After that, a layer normalization operation is performed. Adding the input and output of a submodule together is known as a skip-connection. As blocks can be put in sequence, the skip connections help tremendously reduce the problem of vanishing or exploding gradients. In other words, skip connections are necessary to ensure proper backpropagation of the gradients.\nPositional Embedding\nTransformers take in a complete prompt at once (in contrast to RNNs) and embed this as one big Tensor. As such, transformers do not know which word is at what position in the sentence. This is problematic as the following two sentences mean entirely different things, only dependent on the order of the words:\n\nThe boy chased the bird with a butterfly net.\n\n\nThe bird chased the boy with a butterfly net.\n\nTo that end, a positional embedding is added to allow the model to deduce which word is where. Positional embeddings can be learned using any embedding layer from your favourite AI library. However, the original authors proposed a much more complicated method, which does not require learning any parameters. Please find an elaborate explanation (not mine) on the original positional embedding here.\nOutput\nAfter the prompt is forwarded through all the blocks sequentially, the output is forwarded through one final linear layer. This final linear layer maps the output of the model back to the size of the 'vocabulary'. I.e. if you want to predict the next letter in a message, it would map to 26 (letters) + additional stuff (such as .,-!? etc.).\nThe output of the model is a probability distribution. For GPT models, the output is the probability of each token being the next token in the sequence.\nTraining\nBasic training\nThe basic training process consists of self-supervised learning. Simply put, you gather lots of text, strip the last word from that text, feed it as input into the transformer, check if the prediction matches the word you cut off and backpropagate the error.\nEvery text/sentence/book/webpage can be separated into several samples.\nsample = [\n    [\"This\"],\n    [\"This\", \"is\"],\n    [\"This\", \"is\", \"a\"]] # padding is added until the max-sequence length is reached.\ntargets = [\"is\", \"a\", \"sample\"]\n\nFine-tuning / transfer-learning\nAfter the first stage of training is completed, the model is now a large-language model. As in, it can predict the next word based on a context. However, through fine-tuning/transfer-learning the model can be adapted to better suit the needs of the final application.\nOne of the key reasons why ChatGPT & GPT-4 seem so ridiculously impressive is because of this second stage of training. In this stage, the following process is executed many times:\n\nThe model is given a prompt and generates different answers\nThe different answers are ranked by a human from best to worst.\nThe scores of the different answers are backpropagated.\n\nHowever, transformer models can also be used for different tasks than language generation. They can, for example, be used for sentiment analysis. After doing the basic training, a transformer can be fine-tuned for sentiment analysis by removing the outgoing linear layer and replacing it with a different layer suitable for the task to be executed. Consequently, it can be trained in a supervised fashion on a custom (sentiment analysis) dataset.\nInference (answer generation)\nDoing inference with a transformer is just like training. You insert a prompt and out comes the next word/classification/other.\nFor GPT models, this means that the prompt is extended one word at a time. You insert the prompt, and out comes the first word of the answer. The first word of the answer is now added to the prompt, creating a new, slightly different prompt. This prompt is again forwarded through the model, giving the prediction of a new word.\nAs the output is the probability for each token to be the next one, you can do several things during inference. For one, you can sample from the probability distribution. This induces some randomness into the algorithm. You can also take the token that has the highest probability; then, the model becomes deterministic.\nHow can a transformer do X?\nThe transformer can do X because it has seen enough examples of similar sentences in its training to give a satisfactory output to your prompt. However, for the question 'Why can a transformer not do X?' the answers are vastly more diverse.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Large-language models (LLMs) are models like ChatGPT, GPT-4, Bard, etc., based on the transformer neural network architecture, particularly the 'decoder-only transformer'."
      },
      {
        "type": "Context",
        "text": "The transformer architecture, introduced in the paper 'Attention is All You Need' by Google Brain in 2017, forms the basis for these LLMs/GPT models."
      },
      {
        "type": "Summary",
        "text": "LLMs like GPT models aim to predict the next token/word following a given prompt, utilizing the decoder-only transformer architecture."
      },
      {
        "type": "Definition",
        "text": "The input to a transformer is a prompt, and the output varies based on the model's goal, such as predicting the next token/word in the case of GPT models."
      },
      {
        "type": "Elaboration",
        "text": "The decoder-only transformer comprises key components like embedding, blocks containing attention submodules and feedforward networks, and a final linear layer for output."
      },
      {
        "type": "Visual Description",
        "text": "A visualization is provided to illustrate the transformer architecture and its components."
      },
      {
        "type": "Explanation",
        "text": "Self-attention and multi-head attention mechanisms are crucial in transformers for focusing on relevant parts of the input and processing information across multiple heads."
      },
      {
        "type": "Elaboration",
        "text": "Masked self-attention in the decoder-only transformer deals with sequence padding, while blocks consist of attention submodules, feedforward networks, layer normalization, and skip connections."
      },
      {
        "type": "Elaboration",
        "text": "Positional embeddings are used in transformers to indicate the position of words in a sentence, addressing the issue of word order ambiguity."
      },
      {
        "type": "Summary",
        "text": "The output of a transformer model is a probability distribution, and training involves self-supervised learning and fine-tuning for specific applications."
      },
      {
        "type": "Application",
        "text": "Transformers can be fine-tuned for tasks like sentiment analysis by adapting the model through supervised training on custom datasets."
      },
      {
        "type": "Procedure",
        "text": "Inference with a transformer involves extending the prompt one word at a time to generate answers, with options to sample from the probability distribution or choose the token with the highest probability."
      },
      {
        "type": "Question",
        "text": "The transformer's ability to perform tasks is attributed to its training on similar examples, but the limitations of transformers in certain scenarios lead to diverse answers."
      }
    ]
  },
  {
    "site": "ai",
    "question_title": "How can Transformers handle arbitrary length input?",
    "question_body": "The transformer, introduced in the paper Attention Is All You Need, is a popular new neural network architecture that is commonly viewed as an alternative to recurrent neural networks, like LSTMs and GRUs.\nHowever, having gone through the paper, as well as several online explanations, I still have trouble wrapping my head around how they work. How can a non-recurrent structure be able to deal with inputs of arbitrary length?\n",
    "answer_body": "Actually, there is usually an upper bound for inputs of transformers, due to the inability of handling long-sequence. Usually, the value is set as 512 or 1024 at current stage.\nHowever, if you are asking handling the various input size, adding padding token such as [PAD] in BERT model is a common solution. The position of [PAD] token could be masked in self-attention, therefore, causes no influence. Let's say we use a transformer model with 512 limit of sequence length, then we pass a input sequence of 103 tokens. We padded it to 512 tokens. In the attention layer, positions from 104 to 512 are all masked, that is, they are not attending or being attended.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Actually, there is usually an upper bound for inputs of transformers, due to the inability of handling long-sequence. Usually, the value is set as 512 or 1024 at current stage."
      },
      {
        "type": "Procedure",
        "text": "However, if you are asking handling the various input size, adding padding token such as [PAD] in BERT model is a common solution."
      },
      {
        "type": "Elaboration",
        "text": "The position of [PAD] token could be masked in self-attention, therefore, causes no influence."
      },
      {
        "type": "Example",
        "text": "Let's say we use a transformer model with 512 limit of sequence length, then we pass an input sequence of 103 tokens. We padded it to 512 tokens. In the attention layer, positions from 104 to 512 are all masked, that is, they are not attending or being attended."
      }
    ]
  },
  {
    "site": "physics",
    "question_title": "What is an \"attosecond pulse\", and what can you use it for?",
    "question_body": "The 2023 Nobel Prize in Physics was announced today, and it was awarded to Pierre Agostini, Ferenc Krausz and Anne L\u2019Huillier, for\n\n\u201cexperimental methods that generate attosecond pulses of light for the study of electron dynamics in matter\u201d.\n\nThe documents released by the Nobel Foundation along with the announcement (the popular science background  and the more detailed scientific background do a good job of explaining the basics, but:\nwhy are attosecond pulses exciting, and what can you do with them that you cannot do in any other way?\n",
    "answer_body": "What's the big deal?\nWhen quantum mechanics was being discovered and formalized, in the 1920s and 1930s, our view of physics was deeply rooted in the macroscopic world. We understood that microscopic entities like atoms and molecules existed, and we arrived reasonably quickly at a good understanding of their basic structure, but for a very long time they were very remote objects, whose behaviour was so abstract and disconnected from our everyday experience that it was even kind of pointless to really interrogate it.\nSo, as an example, if you heated up a vial with sodium, then the gas sample in the vial might emit or absorb light at a particular wavelength, and if you worked out the quantum-mechanical maths then you could predict what those wavelengths should be, in terms of quantum jumps between energy levels $-$ but, could you really say what each individual atom in the gas was doing? How could you be sure that those \"quantum jumps\" were even real, if you only ever had access to the macroscopic gas sample, and never to any individual atom?\nMoreover, that same quantum-mechanical maths predicts that the dynamics in an atom will be blazingly fast, and indeed many orders of magnitude faster than any experimental techniques available at the time. So, could you really talk about the electrons \"moving\"? This was aggravated by the fact that the particular choices of quantum-mechanical maths that made sense for this type of experiment talked much more about \"orbitals\" and \"energy levels\", with those mysterious quantum jumps to link them $-$ so maybe it makes more sense to treat those orbitals and energy levels as the \"real\" objects, and disregard the notion that there is any movement in the micro-world?\nHowever, we live in a very different world now. Not only do we have tools like scanning electron microscopy that allow us to observe the atoms that make up a metal surface, we are also now able to hold and control a single atom with delicate electrical \"tweezers\", which then allows us to interrogate it directly. And when we look, much to our chagrin, that individual atom is indeed performing the fabled quantum jumps. More generally, since the turn of the millenium the name of the game (and indeed the routine) has been the observation and control of individual quantum systems.\nA similar story holds for the dynamics of microscopic systems, and for our ability to observe them directly. The discoveries of the laser, and then Q-switching and mode locking allowed laser pulses to get pretty fast, first faster than a microsecond ($10^{-6}\\:\\rm s$) and then faster than a nanosecond ($10^{-9}\\:\\rm s$), respectively, and work in the 1970s and 1980s allowed us to create pulses as short as a picosecond ($10^{-12}\\:\\rm s$) and shorter. If you really push a laser system, using technology known as Chirped Pulse Amplification (which I wrote about previously here when it won its Nobel Prize), you can get down to a few femtoseconds ($10^{-15}\\:\\rm s$). This is very fast for a pulse of light, and it is actually so fast that the pulse of light is no longer a periodic electric-field oscillation, and instead it lasts only for a few cycles. But it is still not fast enough.\nWhy? Because atoms are even faster.\nTo understand how fast atoms are, it is enough to do some basic dimensional analysis. The dynamics of the electrons inside an atom are governed by the Schr\u00f6dinger equation,\n$$\ni\\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m_e}\\nabla^2\\psi -\\frac{e^2}{r}\\psi,\n$$\nand this has only three core constants involved: the reduced Planck constant, $\\hbar$, the electron's mass, $m_e$, and the electron charge $e$. (Or, if you work in SI units, the Coulomb constant $e^2/4\\pi\\epsilon_0$.) And, as it turns out, those constants can be combined into a unique timescale, known as the atomic unit of time,\n$$\nt_\\mathrm{a.u.} = \\frac{\\hbar^3}{m_ee^4} = 24\\:\\rm as,\n$$\nwhich is measured in attoseconds: $1\\:\\rm as = 10^{-18}\\:\\rm s$. As a rule of thumb, the dynamics might be somewhat faster, or somewhat slower, depending on the atom and the conditions, but it will generally stick to that rough order of magnitude.\nAnd that means, in turn, that those dynamics might seem completely out of reach, because the period of oscillation of optical light is still rather slower than this. (For light of wavelength $550\\:\\rm nm$, the period is of about $2\\:\\rm fs$.) So that might make you think that a direct observation of something as fast as atomic dynamics must be out of reach.\nSo how do you make an attosecond pulse?\nThis is the real breakthrough that is being rewarded with today's announcement. Our workhorse is a process known as high-harmonic generation, which uses a highly nonlinear interaction between a gas and a pulse of laser light to generate sharp bursts of radiation $-$ the famed attosecond pulses $-$ which can be much shorter than the period of the pulse that drives the process, and can be as short as a few dozen attoseconds.\nFrom an experimental perspective, what you have to do is simply start with a laser pulse with a fairly long wavelength and slow period (usually in the near-infrared), shine it into a gas cell, and make sure that the pulse is intense. How intense? very intense. Intense enough to directly yank electrons out of the gas atoms and shake them about once they're free. (And, indeed, intense enough that the pulse will burn out the laser amplifier if you let it, as explained in the thread about Chirped Pulse Amplification.)\nThis was done in 1987 by a team led by Anne L'Huillier, and the surprising observation was that the gas emitted harmonics, i.e., additional wavelengths of light at sub-multiples of the original driving wavelength. This was known to occur (second-harmonic generation is almost as old as the laser itself), but L'Huillier and colleagues discovered that if the driving pulse is intense enough, it can generate all sorts of harmonics at crazy high orders, with a very slow decline in emission as the order increases. (Up until the signal reaches a cutoff and decays exponentially, of course.)\nWhat's going on? the basic physics was worked out by Paul Corkum (who was very high in the shortlist for getting the Nobel Prize if it ever did get awarded to attosecond science), and it is known as the three-step model.\n\nImage taken from D. Villeneuve, Contemp. Phys. 59, 47 (2018)\nIn essence, the laser can be thought of as a constant force (and therefore a linear ramp in potential energy) which slowly oscillates and tilts around the potential well that the atomic electron sits in. At the maximum of field intensity, this is enough to yank the electron away (though more on this later), at which point the electron will freely oscillate in the field, gaining energy from the electric field of the light ... up until it crashes into the potential well that it just left, at which point it can recombine back with the ion it left behind, and emit its (now considerable) kinetic energy as a sharp burst of radiation.\nThe coolest things about this collision are that it is very energetic (so the burst of radiation has a high photon energy, and therefore very high frequencies), and that it is very short (it is over in a flash), and it is this short duration that means that the pulses of radiation emitted will be extremely short.\nThe other parts of the Nobel Prize are being awarded for the explicit creation and detection of these sharp bursts of light.\n\nOne thing that happens quite often is that (because the driving pulse is long, and has many periods where the three-step model can happen), the emission is often in the shape of an attosecond-pulse train, sometimes with several dozen sharp bursts following each other in quick succession. Pierre Agostini was the first to directly observe the duration of the bursts within such a train, using a technique known as RABBITT (attoscience has since acquired an \"animal theme\" for our acronyms), and his group was able to show that they were indeed very short, down to as little as $250\\:\\rm as$.\n\nAlternatively, you might want to invest some (considerable) time and energy into finding a way to \"gate\" the emission, so that there is only one burst in the train. (For a fresh-off-the-press review of different ways to \"gate\" the emission see e.g. this preprint.) This gating was achieved by Ferenc Krausz's group, who were able to isolate a single pulse with a duration of $650\\:\\rm as$.\n\n\nOf course, the field has continued to innovate, making things more reliable and robust, but also pushing down the shortest duration achievable. If I understand correctly, the current record is $43\\:\\rm as$, which is very, very short.\n(Another cool record is how high you can push the order of nonlinearity in the process, for which, if I understand correctly, a 2012 classic still holds the prize with a minimal order of nonlinearity of 4,500.)\nWhat can you use these pulses for?\nWe're now down to the most interesting part. Say that you have made one of these attosecond pulses. What can you do with it?\nDirectly observing the wave oscillations of light\nFor me, the most exciting application from the \"classic\" experiments in attoscience is a setup known as \"attosecond streaking\".\nThe basic idea is to take a short attosecond pulse, and overlap it, inside a gas sample, with a slower pulse of infrared light.\n\nThe short pulse has enough photon energy to ionize the gas, and we know that this must happen within the duration of the short pulse. After this ionization, the slower infrared pulse has an electric field which oscillates, and this will impact the final energy and momentum of the electron, but the extent of this effect will depend on when the electron is released, so by changing the time delay between the two, we can scan against this electric field.\n$\\qquad$\n\nThe end result, shown above, is a direct observation of the oscillations of the electric field (raw data on the left, and reconstructed electric field on the right), which is a task that was considered somewhere between impossible and unthinkable for many, many decades after we understood that light was a wave (but only had indirect ways to prove it).\nI've discussed this experiment previously here. For more details (and the source of the figures), see the landmark publication:\n\nDirect measurement of light waves. E. Goulielmakis et al. Science 305, 1267 (2004); author eprint.\n\nDirectly observing electron motion in real time\nSimilarly to observing the motion of the electric field of light, we can also observe the motion of electrons inside an atom. I have discussed this in detail in Is there oscillating charge in a hydrogen atom?, but the short story is that if you prepare an electron in a quantum superposition of two different energy levels, such as the combination\n$$\n\\psi = \\psi_{1s} + \\psi_{2p}\n$$\nof the hydrogen $1s$ and $2p$ levels, the charge density in the atom will oscillate over time:\n\nMathematica source through Import[\"http://halirutan.github.io/Mathematica-SE-Tools/decode.m\"][\"https://i.sstatic.net/KAbFl.png\"]\nThis is not a hypothetical or purely theoretical construct, and we can directly observe it in experiment. The first landmark test, reported in\n\nReal-time observation of valence electron motion. E. Goulielmakis et al. Nature 466, 739 (2010).\n\nwas able to show a clear oscillation in how much a short pulse was absorbed by an oscillating charge distribution caused by spin-orbit interactions (where different parts of the oscillations correspond to different orientations of the charge density, and therefore to different absorption profiles), showing a clear corresponding oscillation in the absorbance:\n\n\n\nSimilarly, a much-beloved example is the observation of charge oscillation dynamics in a bio-relevant molecule, phenylalanine, which was reported in\n\nUltrafast electron dynamics in phenylalanine initiated by attosecond pulses. F. Calegari et al., Science 346, 336 (2014),\n\nand where the ionization of the molecule by a (relatively) short laser pulse (in the near-infrared) is then probed by a (very) short attosecond burst. The resulting dynamics inside the molecule are fairly complicated,\n\nbut they lead to clear oscillations in the signal (with the graph below showing the overall decay, and the oscillations on top of an exponential background) at a very short timescale that is only observable thanks to the availability of attosecond pulses.\n\nWatching quantum interference build up in real time\nI will do one more direct-timing-of-observation, because I think they're really cool. This one is again about a quantum superposition, but one that happens with a free electron. When you ionize an atom, the electron gets released, and one photon gets absorbed. And, more importantly, the details of the energy states that the electron gets released into will be imprinted into the absorbance spectrum of the light.\nIn particular, it is possible to tune things so that you are ionizing close to a resonance: the electron can either ionize directly, or it can spend some time in a highly-excited autoionizing state (also explained here and here) that will fall apart after some time. The end result is that the electron will go into a superposition of both pathways, which will interfere in its spectrum and cause a wonky, nontrivial shape in the absorption spectrum.\nHowever, if we have short pulses of radiation, we are able to control how long we let the electron to sit in that autoionizing state, before we come in with a second pulse of light to disrupt it, and kill the interference:\n\nAnd indeed, when we do this, the build-up of the line and the development of the interference features (and particularly that sharp dip on the right-hand side of the line) is very clearly seen in experiment:\n\nAnd, just to add some more pretty pictures, here it is all stacked together, on the left-hand figure, and on the right a similar experiment showing very clearly the destructive interference building up over time:\n\n\nFor more details, and the sources of the figures, see\n\nObserving the ultrafast buildup of a Fano resonance in the time domain. A. Kaldun et al. Science 354, 738 (2016)\n\nand\n\nAttosecond dynamics through a Fano resonance: Monitoring the birth of a photoelectron. V Gruson et al. Science 354, 734 (2016)\n\nMoreover, it is also possible to use these types of resonances to enhance high-harmonic generation itself, in a process known as resonant HHG. For a nice review written by a colleague (in a paper I coauthored) see Eur. Phys. J D 75, 209 (2021) (arXiv:2101.09335).\nFurther reading\nLong as this post is, I have only just scratched the surface. Here are some additional places to read more about the field:\n\nAttosecond science. D. Villeneuve, Contemp. Phys. 59, 47 (2018) (author eprint)\n\nAttosecond science. P.B. Corkum & F. Krausz. Nature Physics 3, 381 (2007) (author eprint)\n\nThe physics of attosecond light pulses. P. Agostini & L.F. DiMauro. Reports on Progress in Physics 67, 813 (2004) (author eprint)\n\nAttosecond electromagnetic pulses: generation, measurement, and application. Attosecond metrology and spectroscopy. M.Yu. Ryabikin et al. *Physics-Uspekhi 66, 360 (2023)\n\nShining the shortest flashes of light on the secret life of electrons. M. Khokhlova, E. Pisanty & A. Zair. Advanced Photonics 5, 060501 (2023)\n\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "When quantum mechanics was being discovered and formalized, in the 1920s and 1930s, our view of physics was deeply rooted in the macroscopic world."
      },
      {
        "type": "Example",
        "text": "So, as an example, if you heated up a vial with sodium, then the gas sample in the vial might emit or absorb light at a particular wavelength."
      },
      {
        "type": "Question",
        "text": "But, could you really say what each individual atom in the gas was doing? How could you be sure that those 'quantum jumps' were even real, if you only ever had access to the macroscopic gas sample, and never to any individual atom?"
      },
      {
        "type": "Context",
        "text": "Moreover, that same quantum-mechanical maths predicts that the dynamics in an atom will be blazingly fast, and indeed many orders of magnitude faster than any experimental techniques available at the time."
      },
      {
        "type": "Claim",
        "text": "However, we live in a very different world now. Not only do we have tools like scanning electron microscopy that allow us to observe the atoms that make up a metal surface, we are also now able to hold and control a single atom with delicate electrical 'tweezers', which then allows us to interrogate it directly."
      },
      {
        "type": "Procedure",
        "text": "From an experimental perspective, what you have to do is simply start with a laser pulse with a fairly long wavelength and slow period (usually in the near-infrared), shine it into a gas cell, and make sure that the pulse is intense."
      },
      {
        "type": "Elaboration",
        "text": "This was done in 1987 by a team led by Anne L'Huillier, and the surprising observation was that the gas emitted harmonics, i.e., additional wavelengths of light at sub-multiples of the original driving wavelength."
      },
      {
        "type": "Visual Description",
        "text": "The basic idea is to take a short attosecond pulse, and overlap it, inside a gas sample, with a slower pulse of infrared light."
      },
      {
        "type": "Summary",
        "text": "We have progressed from a time when quantum phenomena were abstract and inaccessible to a time where we can directly observe and manipulate individual atoms, leading to breakthroughs in generating attosecond pulses and exploring their applications in various experiments."
      }
    ]
  },
  {
    "site": "physics",
    "question_title": "Cooling a cup of coffee with help of a spoon",
    "question_body": "During breakfast with my colleagues, a question popped into my head:\nWhat is the fastest method to cool a cup of coffee, if your only available instrument is a spoon?\nA qualitative answer would be nice, but if we could find a mathematical model or even better make the experiment (we don't have the means here:-s) for this it would be great! :-D\nSo far, the options that we have considered are (any other creative methods are also welcome):\nStir the coffee with the spoon:\nPros:\n\nThe whirlpool has a greater surface than the flat coffee, so it is better for heat exchange with the air.\nDue to the difference in speed between the liquid and the surrounding air, the Bernoulli effect should lower the pressure and that would cool it too to keep the atmospheric pressure constant.\n\nCons:\n\nJoule effect should heat the coffee.\n\nLeave the spoon inside the cup:\nAs the metal is a good heat conductor (and we are not talking about a wooden spoon!), and there is some part inside the liquid and another outside, it should help with the heat transfer, right?\nA side question about this is what is better, to put it like normal or reversed, with the handle inside the cup? (I think it is better reversed, as there is more surface in contact with the air, as in the CPU heat sinks).\nInsert and remove the spoon repeatedly:\nThe reasoning for this is that the spoon cools off faster when it's outside.\n(I personally think it doesn't pay off the difference between keeping it always inside, as as it gets cooler, the lesser the temperature gradient and the worse for the heat transfer).\n",
    "answer_body": "I We did the experiment. (Early results indicate that dipping may win, though the final conclusion remains uncertain.)\n\n$\\mathrm{H_2O}$ ice bath\ncanning jar\nthermometer\npot of boiling water\nstop watch\n\nThere were four trials, each lasting 10 minutes. Boiling water was poured into the canning jar, and the spoon was taken from the ice bath and placed into the jar.  A temperature reading was taken once every minute. After each trial the water was poured back into the pot of boiling water and the spoon was placed back into the ice bath.\n\n\n Method:                  Final Temp.\n 1. No Spoon              151 F \n 2. Spoon in, no motion   149 F\n 3. Spoon stirring        147 F\n 4. Spoon dipping         143 F\n\nTemperature readings have an uncertainty of $\\pm1\\,\\mathrm{^\\circ F}$.\n\n Red   line:  no Spoon\n Green line:  Spoon in, no motion\n Aqua  line:  Stirring\n Blue  line:  Dipping\n\n\n$$\\begin{array}{|c|cl|cl|cl|cl|} \n\\hline\n\\text{Min} & \\text{No Spoon} & & \\text{Spoon} & & \\text{Stirring} & & \\text{Dipping} \\\\ \\hline\n & \\text{\u00b0F} & \\text{\u00b0C} & \\text{\u00b0F} & \\text{\u00b0C} & \\text{\u00b0F} & \\text{\u00b0C} & \\text{\u00b0F} & \\text{\u00b0C} \\\\ \\hline\n1' & 180 & 82.22 & 175 & 79.44 & 175 & 79.44 & 177 & 80.56 \\\\\n2' & 174 & 78.89 & 172 & 77.78 & 171 & 77.22 & 173 & 78.33 \\\\\n3' & 171 & 77.22 & 168 & 75.56 & 167 & 75 & 168 & 75.56 \\\\\n4' & 168 & 75.56 & 165 & 73.89 & 164 & 73.33 & 164 & 73.33 \\\\\n5' & 164 & 73.33 & 162 & 72.22 & 161 & 71.67 & 160 & 71.11 \\\\\n6' & 161 & 71.67 & 160 & 71.11 & 158 & 70 & 156 & 68.89 \\\\\n7' & 158 & 70 & 156 & 68.89 & 155 & 68.33 & 152 & 66.67 \\\\\n8' & 155 & 68.33 & 153 & 67.22 & 152 & 66.67 & 149 & 65 \\\\\n9' & 153 & 67.22 & 151 & 66.11 & 150 & 65.56 & 146 & 63.33 \\\\\n10' & 151 & 66.11 & 149 & 65 & 147 & 63.89 & 143 & 61.67 \\\\ \\hline\n\\end{array}$$\n",
    "audience_level": "Intermediate",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Procedure",
        "text": "We did the experiment. (Early results indicate that dipping may win, though the final conclusion remains uncertain.)"
      },
      {
        "type": "Context",
        "text": "Materials used: $\\mathrm{H_2O}$ ice bath, canning jar, thermometer, pot of boiling water, stop watch"
      },
      {
        "type": "Procedure",
        "text": "There were four trials, each lasting 10 minutes. Boiling water was poured into the canning jar, and the spoon was taken from the ice bath and placed into the jar. A temperature reading was taken once every minute. After each trial the water was poured back into the pot of boiling water and the spoon was placed back into the ice bath."
      },
      {
        "type": "Summary",
        "text": "Method and Final Temperature results:"
      },
      {
        "type": "Classification",
        "text": "1. No Spoon: 151\u00b0F\\n2. Spoon in, no motion: 149\u00b0F\\n3. Spoon stirring: 147\u00b0F\\n4. Spoon dipping: 143\u00b0F"
      },
      {
        "type": "Context",
        "text": "Temperature readings have an uncertainty of \u00b11\u00b0F."
      },
      {
        "type": "Visual Description",
        "text": "The table shows the temperature readings in Fahrenheit and Celsius for each minute of the experiment for the different methods of handling the spoon."
      }
    ]
  },
  {
    "site": "physics",
    "question_title": "What is $\\Delta t$ in the time-energy uncertainty principle?",
    "question_body": "In non-relativistic QM, the $\\Delta E$ in the time-energy uncertainty principle is the limiting standard deviation of the set of energy measurements of $n$ identically prepared systems as $n$ goes to infinity. What does the $\\Delta t$ mean, since $t$ is not even an observable?\n",
    "answer_body": "Let a quantum system with Hamiltonian $H$ be given.  Suppose the system occupies a pure state $|\\psi(t)\\rangle$ determined by the Hamiltonian evolution.  For any observable $\\Omega$ we use the shorthand\n$$\n  \\langle \\Omega \\rangle = \\langle \\psi(t)|\\Omega|\\psi(t)\\rangle.  \n$$\nOne can show that (see eq. 3.72 in Griffiths QM)\n$$\n  \\sigma_H\\sigma_\\Omega\\geq\\frac{\\hbar}{2}\\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\right|\n$$\nwhere $\\sigma_H$ and $\\sigma_\\Omega$ are standard deviations\n$$\n  \\sigma_H^2 = \\langle H^2\\rangle-\\langle H\\rangle^2, \\qquad \\sigma_\\Omega^2 = \\langle \\Omega^2\\rangle-\\langle \\Omega\\rangle^2\n$$\nand angled brackets mean expectation in $|\\psi(t)\\rangle$.  It follows that if we define\n$$\n  \\Delta E = \\sigma_H, \\qquad \\Delta t = \\frac{\\sigma_\\Omega}{|d\\langle\\Omega\\rangle/dt|}\n$$\nthen we obtain the desired uncertainty relation\n$$\n  \\Delta E \\Delta t \\geq \\frac{\\hbar}{2}\n$$\nIt remains to interpret the quantity $\\Delta t$.  It tells you the approximate amount of time it takes for the expectation value of an observable to change by a standard deviation provided the system is in a pure state.  To see this, note that if $\\Delta t$ is small, then in a time $\\Delta t$ we have\n$$\n  |\\Delta\\langle\\Omega\\rangle| =\\left|\\int_t^{t+\\Delta t} \\frac{d\\langle \\Omega\\rangle}{dt}\\,dt\\right| \\approx \\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\Delta t\\right| = \\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\right|\\Delta t = \\sigma_\\Omega\n$$\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Let a quantum system with Hamiltonian $H$ be given.  Suppose the system occupies a pure state $|\\psi(t)\\rangle$ determined by the Hamiltonian evolution."
      },
      {
        "type": "Procedure",
        "text": "For any observable $\\Omega$ we use the shorthand $\\langle \\Omega \\rangle = \\langle \\psi(t)|\\Omega|\\psi(t)\\rangle."
      },
      {
        "type": "Claim",
        "text": "One can show that $\\sigma_H\\sigma_\\Omega\\geq\\frac{\\hbar}{2}\\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\right|$, where $\\sigma_H$ and $\\sigma_\\Omega$ are standard deviations."
      },
      {
        "type": "Definition",
        "text": "$\\sigma_H^2 = \\langle H^2\\rangle-\\langle H\\rangle^2, \\qquad \\sigma_\\Omega^2 = \\langle \\Omega^2\\rangle-\\langle \\Omega\\rangle^2$ and angled brackets mean expectation in $|\\psi(t)\\rangle$."
      },
      {
        "type": "Procedure",
        "text": "If we define $\\Delta E = \\sigma_H, \\qquad \\Delta t = \\frac{\\sigma_\\Omega}{|d\\langle\\Omega\\rangle/dt|}$, then we obtain the desired uncertainty relation $\\Delta E \\Delta t \\geq \\frac{\\hbar}{2}$."
      },
      {
        "type": "Elaboration",
        "text": "It remains to interpret the quantity $\\Delta t$.  It tells you the approximate amount of time it takes for the expectation value of an observable to change by a standard deviation provided the system is in a pure state."
      },
      {
        "type": "Example",
        "text": "To see this, note that if $\\Delta t$ is small, then in a time $\\Delta t$ we have $|\\Delta\\langle\\Omega\\rangle| =\\left|\\int_t^{t+\\Delta t} \\frac{d\\langle \\Omega\\rangle}{dt}\\,dt\\right| \\approx \\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\Delta t\\right| = \\left|\\frac{d\\langle \\Omega\\rangle}{dt}\\right|\\Delta t = \\sigma_\\Omega$."
      }
    ]
  },
  {
    "site": "physics",
    "question_title": "How can you weigh your own head in an accurate way?",
    "question_body": "I read some methods but they're not accurate. They use the Archimedes principle and they assume uniform body density which of course is far from true. Others are silly like this one:\nTake a knife then remove your head.\nPlace it on some scale\nTake the reading\nre-attach your head.\nI'm looking for some ingenious way of doing this accurately without having to lie on your back  and put your head on a scale which isn't a good idea.\n",
    "answer_body": "Get someone to relax their neck as much as possible, stabilize their torso, then punch them in the head with a calibrated fist and measure the initial acceleration.  Apply $\\vec F=m \\vec a$.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Procedure",
        "text": "Get someone to relax their neck as much as possible, stabilize their torso, then punch them in the head with a calibrated fist and measure the initial acceleration."
      },
      {
        "type": "Formula",
        "text": "Apply $\\vec F=m \\vec a$."
      }
    ]
  },
  {
    "site": "physics",
    "question_title": "Is anti-matter matter going backwards in time?",
    "question_body": "Some sources describe antimatter as just like normal matter, but \"going backwards in time\". What does that really mean? Is that a good analogy in general, and can it be made mathematically precise? Physically, how could something move backwards in time?\n",
    "answer_body": "To the best of my knowledge, most physicists don't believe that antimatter is actually matter moving backwards in time. It's not even entirely clear what would it really mean to move backwards in time, from the popular viewpoint.\nIf I'm remembering correctly, this idea all comes from a story that probably originated with Richard Feynman. At the time, one of the big puzzles of physics was why all instances of a particular elementary particle (all electrons, for example) are apparently identical. Feynman had a very hand-wavy idea that all electrons could in fact be the same electron, just bouncing back and forth between the beginning of time and the end. As far as I know, that idea never developed into anything mathematically grounded, but it did inspire Feynman and others to calculate what the properties of an electron moving backwards in time would be, in a certain precise sense that emerges from quantum field theory. What they came up with was a particle that matched the known properties of the positron.\nJust to give you a rough idea of what it means for a particle to \"move backwards in time\" in the technical sense: in quantum field theory, particles carry with them amounts of various conserved quantities as they move. These quantities may include energy, momentum, electric charge, \"flavor,\" and others. As the particles move, these conserved quantities produce \"currents,\" which have a direction based on the motion and sign of the conserved quantity. If you apply the time reversal operator (which is a purely mathematical concept, not something that actually reverses time), you reverse the direction of the current flow, which is equivalent to reversing the sign of the conserved quantity, thus (roughly speaking) turning the particle into its antiparticle.\nFor example, consider electric current: it arises from the movement of electric charge, and the direction of the current is a product of the direction of motion of the charge and the sign of the charge.\n$$\\vec{I} = q\\vec{v}$$\nPositive charge moving left ($+q\\times -v$) is equivalent to negative charge moving right ($-q\\times +v$). If you have a current of electrons moving to the right, and you apply the time reversal operator, it converts the rightward velocity to leftward velocity ($-q\\times -v$). But you would get the exact same result by instead converting the electrons into positrons and letting them continue to move to the right ($+q\\times +v$); either way, you wind up with the net positive charge flow moving to the right.\nBy the way, optional reading if you're interested: there is a very basic (though hard to prove) theorem in quantum field theory, the TCP theorem, that says that if you apply the three operations of time reversal, charge conjugation (switch particles and antiparticles), and parity inversion (mirroring space), the result should be exactly equivalent to what you started with. We know from experimental data that, under certain exotic circumstances, the combination of charge conjugation and parity inversion does not leave all physical processes unchanged, which means that the same must be true of time reversal: physics is not time-reversal invariant. Of course, since we can't actually reverse time, we can't test in exactly what manner this is true.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "To the best of my knowledge, most physicists don't believe that antimatter is actually matter moving backwards in time. It's not even entirely clear what would it really mean to move backwards in time, from the popular viewpoint."
      },
      {
        "type": "Story",
        "text": "If I'm remembering correctly, this idea all comes from a story that probably originated with Richard Feynman. At the time, one of the big puzzles of physics was why all instances of a particular elementary particle (all electrons, for example) are apparently identical. Feynman had a very hand-wavy idea that all electrons could in fact be the same electron, just bouncing back and forth between the beginning of time and the end."
      },
      {
        "type": "Elaboration",
        "text": "As far as I know, that idea never developed into anything mathematically grounded, but it did inspire Feynman and others to calculate what the properties of an electron moving backwards in time would be, in a certain precise sense that emerges from quantum field theory."
      },
      {
        "type": "Explanation",
        "text": "What they came up with was a particle that matched the known properties of the positron."
      },
      {
        "type": "Example",
        "text": "For example, consider electric current: it arises from the movement of electric charge, and the direction of the current is a product of the direction of motion of the charge and the sign of the charge."
      },
      {
        "type": "Elaboration",
        "text": "If you apply the time reversal operator, you reverse the direction of the current flow, which is equivalent to reversing the sign of the conserved quantity, thus (roughly speaking) turning the particle into its antiparticle."
      },
      {
        "type": "Elaboration",
        "text": "By the way, optional reading if you're interested: there is a very basic (though hard to prove) theorem in quantum field theory, the TCP theorem, that says that if you apply the three operations of time reversal, charge conjugation (switch particles and antiparticles), and parity inversion (mirroring space), the result should be exactly equivalent to what you started with."
      },
      {
        "type": "Implication",
        "text": "We know from experimental data that, under certain exotic circumstances, the combination of charge conjugation and parity inversion does not leave all physical processes unchanged, which means that the same must be true of time reversal: physics is not time-reversal invariant."
      }
    ]
  },
  {
    "site": "christianity",
    "question_title": "What major translations of the Bible are in the Public Domain?",
    "question_body": "Quoting Biblical text is technically a dicey proposition.  While the original manuscripts are obviously public domain, not every translation is.\nIndeed, the NIV even posts the following copyright notice on BibleGateway:\n\nCopyright Information\nThe NIV text may be quoted in any form (written, visual, electronic or audio), up to and inclusive of five hundred (500) verses without express written permission of the publisher, providing the verses do not amount to a complete book of the Bible nor do the verses quoted account for twenty-five percent (25%) or more of the total text of the work in which they are quoted.\nWhen the NIV is quoted in works that exercise the above fair use clause, notice of copyright must appear on the title or copyright page or opening screen of the work (whichever is appropriate) as follows:\nTHE HOLY BIBLE, NEW INTERNATIONAL VERSION\u00ae, NIV\u00ae Copyright \u00a9 1973, 1978, 1984, 2011 by Biblica, Inc.\u2122 Used by permission. All rights reserved worldwide.\nThese Scriptures are copyrighted by the Biblica, Inc.\u2122 and have been made available on the Internet for your personal use only. Any other use including, but not limited to, copying or reposting on the Internet is prohibited. These Scriptures may not be altered or modified in any form and must remain in their original context. These Scriptures may not be sold or otherwise offered for sale.\n\nGiven this, what significant translations of the Bible can I actually quote at length?\n",
    "answer_body": "The issue with copyright translations.\nOne translation that was produced with the specific intention of avoiding copyright entanglements is the World English Bible.  It is modernization of the American Standard Version (ASV) placed into the public domain.  A paragraph from the site's FAQ is worth quoting:\n\nThe copyright laws of most nations and the international treaties that support them are a mixed blessing. By granting authors and translators a legal monopoly (for a limited, but very long, time) on the right of copying and \u201cfirst sale\u201d of their works, the law makers have made writing and translating very profitable for some people whose works are in great demand. This has, no doubt, been a factor in the creation of many of the good Modern English translations of the Holy Bible that we now enjoy. The problem with this system, with respect to the Holy Bible, is that it has had the effect of limiting distribution of God\u2019s Word in modern languages. For example, I cannot legally post copies of the entire New International Version of the Holy Bible on my web site in a downloadable, searchable, and readily copyable format without the permission of the International Bible Society and Zondervan (copyright owner and publisher). Zondervan won\u2019t grant such permission unless they get a significant royalty (they quoted me $10,000 + $10/copy distributed) and unless I convince them that my Bible search software is \u201cgood enough\u201d for them. Needless to say, the Bible search software that I am writing with the intention of distributing as donorware will not come with the NIV.\n\nFurther, organizations such as The Gideons International that distribute Bibles must pay copyright holders a fee if they wish to give away a modern translation (such as the New King James Version) rather than the King James with all of it's Elizabethanisms.  The restrictions can also have an impact on smaller scales.  The NASB licence reads (emphasis in the original):\n\nThe text of the New American Standard Bible\u00ae may be quoted and/or reprinted up to and inclusive of one thousand (1,000) verses without express written permission of The Lockman Foundation, providing the verses do not amount to a complete book of the Bible nor do the verses quoted account for more than 50% of the total work in which they are quoted.\n\nThese restrictions mean it's (probably) fine to quote a passage or two in an answer on Stack Exchange or in an article or book.  But it's not possible to reprint an entire book of the Bible for the purposes of a \"Manuscript Study\" (AKA, \"Communal Discovery Bible Study Method\").  While I doubt the Lockman Foundation will have agents tracking down people who print out Mark for their devotional group to mark up, such a practice violates the intentions of the publisher who fronted the money and commissioned the translation.  As Paul says:\n\nThe elders who rule well are to be considered worthy of double honor, especially those who work hard at preaching and teaching. For the Scripture says, \u201cYOU SHALL NOT MUZZLE THE OX WHILE HE IS THRESHING,\u201d and \u201cThe laborer is worthy of his wages.\u201d\u20141st Timothy 5:17-18 (NASB)\n\nFinally, many translations restrict derivative works (such as the WEB effort and my idea of making a Southern American Version\u2122).  The US laws concerning derivative works is complicated, but creators who don't want to deal with lawyers would be advised to base their work on translations free of copyright restrictions. \nBut God's Word should be free, man!\nThe Hebrew, Aramaic, and Greek texts of the Bible are public domain (and a cornerstone of our cultural heritage).  But it turns out that even the critical text is protected by copyright laws.  For the general public, this is no great loss as there are plenty of unrestricted options and the licences for most translations allow for the most common uses of the texts.  But all of this has serious implications for scholars who honor the wishes of the Bible publishers.  It's tempting to get angry with the publishers or to ignore the restrictions.\nHowever, we have a duty to comply with the licences of our translations so that the hard-working folks who produce them can afford to keep laboring.  Most of them are reasonable people who are as interested in seeing God's Word preached as you are.  In fact I'll let the Crossway (publishers of the ESV) blog have the final word:\n\nWe're not going to come after you if you don't cite quotes from the ESV according to these guidelines. But we'd appreciate it if you did. Using the letters \u201cESV\u201d also helps us track the popularity of the ESV in the blogosphere.\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "The issue with copyright translations."
      },
      {
        "type": "Elaboration",
        "text": "One translation that was produced with the specific intention of avoiding copyright entanglements is the World English Bible. It is a modernization of the American Standard Version (ASV) placed into the public domain."
      },
      {
        "type": "Quote",
        "text": "The copyright laws of most nations and the international treaties that support them are a mixed blessing..."
      },
      {
        "type": "Example",
        "text": "For example, I cannot legally post copies of the entire New International Version of the Holy Bible on my website in a downloadable, searchable, and readily copyable format without the permission of the International Bible Society and Zondervan (copyright owner and publisher)."
      },
      {
        "type": "Caveat",
        "text": "While I doubt the Lockman Foundation will have agents tracking down people who print out Mark for their devotional group to mark up, such a practice violates the intentions of the publisher who fronted the money and commissioned the translation."
      },
      {
        "type": "Claim",
        "text": "But God's Word should be free, man!"
      },
      {
        "type": "Elaboration",
        "text": "The Hebrew, Aramaic, and Greek texts of the Bible are public domain (and a cornerstone of our cultural heritage). But it turns out that even the critical text is protected by copyright laws."
      },
      {
        "type": "Implication",
        "text": "All of this has serious implications for scholars who honor the wishes of the Bible publishers."
      },
      {
        "type": "Qualification",
        "text": "However, we have a duty to comply with the licenses of our translations so that the hard-working folks who produce them can afford to keep laboring."
      },
      {
        "type": "Citation",
        "text": "In fact, I'll let the Crossway (publishers of the ESV) blog have the final word: 'We're not going to come after you if you don't cite quotes from the ESV according to these guidelines. But we'd appreciate it if you did.'"
      }
    ]
  },
  {
    "site": "christianity",
    "question_title": "The Bible seems to allow polygamy. Why doesn't the church?",
    "question_body": "In Exodus 21:10 and Deuteronomy 21:15-16 (among many other passages), it talks about polygamy as if it were acceptable.\nIndeed, there are several holy men (Abraham and Solomon come to mind offhand) that had multiple wives. \nIt seems pretty clear that God doesn't mind polygamy and that it is allowed.\nWhy is there a modern-day restriction on such practices?\nEdit\nAs has been pointed out, I'm using \"polygamy\" in the modern/popular sense (such as \"polygamy camps\" of the FLDS group).  Truly, the actual word is \"polygyny\", which is a man marrying multiple wives.  Other forms of polygamy (polyandry and such) seem pretty clearly against God's word.\nEdit 2\nPer a comment, I'm seeking any answer from any denomination/doctrine that does not support polygamy.  Although there are a few denominations that do, I'm not interested in their history so much.\n",
    "answer_body": "\nNote: Throughout, I use the word \"polygamy\" in place of \"polygyny\", even though I explicitly mean to polygyny.\n\nIn short:\nPolygamy is a sin because it goes against the law.  The law is in place because it is a carryover from the paganistic societies of ancient Rome.  Preventing polygamy was not a biblical concept, but one that came after Jesus, after authorship of the bible, and after the genesis of Christianity.\nWhy Polygamy is a sin\nThe reason that polygamy is no longer allowed by the church is because it's no longer allowed by society and is, therefore, against the law.\nWe Christians are told to uphold the law and subject ourselves to the law\nRomans 13:1-2 (NIV)\n\n1 Let everyone be subject to the governing authorities, for there is no authority except that which God has established. The authorities that exist have been established by God. 2 Consequently, whoever rebels against the authority is rebelling against what God has instituted, and those who do so will bring judgment on themselves.\n\nBecause of this, polygamy is a sin if (and only if) it breaks the law.  Similarly, it is a sin in the same manner and degree that breaking a speed limit is a sin: it's something not forbidden by God and solely forbidden by the law.\nWhy it's illegal\nPolygamy was not uncommon among the Jews in biblical times.  It wasn't until the the Romans came in that polygamy became outlawed.  Even then, it was still allowed; Josephus made notes explaining that polygamy was permitted to Herod because it was permitted by Jewish custom.\nPolygamy modelled by Jesus's parable\nWe can see that polygamy was commonly accepted in the New Testament from the Parable of the Ten Virgins (Matthew 25:1-13).  In this parable, Jesus tells of a bridegroom that is preparing to marry ten women.  This parable was used by Jesus to describes himself.  The polygamous bridegroom in the parable is a reference to Jesus.\nFall of polygamy\nIt was, in fact, the Greek and Roman rules against polygamy that spread (along with the Grecian and Roman empires) and became the culture, forcing out the practice of polygamy.\nMonogomy and polygyny by Walter Scheidel, Stanford University (2009)\n\nThus, even though Greeks and Romans need not have been the first cultures to prescribe monogamy, these are the earliest securely attested cases and, moreover, established a paradigm for subsequent periods that eventually attained global dominance.\n...\nThe true historical significance of Greco-Roman [monogamy] may well lie in its impact on the\nChristian tradition.\n\nIt was these two huge historical forces that established monogamy in our society--not something born of God, but something born of Pagan societies.\nModern Society and Law\nChristianity tied itself to the banner of the Roman empire 300 years after Christ (when Constantine established what would become the Roman Catholic church).  This organization spread Christianity by using the power and authority of the Roman empire.  It was this Roman concept of monogamy (predating Christianity) that carried over from its paganistic roots into the Christian society.  It was at that time (and not before) that Christianity became intertwined with the idea of monogamy.\nThis pagan idea of monogamy from Rome slowly infused into the Christian culture (source); and it was because association with Christianity (and the Roman Catholic Church) that the paganistic, Roman practice remains today.\nThis Roman practice has tangled itself into the Christian culture (via the early Roman Catholic church) and therefore all \"Christian\" nations since (founded under Western culture) have adopted the laws prohibiting polygamy from their outset.\nBecause of this, polygamy is illegal in most parts of the world.  Because it's illegal, it is a sin.\nCatholicism Today\nIt should be noted that Catholicism today still carries over this idea from ancient Rome.  In the Catechism of the Catholic Church, it states \"However polygamy is not in accord with the moral law.\"\nBecause of this, polygamy is solidly outlawed within Catholicism.  Granted, if you read the last section, this should not surprise anyone.\n\nAnswering Arguments Against Polygamy\nGod tolerated the sin for the time being\nMalachi 3:6 says The Lord does not change.\nTo presume that he would accept something during one time in history that he detests at a different point is crazy--heretical even.  It goes against the nature of God (both in the Old Testament and New)\nThe bible shows it only causes trouble in marriage\nThis is really a weak argument.  The problem is that the Bible only tells us stories that are either edifying or interesting.  Yeah, it's going to talk about Abraham's difficulty with his wife and taking up a handmaiden as a wife.  But that story is both interesting and edifying.  The lesson here isn't against polygamy, but rather that we shouldn't marry a woman unless we marry her properly and with the right intentions.\nJacob's wives competed.  But the story there isn't to avoid polygamy, but to focus on God and make your marriages holy and godly.  If your marriages are focused on God instead of on selfish ambition, this won't happen!\nEtc.\nEach argument that says \"polygamy is wrong because it leads to dischord\" can also be used to say \"marriage is wrong\" for the same reasons.  Those are the marriages not focused on God--that is the source of their dischord.\nOn the contrary, if we look at the life of David, we see that he was a man highly loved by God and yet had multiple wives.  Some, however, use this very example to show how polygamy is wrong because his children fought (with each other and their parents).  However, can we blame this on David or his wives?  Absolutely not.\nGod only created one Eve for Adam\nNote that this was written by Moses, who had (at least) two wives\nThis is also a weak argument.  When God created Eve, he created her as a helper.  Whether or not he intended for this to be a marriage situation is of secondary concern and something that we cannot know 1 2.\nOn the contrary, though, God has given us a pattern to follow for marriage:\nEphesians 5:22-25\n\n22 Wives, submit yourselves to your own husbands as you do to the Lord. 23 For the husband is the head of the wife as Christ is the head of the church, his body, of which he is the Savior. 24 Now as the church submits to Christ, so also wives should submit to their husbands in everything.\n25 Husbands, love your wives, just as Christ loved the church and gave himself up for her\n\nWe are to model our marriages after the the relationship that Jesus has with the church, not after the helper relationship that Adam had with Eve.\nFurthermore, in the Parable of the Ten Virgins (Matthew 25:1-13), Jesus describes himself using an analogy of a bridegroom that is preparing to marry ten women.  Why would Jesus really compare himself to a polygamist if it were detestable to him?\n\"Elders should be men of one wife\"\nMany use Titus 1:6 and 1 Timothy 3:2,12 against polygamy.  They say that these verses talk about how deacons, overseers, and elders should be men of \"one wife\".\nPoint 1:  Simply because elders should be a \"man of one wife\", does not prevent polygamy among everyone else in the church.\nPoint 2: translation of this \"one wife\" is not as cut-and-dry as it may seem\nTo fully understand point 2, I want to compare these three passages to 1 Timothy 5:9:\n\nA widow is to be put on the list only if she is not less than sixty years old, having been the wife of one man\n\nThe greek for the word \"one\" here is heis. From Strongs Concordance 1520, this word is translated as one, single.\nHowever, if we look at Titus 1:6 and 1 Timothy 3:2,12, the word we see for \"one\" is mias.  This is the same word used in Matthew 28:1 for \"first\".\nWhile it's not definitive that mias should be translated as \"first\" instead of \"one\", it definitely shows that \"a man of one wife\" can have multiple translations.\nIn fact, let's look at the NIV version of this:\nTitus 1:6 (NIV)\n\nAn elder must be blameless, faithful to his wife, a man whose children believe and are not open to the charge of being wild and disobedient.\n\n\"faithful to his wife\"?  Why did they translate it that way?  Is it because they're trying to \"please the polygamy crowd\"?  Absolutely not!\nThe purpose of this passage is to indicate that the elders should be above reproach, faithful to his wife.  If you accept the mias as \"first\", it changes the translation of this passage to \"man of his first wife\", which means someone who's never been divorced!\nThat is the purpose of this passage: not to exclude polygamist, but to find men who should be held up as models for the community.  Men who are faithful and can maintain a marriage faithfully.\nTwo become one flesh\nNote that this was written by Moses, who had (at least) two wives\nGenesis 2:24\n\nThat is why a man leaves his father and mother and is united to his wife, and they become one flesh.\n\nThere are several passages that show that a man becomes \"one flesh\" with a woman regardless if it is his wife or a prostitute.\nThe idea of \"one flesh\" does not mean that the man is exclusive to that woman.  It means that they join themselves as if married.  This is true regardless of who the man does this with.\nThis is also clear in Ephisians 6:16-17 (NIV)\n\n16 Do you not know that he who unites himself with a prostitute is one with her in body? For it is said, \u201cThe two will become one flesh.\u201d 17 But whoever is united with the Lord is one with him in spirit.\n\nIt is clear that many believers can unite with the Lord as many prostitutes can unite with one man.\nFurthermore, in John Skinner's Genesis: International Critical Commentary (T & T Clarke, Edinburgh, 1930) p. 70, Skinner notes that the word for \"flesh\" here is synonymous for \"clan\" or \"family group\"--both in the Hebrew and Arabic.  This word for \"flesh\", in fact, is the same word used here in Leviticus 25:49\n\nAn uncle or a cousin or any blood relative in their clan may redeem them. Or if they prosper, they may redeem themselves.\n\nClearly, there is no restriction of one man becoming an exclusive single flesh with a woman.\nOther Arguments\nThere are many other arguments that can be heard and answered at BiblicalPolygamy.com\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "Note: Throughout, I use the word 'polygamy' in place of 'polygyny', even though I explicitly mean to polygyny."
      },
      {
        "type": "Claim",
        "text": "Polygamy is a sin because it goes against the law, which is a carryover from the paganistic societies of ancient Rome."
      },
      {
        "type": "Evidence",
        "text": "Preventing polygamy was not a biblical concept but one that came after Jesus, after authorship of the bible, and after the genesis of Christianity."
      },
      {
        "type": "Rule",
        "text": "Christians are told to uphold the law and subject themselves to the law (Romans 13:1-2)."
      },
      {
        "type": "Implication",
        "text": "Polygamy is considered a sin only if it breaks the law, similar to breaking a speed limit."
      },
      {
        "type": "Historical Reference",
        "text": "Polygamy was not uncommon among the Jews in biblical times but became outlawed under Roman influence."
      },
      {
        "type": "Analogy",
        "text": "The Parable of the Ten Virgins in the New Testament is used to illustrate the acceptance of polygamy during that time."
      },
      {
        "type": "Evidence",
        "text": "Greek and Roman rules against polygamy spread and became cultural norms, influencing the practice's decline."
      },
      {
        "type": "Citation",
        "text": "Monogamy and polygyny by Walter Scheidel, Stanford University (2009) provides insights into the historical impact of Greco-Roman practices on monogamy."
      },
      {
        "type": "Implication",
        "text": "Christianity's association with Roman monogamy led to the prohibition of polygamy in Christian societies."
      },
      {
        "type": "Claim",
        "text": "Polygamy is illegal in most parts of the world, making it a sin due to its legal status."
      },
      {
        "type": "Context",
        "text": "Catholicism continues to uphold the prohibition of polygamy, reflecting the influence of ancient Roman practices."
      },
      {
        "type": "Counterexample",
        "text": "Arguments against polygamy based on biblical texts are countered with interpretations and contextual considerations."
      },
      {
        "type": "Elaboration",
        "text": "Various arguments against polygamy are addressed, highlighting the complexities and interpretations of relevant biblical passages."
      }
    ]
  },
  {
    "site": "christianity",
    "question_title": "What evidence is there that Jesus was a carpenter?",
    "question_body": "I was about to say this in another discussion somewhere else, and wondered: do we know that?  As in \"it's mentioned or at least hinted at in $book $chapter:$verse\"?\nOr is it just a very likely conjecture?\n",
    "answer_body": "We know that Joseph was a \u03c4\u03ad\u03ba\u03c4\u03c9\u03bd (tekton)- traditionally a carpenter but literally, any craftsman who worked with his hands, from Matthew 13:55 (Is this not the carpenter's son?).  Traditionally, boys would follow in their father's occupation, hence the tradition that Jesus was a carpenter.\nAdditionally in Mark 6:3, we get the same word describing Jesus himself. (Is this not the carpenter?). Mark is probably the older source ( the Farrer hypothesis ) from which Matthew is drawing. With Matthew's desire to portray Jesus as king, it makes sense that he would prefer to move such a menial task to the dad and let people just assume that Jesus would have been trained up in the family trade. \n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "We know that Joseph was a \u03c4\u03ad\u03ba\u03c4\u03c9\u03bd (tekton)- traditionally a carpenter but literally, any craftsman who worked with his hands, from Matthew 13:55 (Is this not the carpenter's son?)."
      },
      {
        "type": "Context",
        "text": "Traditionally, boys would follow in their father's occupation, hence the tradition that Jesus was a carpenter."
      },
      {
        "type": "Evidence",
        "text": "Additionally in Mark 6:3, we get the same word describing Jesus himself. (Is this not the carpenter?)."
      },
      {
        "type": "Hypothesis",
        "text": "Mark is probably the older source (the Farrer hypothesis) from which Matthew is drawing."
      },
      {
        "type": "Elaboration",
        "text": "With Matthew's desire to portray Jesus as king, it makes sense that he would prefer to move such a menial task to the dad and let people just assume that Jesus would have been trained up in the family trade."
      }
    ]
  },
  {
    "site": "christianity",
    "question_title": "Why does Judaism celebrate Sabbath on Saturday and most Christians celebrate Sabbath on Sunday?",
    "question_body": "When did most Christians (possibly excluding exceptions such as Seventh Day Adventists) and Jews start celebrating Sabbath on different days of the week and why? \n",
    "answer_body": "To be clear, most Christians do not celebrate \"The Sabbath\" on Sunday. Strictly speaking, Christians do not celebrate the Sabbath at all* (although many Christians still refer to their \"day of rest\" as their \"sabbath day\", even though this has no direct relationship to the Jewish Sabbath.)  Christians traditionally celebrate on Sundays because this is the day Christ was raised from the grave.\nI have not yet found a specific, authoritative reference for the fact that Christians worship on Sunday because that is when Christ was raised, but some references include here and here\n*Apparently some Christian groups do call Sunday \"the Sabbath\" (with a capital S), but I believe this is a minority view out of ignorance of the significance of words, and is rarely considered dogma.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Definition",
        "text": "Most Christians do not celebrate 'The Sabbath' on Sunday."
      },
      {
        "type": "Caveat",
        "text": "Christians do not celebrate the Sabbath at all, although many refer to their 'day of rest' as their 'sabbath day'."
      },
      {
        "type": "Context",
        "text": "Christians traditionally celebrate on Sundays because this is the day Christ was raised from the grave."
      },
      {
        "type": "Evidence",
        "text": "I have not yet found a specific, authoritative reference for the fact that Christians worship on Sunday because that is when Christ was raised, but some references include here and here."
      },
      {
        "type": "Caveat",
        "text": "Some Christian groups do call Sunday 'the Sabbath' (with a capital S), but this is considered a minority view out of ignorance of the significance of words and is rarely considered dogma."
      }
    ]
  },
  {
    "site": "christianity",
    "question_title": "How were the books of the New Testament chosen?",
    "question_body": "I'm interested in finding out:\n\nwhat the process of choosing the books of the New Testament was, \nwhen did it occur,\nwho did the choosing, and\nwhat are the supporting evidences\n\n",
    "answer_body": "The canon developed gradually over the course of more than 300 years. In many cases, when decisions were made, they were simply to acknowledge what was already being read in the churches.\nThe process started early. Already in 2 Peter 3:16, there is a reference to the letters of Paul:\n\nThere are some things in them hard to understand, which the ignorant and unstable twist to their own destruction, as they do the other scriptures.\n\nSo at least some of the churches were already circulating Paul's letters and reading them as scripture.\nThe Gospels were written after Paul's letters, and the Apostolic Fathers quoted most often from Matthew, but also sometimes from Mark and Luke, and eventually from John.\nBy the late 2nd century, Irenaeus (Against Heresies 3.11.8) was claiming that the canon must contain exactly four gospels:\n\nIt is not possible that the Gospels can be either more or fewer in number than they are. For, since there are four zones of the world in which we live, and four principal winds, while the Church is scattered throughout all the world, and the \u201cpillar and ground\u201d of the Church is the Gospel and the spirit of life; it is fitting that she should have four pillars, breathing out immortality on every side, and vivifying men afresh.\n\nThis was to counter fringe groups that were producing their own gospels, as well as Marcion, who argued for just one gospel (Luke).\nWe don't know who compiled the list known as the Muratorian Fragment, but it also dates from the second century and contains this canon:\n\nFour gospels (the existing fragment begins by naming Luke as the third and John as the fourth)\nThe Book of Acts\nThirteen letters of Paul (and then names two letters attributed to Paul but not accepted as genuine)\nJude\nTwo letters from John\nRevelations \"of John and Peter\"\n\"the Wisdom written by friends of Solomon in his honour\"\n\nHebrews, James, 2 Peter, and 3 John were not yet accepted.\nThe fragment also recommends the Shepherd of Hermas as being worth reading but not qualifying for the canon because it was written \"quite lately in our time\".\nBy the early fourth century, the church historian Eusebius (Church History 3.25) sorts the known early writings into cateogories.\nThe first category is:\n\nFour Gospels plus Acts\n\nNext in importance are:\n\nThe letters of Paul\nOne letter each from John and Peter\n\nThese are the writings that are universally accepted by the church. Eusebius adds that some believe Revelation should be included in this set.\nThe next category includes books that are disputed by some:\n\nJames\nJude\n2 and 3 John\n2 Peter\n\nThese would all eventually be added to the canon.\nThe next category includes books that rejected from the canon but worth reading:\n\nActs of Paul\nThe Shepherd\nApocalypse of Peter\nLetter from Barnabas\nThe Teaching of the Apostles\n\nHe mentions that some would place Revelation in this group, and others would include the Gospel of the Hebrews.\nAnd finally, Eusebius mentions other writings that are considered heretical, which he says should be \"cast aside as absurd and impious.\" These include the gospels of Peter and Thomas, among others.\nThe first known list of canon that matches today's New Testament is found in the Easter Letter of Athanasius for the year 367:\n\nAgain it is not tedious to speak of the [books] of the New Testament. These are, the four Gospels, according to Matthew, Mark, Luke, and John. Afterwards, the Acts of the Apostles and Epistles (called Catholic), seven, viz. of James, one; of Peter, two; of John, three; after these, one of Jude. In addition, there are fourteen Epistles of Paul, written in this order. The first, to the Romans; then two to the Corinthians; after these, to the Galatians; next, to the Ephesians; then to the Philippians; then to the Colossians; after these, two to the Thessalonians, and that to the Hebrews; and again, two to Timothy; one to Titus; and lastly, that to Philemon. And besides, the Revelation of John.\n\nAthanasius then adds that other books\u2014the Wisdom of Solomon, the Shepherd, the Teaching of the Apostles, et al.\u2014should be read by new converts. So even as the canon was being defined, there was recognition that among the excluded books were some that were part of the church teaching, and some that were not.\nFollowing Athanasius, we see the beginning of a general agreement throughout the church. The Synod of Hippo in 393 published a list of New Testament books identical to Athanasius' list, but also included an Old Testament canon. The Council of Carthage of 397 published a similar list, but is notable for separating Hebrews from the letters of Paul. The Latin Vulgate translation, commissioned by Pope Damasus in 382 and completed by Jerome in 405, contains the same set of books listed in these. The Decretum Gelasianum, widely thought to be associated with the same Pope Damasus, lists the same 27 books but makes a distinction among the letters of John (\"of the Apostle John, one letter, of the other John the Elder, two letters\").\nThe Council of Carthage of 419 lists the same books, but does not make a distinction between two writers named John, and does not separate Hebrews from the letters of Paul.\nIn the East, there would still be disagreement for more than a century. The Council of Laodicea of 364 accepted all of the current New Testament books except Revelation. The Apostolic Constitutions, compiled around 380, has a Bible canon including this same list, but adding three writings attributed to Clement of Rome.\nThe first version of the Peshitta, the Syriac translation of the Bible, omitted the books of 2 Peter, 2-3 John, Jude, and Revelation; these books would not be translated into Syriac until the 6th century.\nThe Quinisext Council or Council in Trullo was held in Constantinople in 692 to reaffirm the rulings of the Fifth and Sixth Ecumencal Councils; while this council did not publish a formal Bible canon, it did reaffirm the canons previously published, including those of Athanasius and others with the Book of Revelation. This set the stage for the 7th Ecumenical Council (Second Council of Nicea) in 787, which finally established what we now know as the 27-book New Testament canon for the entire church.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "The canon developed gradually over the course of more than 300 years."
      },
      {
        "type": "Historical Reference",
        "text": "In many cases, when decisions were made, they were simply to acknowledge what was already being read in the churches."
      },
      {
        "type": "Evidence",
        "text": "The process started early. Already in 2 Peter 3:16, there is a reference to the letters of Paul."
      },
      {
        "type": "Claim",
        "text": "At least some of the churches were already circulating Paul's letters and reading them as scripture."
      },
      {
        "type": "Claim",
        "text": "By the late 2nd century, Irenaeus was claiming that the canon must contain exactly four gospels."
      },
      {
        "type": "Counterexample",
        "text": "This was to counter fringe groups that were producing their own gospels, as well as Marcion, who argued for just one gospel (Luke)."
      },
      {
        "type": "Evidence",
        "text": "The first known list of canon that matches today's New Testament is found in the Easter Letter of Athanasius for the year 367."
      },
      {
        "type": "Summary",
        "text": "Following Athanasius, we see the beginning of a general agreement throughout the church on the New Testament canon."
      },
      {
        "type": "Historical Reference",
        "text": "The Synod of Hippo in 393 and the Council of Carthage of 397 published lists of New Testament books."
      },
      {
        "type": "Comparison",
        "text": "The Council of Carthage of 397 is notable for separating Hebrews from the letters of Paul."
      },
      {
        "type": "Comparison",
        "text": "The Council of Carthage of 419 lists the same books as the Council of Carthage of 397 but does not make a distinction between two writers named John."
      },
      {
        "type": "Historical Reference",
        "text": "The Quinisext Council or Council in Trullo in 692 reaffirmed the canons previously published, setting the stage for the 7th Ecumenical Council in 787."
      }
    ]
  },
  {
    "site": "linguistics",
    "question_title": "Is there a language where there are personal pronouns for the first or second person that have gender?",
    "question_body": "Is there a language where there are personal pronouns for the first or second person that have gender? Like a feminine \"I\" or a masculine \"you\".\n",
    "answer_body": "In Thai, 1st person singular pronouns differ by gender:\n\nMasc.: \u0e1c\u0e21 [p\u02b0\u01d2m]\nFem.: \u0e14\u0e34\u0e09\u0e31\u0e19 [d\u00ec\u0294t\u0361\u0255\u02b0\u00e1n]\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "In Thai, 1st person singular pronouns differ by gender:"
      },
      {
        "type": "Example",
        "text": "Masc.: \u0e1c\u0e21 [p\u02b0\u01d2m]"
      },
      {
        "type": "Example",
        "text": "Fem.: \u0e14\u0e34\u0e09\u0e31\u0e19 [d\u00ec\u0294t\u0361\u0255\u02b0\u00e1n]"
      }
    ]
  },
  {
    "site": "linguistics",
    "question_title": "Is there a word in a dead or lost language that we lost the definition to?",
    "question_body": "Is there a word we lost the definition to? A word whose definition we lost to history? Something that is a part of our history but we forgot the meaning with time\n",
    "answer_body": "Ancient Greek word \u03a3\u0391\u03a3\u03a4\u0397\u03a1 (sast\u0113r)\nFrom 1890 to 1899, in pieces, a white marble slab was found by archaeologists in the ruins of an Ancient Greek colony Chersonesus, Greek \u03a7\u03b5\u03c1\u03c3\u03cc\u03bd\u03b7\u03c3\u03bf\u03c2 (Khers\u00f3n\u0113sos), on the Crimean Peninsula, established in the 6th century BC. The slab (photo) was inscribed with a text in Ancient Greek being the civic oath of the Chersonesites (citizens of Chersonesus): description, Ancient Greek text and English translation. The text is thought to be inscribed in the beginning of the 3rd century BC.\nAmong the understandable oaths (\u201cI will not betray anything to anyone, neither a Hellenic nor a barbarian,\u201d \u201cI will not violate democracy,\u201d \u201cI will not plot a conspiracy,\u201d \u201cI will be an enemy to malefactors\u201d) there is one: \u201cI will protect the saster (\u03a3\u0391\u03a3\u03a4\u0397\u03a1) for the people\" (\u03ba\u03b1\u03b9 \u03c4\u03bf\u03bd \u03c3\u03b1\u03c3\u03c4\u03b7\u03c1\u03b1 \u03c4\u03c9\u03b9 \u03b4\u03b1\u03bc\u03c9\u03b9 \u03b4\u03b9\u03b1\u03c6\u03c5\u03bb\u03b1\u03be\u03c9, lines 24\u201325).\nThis word is not found in any other Greek text of the Old or New time. The literature on saster is extensive. There are many hypotheses, including some very eccentric ones. Max Fasmer and Lev Yelnitsky, for example, believed that the saster was the Scythian governor of Chersonesos, S. A. Zhebelev \u2014 that it was some kind of sacred object, for example an idol; V.V. Latyshev (the first publisher of the inscription) - that this is a kind of legal concept, for example, a civil oath. Most recently, I. Markov argues it is the city treasury. Parallels were sought for this word in Iranian and other languages. Historical novels appeared, which featured the sacred saster towering over the Chersonesus coast; in Sevastopol, a modern city situated near the ancient Chersonesus, a festival called \"Saster of Chersonesos\" was held. On the Internet, you can listen to a song with the words \"And I will find a magic saster\" (with an emphasis on \"a\") and read verses with the line \"An unknown saster hiding from us\" (with an emphasis on \"e\").\nNo one knows for sure what \u03a3\u0391\u03a3\u03a4\u0397\u03a1 is.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": []
  },
  {
    "site": "linguistics",
    "question_title": "What are the historical origins of terms for north, south, east and west?",
    "question_body": "In the course of researching the etymology of the word \"Australia\", I was trying to find the Latin words for north and south (the cardinal directions). I found some websites that translate north as \"Septentrionalis\", but I understand this to refer to the seven oxen, or what we today call the Big Dipper, as it is in the northern sky. Other websites translate north as \"boreas\" or \"aquilon\" though I think that the former is actually a Greek wind god and the later is his Roman name.\nNow of course in English the words north, south, east, and west have no transparent etymologies; they are just the words for the directions. But it seems that Latin and Greek had direction words that were derived from other things (gods, stars, oxen, winds). \nThis line of thought leads to a question: historically and cross-linguistically, how did terms for the cardinal directions arise? Or differently stated, is it the case that the cardinal direction terms are derived from similar processes across different language families?\n",
    "answer_body": "Direction words arise from the need to coordinate direction. Thus, their nature and usage can vary widely from one language to another. To start, here are a few aspects of a people and their land that can influence the form that direction words / direction morphemes end up taking:\n\nThe types of activities requiring coordination (migration, herding, hunting, fishing, gathering, farming, trade relations, war, etc.)\nThe geographical features of the area (coastline, a central river, tributaries, woods, islands, steppe, mountains).\nThe current state of inherited cultural knowledge about direction (including mythology; this may largely go back to (1) and (2) from prior states of the language).\n\nThe systems that thus arise may be influenced to be similar to the common cardinal directions in English, or instead make the primary distinction \"upstream\" vs. \"downstream,\" \"inland\" vs. \"toward the coast,\" or \"direction towards\" vs. \"away from a certain very distant landmark.\" Taken beyond that landmark, a person might not be as effective at communicating navigation. (Although, technically, the same thing can happen in English: explain which way to go from the North Pole!)\nIn Orientation Systems of the North Pacific Rim by Michael Fortescue, a close examination of the orientation systems of Wakashan, Tsimshianic, Haida, Tlingit, Eyak, Athabaskan, Eskimo-Aleut, Tungusic, Nivkh, and Ainu leads to the enticing conclusion that by conceiving of a language's orientation system diachronically, looking at etymologies of direction words and inspecting any inherently paradoxical methods of expressing direction, we can evidence a hypothesis as to whether a language is relatively new to a region (say, having arried within the past 2000 years), as well as perhaps find out where they came from!\nFor example, if people in a dialect continuum settle a turning peninsula, the meaning of a direction word at the source may be different from at the tip. Similarly, whereas most IE languages use the PIE root *aus- \"to shine, dawn\" for \"east,\" the Latin word australis \"south\" may be from the same root. This is \"perhaps is based on a false assumption about the orientation of the Italian peninsula, 'with shift through \"southeast\" explained by the diagonal position of the axis of Italy'...Or perhaps the connection is more ancient, and [directly] from PIE root *aus- 'to shine,' source of aurora, which also produces words for 'burning,' with reference to the 'hot' south wind that blows into Italy.\" OE\nTo address your question about English (all quotations from the Online Etymology Dictionary, OE):\n\nnorth < nor\u00f0 < *nurtha possibly derives from PIE *ner- \"left, below\" \"as north is to the left when one faces the rising sun (cf. Skt. narakah 'hell,' Gk. enerthen 'from beneath,' Oscan-Umbrian nertrak 'left').\" OE\nsouth < su\u00f0 < *sunthaz is \"perhaps related to base of *sunnon 'sun,' with sense of \"the region of the sun.\" OE\neast < east < *aus-to-, *austra-, \"from PIE *aus- 'to shine,' especially 'dawn' (cf. Skt. ushas 'dawn,' Gk. aurion 'morning,' O.Ir. usah, Lith. auszra 'dawn,' L. aurora 'dawn,' auster 'south'), lit. 'to shine.'\" OE\nwest < west < *wes-t- \"from PIE *wes- (source of Gk. hesperos, L. vesper 'evening, west'), perhaps an enlarged form of root *we- 'to go down' (cf. Skt. avah 'downward'), and thus lit. 'direction in which the sun sets.'\" OE\n\nSome languages use non-compound words for the ordinal (secondary) directions. For example, Finnish (with help from Wiktionary and Finnish Wikipedia):\n\nluode \"northwest,\" possibly the same etymology as identical luode meaning \"ebb / low tide,\" loan from a Germanic language, cognate of German Flut, Swedish flod.\npohjoinen \"north,\" from pohja \"bottom,\" is because the sun is in the north when it's underneath the horizon, possibly also because the back of a dwelling should be facing the north so as to maximize warmth.\nkoillinen \"northeast,\" from koi \"dawn.\"\nit\u00e4 \"east,\" possibly related to it\u00e4\u00e4 \"to germinate,\" that the sun grows in the east.\nkaakko \"southeast,\" equated with kaakkuri \"red-throated diver (loon)\" and kuikka \"black-throated diver (loon).\" Compare Latin ornithias \"bird-wind,\" the spring wind that brings the birds.\netel\u00e4 \"south,\" antonymously to pohjois, is the direction in which the front of the house should face. Compare eteen \"to the front.\" The Estonian cognate edel means \"southwest.\"\nlounas \"southwest, lunch\" indicates the direction the sun is in at lunchtime. The Estonian cognate l\u00f5una means \"south.\"\nl\u00e4nsi \"west\" may have to do with the day being \"flattened\" (l\u00e4nt\u00e4t\u00e4\u00e4n ~ litistet\u00e4\u00e4n) as the evening arrives.\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Direction words arise from the need to coordinate direction. Thus, their nature and usage can vary widely from one language to another."
      },
      {
        "type": "Elaboration",
        "text": "To start, here are a few aspects of a people and their land that can influence the form that direction words / direction morphemes end up taking:"
      },
      {
        "type": "List",
        "text": [
          {
            "type": "Example",
            "text": "The types of activities requiring coordination (migration, herding, hunting, fishing, gathering, farming, trade relations, war, etc.)"
          },
          {
            "type": "Example",
            "text": "The geographical features of the area (coastline, a central river, tributaries, woods, islands, steppe, mountains)."
          },
          {
            "type": "Example",
            "text": "The current state of inherited cultural knowledge about direction (including mythology; this may largely go back to (1) and (2) from prior states of the language)."
          }
        ]
      },
      {
        "type": "Implication",
        "text": "The systems that arise may be influenced to be similar to the common cardinal directions in English or take different primary distinctions based on the specific needs of the people and the land."
      },
      {
        "type": "Example",
        "text": "For example, if people in a dialect continuum settle a turning peninsula, the meaning of a direction word at the source may be different from at the tip."
      },
      {
        "type": "Citation",
        "text": "In Orientation Systems of the North Pacific Rim by Michael Fortescue, a close examination of various language orientation systems provides insights into the historical and geographical influences on direction words."
      },
      {
        "type": "Hypothesis",
        "text": "By looking at etymologies of direction words and paradoxical methods of expressing direction, we can hypothesize whether a language is relatively new to a region or trace its origins."
      },
      {
        "type": "Comparison",
        "text": "Comparing the orientation systems of different languages can reveal patterns and connections, such as the evolution of direction words over time."
      },
      {
        "type": "Elaboration",
        "text": "The etymologies of direction words in English and other languages provide insights into the cultural and historical contexts that shaped these terms."
      },
      {
        "type": "Example",
        "text": "For instance, the origins of English direction words like north, south, east, and west are traced back to PIE roots and cultural associations with celestial and geographical phenomena."
      },
      {
        "type": "Comparison",
        "text": "Some languages use non-compound words for ordinal directions, showcasing different linguistic strategies for expressing spatial orientation."
      },
      {
        "type": "Example",
        "text": "Finnish direction words like luode, pohjoinen, koillinen, it\u00e4, kaakko, etel\u00e4, lounas, and l\u00e4nsi demonstrate unique etymologies and cultural interpretations of cardinal and ordinal directions."
      }
    ]
  },
  {
    "site": "linguistics",
    "question_title": "Is there a difference between an affricate and a plosive+fricative consonant cluster?",
    "question_body": "Is there a difference between an affricate and a plosive+fricative consonant cluster?\nAccording to wikipedia, there is a difference between a plosive+fricative sequence, as in the following example\n\ncatch it /k\u00e6t\u0361\u0283.\u026at/\ncat shit /k\u00e6t.\u0283\u026at/\n\nBut I honestly can't hear the phonetic difference unless the speaker carefully puts a pause between the morphological boundaries.\nThe semicircle in the IPA /t\u0361\u0283/ seems to suggest that /t/ and /\u0283/ are coarticulated, or at least articulated together more than in a typical consonant cluster.\nIs there any difference between the affricate /t\u0361\u0283/ and the cluster /t\u0283/? (similarly with /d\u026e/, /p\u032af/, /\u02a5/, and so on) If so, what's the difference, and how could one tell the difference between an affricate and a simple plosive+fricative sequence?\n",
    "answer_body": "\nBut I honestly can't hear the phonetic difference unless the speaker carefully puts a pause between the morphological boundaries.\n\nYou have very good instincts, because this statement is halfway to the answer. An exploration of the topic is best started in a discussion of the differences between the words shoe and chew, which differ only in that the former has a fricative initially and the latter has an affricate.\nThe period of silence (caused by voiceless stop closures like [t]) that characterizes stops and the \"stop\" portion of affricates is indistinguishable from regular silence. We only ever \"identify\" the presence of \"stoppiness\" as well as the nature of the stoppiness (e.g., bilabial vs. velar) by the influences that the silences have on neighboring segments.\nIn the case of the postalveolar fricative versus the postalveolar affricate, in the postalveolar affricate, the stop asserts its status by altering the nature (non-technical term used here; I will elaborate) of the following fricative.\nLet's compare my totally amateur recordings of shoe versus chew.\nShoe\n\nChew\n\nWhat do you notice?\nThere are two major differences that linguists have narrowed down as the cues our ears and minds use to differentiate shoe from chew:\n\nThe fricative is longer in duration than the affricate in general.\nThe loudest point in the fricative occurs much later as a proportion of total fricative length compared to the affricate.\n\nYou can even test this out for yourself by doing some basic audio editing in Praat or Audacity.\nWhen our brains hear catch it or cat shit, it is applying these two metrics to figure out which one it heard. Slow ascension to maximum amplitude and relatively long frication noise? Must be shit.\nThis only really tells part of the story, though. Let's explore the production side a little.\nWhen an oral stop happens, the tongue (or lips) completely constrict the oral pathway. No air (and consequently sound, to an extent) can get in or out. That being said, in the transition process when a non-obstruent like a fricative or an oral sonorant follows a stop in the same syllable (this does not apply all the time, but the story is really complicated), what happens is that in the last few dozen milliseconds air starts coming out from the bottom of the vocal tract. Air pressure behind the stops exceeds air pressure outside the stop. And when the stop is released, air molecules quickly crack from behind the stop to in front of it relatively quickly (on releases of all oral stops, aspirated or not, especially into vowels, you can feel a marked burst of air on your lips or if you put your hand in front of your mouth). This is what we call the stop burst.\nNow, many different sorts of things can happen following the stop burst. In the case of the fricatives after stops, the closure relaxes slightly, causing the air to burst, but it remains relatively tight, and air traveling turbulently as it goes through and comes out of that tight corridor is what causes frication noise.\nSo the story of the affricate (stop into fricative) is that significantly higher than atmospheric pressure builds up behind the closure. When the stop bursts and the tongue goes into constriction position for the fricative, that high pressure of air is released over the early span of the fricative, causing a high amount of noise.\nCompare this to what happens when you pronounce a bare fricative. In those cases (e.g., shoe), your tongue goes into fricative position (small constriction), and then your lower vocal tract just starts pushing air through the constriction. The noise does not begin amidst extremely high pressure behind the constriction. That lack of an air pressure gradient causes it to be quieter.\nThe quicker climb of the affricate case causes a \"critical noise/vibration\" level to be reached more quickly and the tongue to retreat from its constriction position, hence the shorter duration of fricatives in general (the critical level hypothesis is not that thoroughly explored to my knowledge, but it seems to explain a lot). \nSo that's the story of fricative versus affricate.\nBring it all the way back to your case, what happens when there's a \"syllable break\" is important. When there is a syllable break after the stop, the pressure buildup behind the stop closure releases (sometimes audibly, depending on the dialect). Then, the tongue assumes the fricative constriction position during a period of neutral pressure, and then the air starts flowing, and the sound occurs as a regular fricative.\nI'm sure this is poorly edited, so I welcome volunteers who would be kind enough to correct me on all the errors I may have made.\n@aedia asked in a comment a very interesting question that I'll address as an edit:\n\nperhaps try something like at shoe vs. achoo \n\nThat's actually a slightly different case in my mind. The prosodic characteristics of at (being a very weak unit) in at shoe may cause segmentation issues. While I still haven't read anything that's totally convinced me that phonemic affricates exist in English, I suspect that proponents of phonemic affricates might argue that the resegmented form is proof that /t\u0283/ the cluster and /t\u0283/ the affricate are distinct. Rambling aside, this doesn't appear to prevail in my dialect and the fricative in at shoe emerges like a vanilla fricative:\nAt shoe\n\nAchoo\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "But I honestly can't hear the phonetic difference unless the speaker carefully puts a pause between the morphological boundaries."
      },
      {
        "type": "Elaboration",
        "text": "You have very good instincts, because this statement is halfway to the answer. An exploration of the topic is best started in a discussion of the differences between the words shoe and chew, which differ only in that the former has a fricative initially and the latter has an affricate."
      },
      {
        "type": "Explanation",
        "text": "The period of silence (caused by voiceless stop closures like [t]) that characterizes stops and the 'stop' portion of affricates is indistinguishable from regular silence. We only ever 'identify' the presence of 'stoppiness' as well as the nature of the stoppiness (e.g., bilabial vs. velar) by the influences that the silences have on neighboring segments."
      },
      {
        "type": "Comparison",
        "text": "Let's compare my totally amateur recordings of shoe versus chew."
      },
      {
        "type": "Visual Description",
        "text": "Shoe\\n\\nChew"
      },
      {
        "type": "Question",
        "text": "What do you notice?"
      },
      {
        "type": "Claim",
        "text": "There are two major differences that linguists have narrowed down as the cues our ears and minds use to differentiate shoe from chew:"
      },
      {
        "type": "Elaboration",
        "text": "The fricative is longer in duration than the affricate in general.\\nThe loudest point in the fricative occurs much later as a proportion of total fricative length compared to the affricate."
      },
      {
        "type": "Procedure",
        "text": "You can even test this out for yourself by doing some basic audio editing in Praat or Audacity."
      },
      {
        "type": "Implication",
        "text": "When our brains hear catch it or cat shit, it is applying these two metrics to figure out which one it heard."
      },
      {
        "type": "Elaboration",
        "text": "This only really tells part of the story, though. Let's explore the production side a little."
      },
      {
        "type": "Elaboration",
        "text": "When an oral stop happens, the tongue (or lips) completely constrict the oral pathway. No air (and consequently sound, to an extent) can get in or out."
      },
      {
        "type": "Elaboration",
        "text": "Now, many different sorts of things can happen following the stop burst. In the case of the fricatives after stops, the closure relaxes slightly, causing the air to burst, but it remains relatively tight, and air traveling turbulently as it goes through and comes out of that tight corridor is what causes frication noise."
      },
      {
        "type": "Elaboration",
        "text": "So the story of the affricate (stop into fricative) is that significantly higher than atmospheric pressure builds up behind the closure."
      },
      {
        "type": "Elaboration",
        "text": "Compare this to what happens when you pronounce a bare fricative. In those cases (e.g., shoe), your tongue goes into fricative position (small constriction), and then your lower vocal tract just starts pushing air through the constriction."
      },
      {
        "type": "Elaboration",
        "text": "The quicker climb of the affricate case causes a 'critical noise/vibration' level to be reached more quickly and the tongue to retreat from its constriction position, hence the shorter duration of fricatives in general."
      },
      {
        "type": "Elaboration",
        "text": "Bring it all the way back to your case, what happens when there's a 'syllable break' is important."
      },
      {
        "type": "Question",
        "text": "I'm sure this is poorly edited, so I welcome volunteers who would be kind enough to correct me on all the errors I may have made."
      },
      {
        "type": "Question",
        "text": "@aedia asked in a comment a very interesting question that I'll address as an edit:"
      },
      {
        "type": "Comparison",
        "text": "That's actually a slightly different case in my mind."
      }
    ]
  },
  {
    "site": "linguistics",
    "question_title": "Is future tense in English really a myth?",
    "question_body": "Does English really have two tenses - present and past? Some linguists argue that it is a Latinate fallacy to think that English has three tenses.\nSome English professors and even some native speakers do not accept the proposition.\nIf it is true, why are the standard grammar books published by Cambridge and Oxford Publications still mentioning the term future tense? Is it not misleading the learners?\nCan we call the two sentences given below present continuous?\n\n\"I am working here today.\"\n\"I will be working here tomorrow.\"\n\nI have doubts regarding others forms too.\nMy question is: Does English really have only two tenses?\nI hope the answer will be comprehensive.\n",
    "answer_body": "\nDoes English really have only two tenses.\n\nIt depends how you define \"tense\", but to most linguists, yes.\nAll languages can mark the time when an event occurs, to any degree of specificity you want. You can say \"I played a game\", or \"I played a game yesterday\", or \"I played a game at 11:35am on September 4th\", and so on.\nLinguists generally only call it \"tense\" (sometimes \"morphological tense\", though that's not quite the same thing) when this marking is mandatory. For example, if I played a game at 11:35am on September 4th, then \"I played a game yesterday\" is perfectly correct (as of the time of writing), and \"I played a game\" is also fine. But that -ed is mandatory for past-tense verbs in English: that part can't be left off. As soon as I switch to \"I play a game\", the meaning has changed significantly.\nAnd in English, the only mandatory morphological distinction is between past and non-past. It's conventional to use \"will\" to mark events happening in the future, but you can also have future meanings without it: how about \"I'm going to play a game\"? Or \"after I play this game [I'm going to go get pizza]\"? In both cases, the game-playing will happen in the future, but no \"will\" is required. The only thing that is required is using a non-past form, since the event is non-past: we can't say *\"after I played this game [I'm going to go get pizza]\". (The star before it is linguistics shorthand for \"this isn't valid\".)\nThis is why some languages, like Mandarin, are said to be tenseless. Mandarin is certainly capable of expressing whether an event happens in the past, present, or future. But this marking is not mandatory: it's entirely optional, like whether to include \"\u2026yesterday\" or \"\u2026tomorrow\" in English. So linguists say Mandarin has no (morphological) tenses at all.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Question",
        "text": "Does English really have only two tenses."
      },
      {
        "type": "Claim",
        "text": "It depends how you define 'tense', but to most linguists, yes."
      },
      {
        "type": "Explanation",
        "text": "All languages can mark the time when an event occurs, to any degree of specificity you want. You can say 'I played a game', or 'I played a game yesterday', or 'I played a game at 11:35am on September 4th', and so on."
      },
      {
        "type": "Definition",
        "text": "Linguists generally only call it 'tense' (sometimes 'morphological tense', though that's not quite the same thing) when this marking is mandatory."
      },
      {
        "type": "Example",
        "text": "For example, if I played a game at 11:35am on September 4th, then 'I played a game yesterday' is perfectly correct (as of the time of writing), and 'I played a game' is also fine."
      },
      {
        "type": "Explanation",
        "text": "But that -ed is mandatory for past-tense verbs in English: that part can't be left off. As soon as I switch to 'I play a game', the meaning has changed significantly."
      },
      {
        "type": "Explanation",
        "text": "And in English, the only mandatory morphological distinction is between past and non-past."
      },
      {
        "type": "Example",
        "text": "It's conventional to use 'will' to mark events happening in the future, but you can also have future meanings without it: how about 'I'm going to play a game'? Or 'after I play this game [I'm going to go get pizza]'?"
      },
      {
        "type": "Explanation",
        "text": "In both cases, the game-playing will happen in the future, but no 'will' is required. The only thing that is required is using a non-past form, since the event is non-past."
      },
      {
        "type": "Caveat",
        "text": "We can't say *'after I played this game [I'm going to go get pizza]'. (The star before it is linguistics shorthand for 'this isn't valid'.)"
      },
      {
        "type": "Explanation",
        "text": "This is why some languages, like Mandarin, are said to be tenseless. Mandarin is certainly capable of expressing whether an event happens in the past, present, or future. But this marking is not mandatory: it's entirely optional, like whether to include '\u2026yesterday' or '\u2026tomorrow' in English."
      },
      {
        "type": "Claim",
        "text": "So linguists say Mandarin has no (morphological) tenses at all."
      }
    ]
  },
  {
    "site": "fitness",
    "question_title": "I have extremely bad posture, what can I do?",
    "question_body": "I work 9 till 5 on my pc, then I go home, use my pc, then play a game or two.\nMy problem is I'm always sitting in my chair and my posture is starting to get really bad! People say \"just sit up straight and it will sort itself out\" and thats impossible!\nI'm a programmer, so I sit up straight and 30 seconds later I'm hunched over and don't even realise it!\nWhat can I do? Certain stretches? Exercises?\nI go swimming allot and I am in shape with a perfect BMI.\n",
    "answer_body": "You can do stretches, strengthening and breathing exercises to improve your posture.\nBut you also have to become aware of your posture and find a way to interrupt prolonged sitting intervals with brief breaks.  A good ergonomic chair is also helpful.\nMusculo-Skeletal Effects of Poor Sitting Posture:\nPoor sitting posture can create muscle imbalances over time.\n\nIn general, your front stuctures shorten, with muscles like your hip flexors and pectorals tightening up - pulling you into a ball.  Your crunched sitting posture caves your lower ribs down towards your abdomen which prevents the diaphragm from expanding freely. \nAt the same time the muscles of your back stretch out and weaken, especially in the range where they should be working to maintain your posture.\n\nAs this posture persists over time, you joints begin to lose normal range of motion as well making it more difficult to assume a good posture.\nCorrecting Slumped Sitting Posture:\nTailor an exercise program to stretch tightened muscles and strengthen weakened muscles.  Additionally, you need a method to become aware of your posture and correct it while you are sitting.  This is difficult because as you say, your attention is on your work. Exercise programs like Yoga, Tai-Chi and Pilates are good because they all address and make you very aware of your posture, joint alignments, flexibility, core control and breathing.\nUse Specific Exercises to Correct Muscle Imbalances:\n\nBack, Upper Back and Scapular muscles:  Back Extensions strengthen your paraspinals.  Use different arm positions (Y, T, W, L) to target your lower traps, mid traps, rhomboids and scapular stabilizors.  Learn the feel of retracting your scapulas.\nYou can do these on the floor next to your desk. \nOr if you prefer not to get on the floor, use resistance bands for reverse flys, wide rows, narrow rows and rotations. \nAway from work, you can also strengthen these muscles using weights, cables and body weight exercises like inverted rows, cable rows, bent over rows, reverse flys etc.  And squats are a good for strengthening multiple muscles important to posture.\nCore:  Plank, Side Plank, Bird Dog and Bridge will stabilize your trunk and spine.\nStretches: - Hip Flexors, Hamstrings, Abs and Pecs.  The wall pec stretch stretches the pecs but also contracts the rhomboid and trapezius scapular muscles to help improve the positioning of the shoulder and shoulder blade and is easy to do at work. \nSitting Posture and Breathing Awareness - This sitting posture exercise helps make you aware of your sitting alignment and expanding your diaphragm.  Practice a few diaphramatic breaths throughout the day along with this sitting exercise to elongate your spine.\n\nCreate your ideal short exercise routine that you can do during short breaks during the day. An exercise ball next to your desk can help you target these muscles in just a few minutes:\n\nLie back over the ball and stretch out your front \nLie face down over the ball and do the Y, W, T, L exercises.\nAnd consider using the ball as your desk chair for short periods.\n\nRemembering your Posture throughout the day:\nThis is the hard part.  As your muscle imbalances begin to resolve and your diaphragmatic breathing improves, you\u2019ll find that the slouched posture becomes less comfortable and you will automatically begin to sit better.  Until then, use a timed reminder or try tying your posture corrections to tasks that you do at the computer regularly.  For example, correct your posture each time you check your email or some other specific link.  Turn on your web cam for visual reminders.\nKeep at it until you feel the improvement.  You'll have less problems going forward if you improve your posture.  It either gets better - or it gets worse.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Procedure",
        "text": "You can do stretches, strengthening and breathing exercises to improve your posture."
      },
      {
        "type": "Procedure",
        "text": "But you also have to become aware of your posture and find a way to interrupt prolonged sitting intervals with brief breaks."
      },
      {
        "type": "Claim",
        "text": "A good ergonomic chair is also helpful."
      },
      {
        "type": "Context",
        "text": "Musculo-Skeletal Effects of Poor Sitting Posture:"
      },
      {
        "type": "Elaboration",
        "text": "Poor sitting posture can create muscle imbalances over time."
      },
      {
        "type": "Elaboration",
        "text": "In general, your front structures shorten, with muscles like your hip flexors and pectorals tightening up - pulling you into a ball."
      },
      {
        "type": "Elaboration",
        "text": "Your crunched sitting posture caves your lower ribs down towards your abdomen which prevents the diaphragm from expanding freely."
      },
      {
        "type": "Elaboration",
        "text": "At the same time the muscles of your back stretch out and weaken, especially in the range where they should be working to maintain your posture."
      },
      {
        "type": "Cause/Effect",
        "text": "As this posture persists over time, your joints begin to lose normal range of motion as well making it more difficult to assume a good posture."
      },
      {
        "type": "Procedure",
        "text": "Correcting Slumped Sitting Posture:"
      },
      {
        "type": "Procedure",
        "text": "Tailor an exercise program to stretch tightened muscles and strengthen weakened muscles."
      },
      {
        "type": "Procedure",
        "text": "Additionally, you need a method to become aware of your posture and correct it while you are sitting."
      },
      {
        "type": "Comparison",
        "text": "Exercise programs like Yoga, Tai-Chi and Pilates are good because they all address and make you very aware of your posture, joint alignments, flexibility, core control and breathing."
      },
      {
        "type": "Procedure",
        "text": "Use Specific Exercises to Correct Muscle Imbalances:"
      },
      {
        "type": "Elaboration",
        "text": "Back, Upper Back and Scapular muscles: Back Extensions strengthen your paraspinals."
      },
      {
        "type": "Elaboration",
        "text": "Core: Plank, Side Plank, Bird Dog and Bridge will stabilize your trunk and spine."
      },
      {
        "type": "Elaboration",
        "text": "Stretches: - Hip Flexors, Hamstrings, Abs and Pecs."
      },
      {
        "type": "Procedure",
        "text": "Sitting Posture and Breathing Awareness - This sitting posture exercise helps make you aware of your sitting alignment and expanding your diaphragm."
      },
      {
        "type": "Procedure",
        "text": "Create your ideal short exercise routine that you can do during short breaks during the day."
      },
      {
        "type": "Procedure",
        "text": "Remembering your Posture throughout the day:"
      },
      {
        "type": "Elaboration",
        "text": "As your muscle imbalances begin to resolve and your diaphragmatic breathing improves, you\u2019ll find that the slouched posture becomes less comfortable and you will automatically begin to sit better."
      },
      {
        "type": "Procedure",
        "text": "Until then, use a timed reminder or try tying your posture corrections to tasks that you do at the computer regularly."
      },
      {
        "type": "Elaboration",
        "text": "Keep at it until you feel the improvement. You'll have less problems going forward if you improve your posture."
      }
    ]
  },
  {
    "site": "fitness",
    "question_title": "Ensuring the body burns fat instead of muscle?",
    "question_body": "When exercising, our bodies take energy from our blood sugar, then uses glycogen as a source for energy, and finally burns fat or muscle when the glycogen is used up.\nA nutritionist told me that whether your body will start burning muscles or fat depends on the timing in which the two sources before (blood sugar and glycogen) were burnt. If this process take the proper time, then it will burn fat as desired.\nI would like to know how to make sure we will burn fat when working out instead of muscles.\n",
    "answer_body": "Energy metabolism is not a very well understood system in the sense that while the biochemical reactions are well known, their dynamics is highly variable depending on the individual. I find it disturbing that so many people have their own understanding of how their body work, without any sound reasoning behind it. Below I'll try to give some background information to the chemistry of it. \nThe biochemistry behind it is essentially very complicated and is often over-simplified. The truth is, different parts of the body use different sources for energy. A common example is the brain, which can ONLY use glucose as the energy source. \nTo argue against @camara90100's post, ATP is NOT an energy source but instead an energy carrier. ATP molecule is carries three phosphate groups as its name suggests. By breaking the these bonds (i.e. ATP -> ADP + P) energy is released which is used in some other reaction in the body. When the body \"burns\" sugars, or anything else, it uses the energy to synthesize more ATP molecules, or to reverse the original reaction. \nWhether or not lactic acid is produced from breaking sugars is dependent on the oxygen supply to the surrounding tissue, if you cannot supply the tissue with enough oxygen a less than optimal reaction will take place where one of the byproducts is lactic acid. Lactic acid build-up in the tissue will ultimately lead to \"cramps\" as your body is telling you stop what you are doing since your metabolism cannot keep up with the physical activity you put yourself through. \nFurther more, there is an interplay between simple sugars and complex ones (carbs), as well as between carbs and fat. Excess blood sugar is processed in the liver to produce glycogen which is a long-term-storage of sugars. However glycogen is not the only way to store fuel, evolutionarily we are developed to \"store energy\" in case food becomes scarce. In that sense it's important to understand that fat is not a undesired trash molecule, but a perfectly healthy part of the metabolism. I recall reading some article on a critical limit on body fat index and normal brain function, where the authors have discussed individuals with extremely low body fat percentage were performing less than average on intellectual tasks. \nLong story short, I don't believe that you can \"guarantee\" that you're burning only fat and no proteins during some training, especially considering that all these reactions I've described (and many more) have different rates on different individuals. Individuals with higher metabolic rates will end up breaking down muscle tissue through physical training instead of building muscle mass if they cannot keep up with the food intake. So I suggest you look over your diet so that you do not consume excess amounts of fat or carbs, and plan your training so that it matches with your own metabolic rate. \nPS: Sorry for the long post but I hope it helps people get a better grasp of things.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Energy metabolism is not a very well understood system in the sense that while the biochemical reactions are well known, their dynamics is highly variable depending on the individual."
      },
      {
        "type": "Claim",
        "text": "I find it disturbing that so many people have their own understanding of how their body work, without any sound reasoning behind it."
      },
      {
        "type": "Summary",
        "text": "Below I'll try to give some background information to the chemistry of it."
      },
      {
        "type": "Elaboration",
        "text": "The biochemistry behind it is essentially very complicated and is often over-simplified."
      },
      {
        "type": "Example",
        "text": "A common example is the brain, which can ONLY use glucose as the energy source."
      },
      {
        "type": "Counterexample",
        "text": "To argue against @camara90100's post, ATP is NOT an energy source but instead an energy carrier."
      },
      {
        "type": "Explanation",
        "text": "ATP molecule carries three phosphate groups as its name suggests. By breaking these bonds (i.e. ATP -> ADP + P) energy is released which is used in some other reaction in the body."
      },
      {
        "type": "Cause/Effect",
        "text": "Whether or not lactic acid is produced from breaking sugars is dependent on the oxygen supply to the surrounding tissue."
      },
      {
        "type": "Implication",
        "text": "Lactic acid build-up in the tissue will ultimately lead to 'cramps' as your body is telling you to stop what you are doing since your metabolism cannot keep up with the physical activity you put yourself through."
      },
      {
        "type": "Comparison",
        "text": "There is an interplay between simple sugars and complex ones (carbs), as well as between carbs and fat."
      },
      {
        "type": "Elaboration",
        "text": "Excess blood sugar is processed in the liver to produce glycogen which is a long-term-storage of sugars."
      },
      {
        "type": "Claim",
        "text": "Fat is not an undesired trash molecule, but a perfectly healthy part of the metabolism."
      },
      {
        "type": "Evidence",
        "text": "I recall reading some article on a critical limit on body fat index and normal brain function, where the authors have discussed individuals with extremely low body fat percentage were performing less than average on intellectual tasks."
      },
      {
        "type": "Implication",
        "text": "Individuals with higher metabolic rates will end up breaking down muscle tissue through physical training instead of building muscle mass if they cannot keep up with the food intake."
      },
      {
        "type": "Recommendation",
        "text": "So I suggest you look over your diet so that you do not consume excess amounts of fat or carbs, and plan your training so that it matches with your own metabolic rate."
      }
    ]
  },
  {
    "site": "fitness",
    "question_title": "What is the purpose of 'cooling down'?",
    "question_body": "After a hard work out, it's generally a good idea to not stop immediately, but rather to cool down.  What is the specific purpose of cooling down?  Does not properly cooling down affect the actual efficiency of the work out itself?\n",
    "answer_body": "According to Physiology of Sports and Exercise:\n\nEvery endurance exercise session should conclude with a cool-down period. Cool-down is best accomplished by slowly reducing the intensity of the endurance activity during the last several minutes of your workout. After running, for example, a slow, restful walk for several minutes helps prevent blood from pooling in your extremities. Stopping abruptly after an endurance exercise bout causes blood to pool in your legs and can result in dizziness or fainting.\n\nAlso you should remember that intensive exercising effects your entire body: your heart is pumping, your lungs are ventilating, your blood is racing through your body, your muscles are contracting, your liver is producing energy. If you were to suddenly stop, your body will slightly lag behind your abrupt change in exercise intensity (or the lack thereof). \nWhen you've been working out near the lactate threshold, your body has also been piling some amount of lactate. If you lower the intensity of your workout, such that you get enough oxygen, your muscles will start using lactate as a fuel and get rid of it. Because burning lactate creates a lot of waste products like CO2 and creatine kinase, it's advisable to keep up some level of activity, so that your body can get rid of it easily.\nDuring workouts, your body releases hormones such as adrenaline and endorphins, if you were to abruptly stop exercising, you maintain higher levels of these hormones which can cause a feeling of restlessness or a sleepless night.\nAnother good reason is that when you exercise, your muscles stretch and shorten a lot. If you were to stop abruptly, you leave your muscles in a somewhat misaligned state. By cooling down, you gradually limit the range of motion back to the state it would be in when you're in rest.\nSo basically, the goal of cooling down is giving your body time to readjust itself to the change in requirements. \nHowever, a study by Law and Herbert from the university of Sydney indicated that cooling down did not reduce delayed-onset muscle soreness. Which might indicate that this isn't directly related to residual waste products. Furthermore, Tanaka, an exercise physiologist from the University of Texas claims it's an understudied topic and that there's no science behind the advice.\nI'd like to point out that my while my answer is based on the physiological processes going on during exercise, I don't have any publications to back them up. Lucky for me, neither has he.\n",
    "audience_level": "Intermediate",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Cool-down: Concluding phase of an endurance exercise session involving a gradual reduction in activity intensity to aid in the body's recovery process."
      },
      {
        "type": "Procedure",
        "text": "To cool down effectively, slowly reduce the intensity of the endurance activity during the last several minutes of the workout."
      },
      {
        "type": "Example",
        "text": "After running, a slow, restful walk for several minutes can prevent blood from pooling in the extremities."
      },
      {
        "type": "Implication",
        "text": "Abruptly stopping after an endurance exercise bout can lead to blood pooling in the legs, potentially causing dizziness or fainting."
      },
      {
        "type": "Elaboration",
        "text": "Intensive exercise affects various bodily functions such as heart rate, lung ventilation, blood circulation, muscle contraction, and energy production."
      },
      {
        "type": "Implication",
        "text": "Sudden cessation of intense exercise can cause the body to lag behind in adjusting to the change in activity level."
      },
      {
        "type": "Implication",
        "text": "Lowering exercise intensity post-workout can help muscles utilize lactate as fuel and eliminate waste products like CO2 and creatine kinase."
      },
      {
        "type": "Implication",
        "text": "Maintaining some level of activity post-exercise aids in the efficient removal of waste products generated during intense workouts."
      },
      {
        "type": "Implication",
        "text": "Abruptly stopping exercise can lead to elevated levels of hormones like adrenaline and endorphins, potentially causing restlessness or sleep disturbances."
      },
      {
        "type": "Implication",
        "text": "Cooling down gradually helps muscles return to their resting state by gradually limiting the range of motion."
      },
      {
        "type": "Claim",
        "text": "Cooling down may not directly reduce delayed-onset muscle soreness, as indicated by a study by Law and Herbert from the University of Sydney."
      },
      {
        "type": "Claim",
        "text": "The effectiveness of cooling down is debated, with some experts like Tanaka from the University of Texas suggesting it is an understudied topic lacking scientific evidence."
      },
      {
        "type": "Qualification",
        "text": "The information provided is based on physiological processes during exercise, but lacks specific citations or publications for reference."
      }
    ]
  },
  {
    "site": "fitness",
    "question_title": "I am underweight. How do I gain weight and muscle?",
    "question_body": "I am a man in my twenties. Although I am 5 feet 6 inches (167 cm) tall, I weigh only 100 pounds (45 kg). I want to gain weight as soon as possible. How do I gain weight and muscle as fast as possible?\n",
    "answer_body": "To gain good weight, to bulk, to add muscle, you need to:\n\nStimulate growth by lifting heavy\nProvide fuel for growth by eating a lot\nPrioritize your goal by getting your life in order\n\nMost healthy people who do these things gain weight. Mostly muscle.\n1. LIFT HEAVY\nTell your body that it needs to get bigger by lifting heavy. \nEither buy a barbell and a power rack, or join a gym that has one. Get a copy of Starting Strength (the wiki is a good overview and quick-start guide; the book is a full description of the program, including excellent instructions on the lifts) and start lifting heavy. Compound exercises like squats, chin-ups, deadlifts, and presses will stimulate whole-body growth. Light, easy weights won't make you bigger or stronger, so while it's important to stay safe, make sure you're lifting heavy, challenging weights. Lifting three times per week is probably the best compromise between frequent exercise and ample rest.\n2. EAT BIG\nProvide your body the raw materials it needs to make you bigger.\nEat a ton of food. Real food is far superior to processed crap, but you'll need to eat a lot. Your best bets are high-animal-protein items like meat, eggs, milk, and fish, but you should also make sure to eat a huge amount of vegetables, greens, starches such as sweet potatoes and rice, and good fats like pastured butter, coconut oil, olive oil, and avocado.\nIf you're ever hungry, you're not eating enough. When in doubt, eat more. Lots more. Plan your meals. Cook in advance.\n3. PRIORITIZE\nThere are things which get in the way of growing muscle. Decide if getting bigger and stronger is actually your goal. It's okay if it's not. \nThings which can hamper your getting-bigger-and-stronger goal include:\n\nNot sleeping enough\nNot sticking to the heavy-lifting-and-big-eating program detailed above\nEndurance exercise or high-intensity conditioning, which could include running, cycling, swimming, long hikes, snowboarding, metcons, sprinting, HIIT, ball sports...\nBeing too stressed, working too much, not getting enough sun or social life\nBeing a picky eater\nRefusing to acquire necessary equipment\n\nRest is crucial. Sleep is the primary time for your body to grow. Staying up talking with friends is what makes life enjoyable, but six hours of sleep will keep you from growing. The body also does a lot of growing on days off from lifting, so don't fill those up with other exercise. Certain types of exercise are more prone preventing muscle gain than others. Running is great--I love sprints!--but it doesn't help make me bigger. \nI want to get bigger and stronger, but sometimes I also want to play Ultimate frisbee or go hiking. When I'm serious about getting bigger, I skip the hiking or keep it short, and I don't play Ultimate. When I'm okay with progressing slowly, I go ahead and play Ultimate and go on longer hikes, but I realize that they are counterproductive to the sole pursuit of getting bigger and stronger.\nThe same goes for food. I value food quality. I vastly prefer organic vegetables, local produce, and grass-fed meat and eggs for a variety of economic, ethical, and health reasons. For lunch at work, I need to choose between planning ahead and cooking beforehand, getting a factory-farm-meat sandwich from the deli, or going hungry and stymieing my growth. There are similar choices for vegetarians and people with other food restrictions.\nMany people are short on money or space, and wonder if there are alternatives to a barbell and squat rack. The simple fact is that barbells are best for getting bigger and stronger. Other methods like dumbbells or even bodyweight exercises definitely work, but a barbell and squat rack is the simplest and fastest solution. Why? First, it can be loaded in small increments, so you can progressively challenge yourself without big jumps in weight. Second, barbells allow for much heavier loads than anything else. Without proper equipment, progress is slower and less effective.\nUnderstand these choices and make them for yourself.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Summary",
        "text": "To gain good weight, to bulk, to add muscle, you need to stimulate growth by lifting heavy, provide fuel for growth by eating a lot, and prioritize your goal by getting your life in order."
      },
      {
        "type": "Procedure",
        "text": "1. LIFT HEAVY: Tell your body that it needs to get bigger by lifting heavy. Compound exercises like squats, chin-ups, deadlifts, and presses will stimulate whole-body growth. Lifting three times per week is probably the best compromise between frequent exercise and ample rest."
      },
      {
        "type": "Procedure",
        "text": "2. EAT BIG: Eat a ton of food, focusing on high-animal-protein items, vegetables, greens, starches, and good fats. Plan your meals and cook in advance to ensure you're eating enough."
      },
      {
        "type": "Procedure",
        "text": "3. PRIORITIZE: Identify and address factors that may hinder muscle growth, such as lack of sleep, not following the lifting and eating program, excessive endurance exercise, stress, and inadequate rest. Make choices that align with your goal of getting bigger and stronger."
      },
      {
        "type": "Elaboration",
        "text": "Rest is crucial for muscle growth, and certain types of exercise and food choices can either support or hinder your goal. Understanding these factors and making informed decisions is essential."
      }
    ]
  },
  {
    "site": "fitness",
    "question_title": "Should I drink water before, during or after eating?",
    "question_body": "As the title says, I would like to know what is the best time to drink water, is it before, during, or after having a meal?\nAlso suppose the answer was during, what quantity is recommended?\n",
    "answer_body": "Here's an excerpt from a blog post I did a while back addressing this question: \n\nThe \u201cplug\u201d in the bottom of your\n  stomach is a stoma not a valve. It can\n  be pushed open, so when you drink\n  during a meal the liquid can push the\n  food out of your stomach pouch and\n  down into your intestines. There are\n  two reasons you should care:\nThis frees up extra room in your\n  stomach pouch so you\u2019ll eat more\n  during that meal. You\u2019ll feel hungry\n  again sooner.\nA recommendation that I\u2019ve stumbled\n  across is the 15/30 rule. Don\u2019t drink\n  anything from 15 minutes before a meal\n  until 30 minutes after a meal. This\n  isn\u2019t a license to under-hydrate, just\n  a guideline on when to drink.\n\n\nUpdate: The comment thread has indicated a desire for \"proof\" (pedants! :-) ), so here is some reference information and some reasoning built on top of that in support of the above hypothesis.\nFirst, the amount of fluid in the stomach is positively correlated with the rate of gastric emptying. It so happens that 30 minutes is an approximate inflection point in the gastric emptying of both a solid and liquid meal, per the chart below (same source).\n\nI'll assert that drinking water will render the net contents of the stomach more fluid, moving the gastric emptying rate from the blue curve towards the red curve. \nThis seems to confirm the conclusion that avoiding liquid during the meal will help stave off the next bout of hunger because a more solid meal will take longer to empty. The 15 minutes before also seems to fit, as roughly half of liquid consumed 15 minutes beforehand will have been emptied before the meal starts.\nIf one further considers the dynamics, drinking water immediately before a meal will help fill up the stomach and reduce immediate hunger but will cause that meal to be digested faster, so I hypothesize that you will be hungrier sooner with this approach.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Here's an excerpt from a blog post addressing the question about the impact of drinking during a meal on stomach pouches and hunger levels."
      },
      {
        "type": "Elaboration",
        "text": "The 'plug' in the bottom of the stomach is a stoma, not a valve, which can be pushed open to allow liquid to push food out of the stomach pouch and into the intestines."
      },
      {
        "type": "Implication",
        "text": "Drinking during a meal can free up space in the stomach pouch, leading to increased food intake during the meal and potentially causing hunger to return sooner."
      },
      {
        "type": "Procedure",
        "text": "A recommendation mentioned is the 15/30 rule, advising against drinking anything from 15 minutes before a meal until 30 minutes after a meal."
      },
      {
        "type": "Qualification",
        "text": "The 15/30 rule is not meant to promote dehydration but rather serves as a guideline for optimal timing of fluid intake."
      },
      {
        "type": "Evidence",
        "text": "Fluid in the stomach positively correlates with the rate of gastric emptying, with 30 minutes being a critical point for both solid and liquid meals."
      },
      {
        "type": "Hypothesis",
        "text": "Drinking water during a meal can make stomach contents more fluid, potentially accelerating gastric emptying and leading to quicker hunger onset."
      },
      {
        "type": "Implication",
        "text": "Avoiding liquid during a meal may help delay hunger by slowing down the emptying process, especially for more solid meals."
      }
    ]
  },
  {
    "site": "law",
    "question_title": "Can I legally include a line break in my child's name?",
    "question_body": "Ethics aside, is it legal (or even possible) to include a line break (newline) in my child's name? Preferably at the end of the child's first name, directly after the last letter. So instead of (for example) the name \"John Doe\", the name would always be written out \"John Doe\". And then when the first name only is written out the line break would have to be included, such as \"My child's first name is John and his last name is Doe\". If this is legal, how would I go about making sure the line break is included on the name section of the birth certificate?\n",
    "answer_body": "In the US, any legal restrictions on names are implemented at the state level\u2014\u2014although broad administrative restrictions exist on the federal level. Some states may restrict use of diacritics (ubiquitous in Vietnamese) or Arabic numerals (but not Roman numerals). At the other extreme, in Washington state, there is no requirement to include a name at all in the case of live birth of known parentage. In the case of delayed report of live birth, and \"An individual requesting the delayed report of live birth of an individual under twelve years of age must establish the facts concerning full name, date, and place of live birth\". But no restrictions are imposed on names that can be so reported.\nTheoretically, one could attempt to register a child with the name \ud806\udc13\ud806\udc33\ud806\udc22 (in the Dogra script), which would cause technical problems for the registrar's office. It is likely that the clerk taking in the form would respond something along the lines of \"Huh?\" and \"How do you spell that\". Similarly, one might try to register a birth name Ho\u00e0ng Ph\u1ee7 Ng\u1ecdc T\u01b0\u1eddng, which would not be particularly difficult to deal with but might still stress the system (it depends on the county). In the latter case the name might be quietly converted to Hoang Phu Ngoc Tuong. In the former case, it is virtually guaranteed that the clerk would have no recourse but to insist on a romanization. Then the person registering would be insistent, they would file a lawsuit, and the courts would make some decision. It is most likely that the courts would be sympathetic to the practical concerns of the registrar and would not demand a huge overhaul of computer systems to allow any arbitrary graphic representation as a legal name.\nThe State Department has regulations regarding names at 8 FAM 403:\nPersonally Identifying Information. 8 FAM 403.1-3(C) addresses punctuation, special characters and symbols, diacritical marks, and non-Latin alphabets. They do not prohibit anything in names, instead they acknowledge that not everything is supported, and there is a long discussion of \"discrepancies\" which would explain the passport name \"Nyema\" for \ud806\udc13\ud806\udc33\ud806\udc22. Passport names comply with the\nInternational Civil Aviation Organization standard. Social Security has a different set of rules where spaces, numbers, hyphens, slashes or any other special characters are not allowed for names, even including length limits where first, middle and last names can be maximally 10, 7 and 13 characters long (enter the first 10, 7 and 13 characters).\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "In the US, any legal restrictions on names are implemented at the state level\u2014\u2014although broad administrative restrictions exist on the federal level."
      },
      {
        "type": "Example",
        "text": "Some states may restrict use of diacritics (ubiquitous in Vietnamese) or Arabic numerals (but not Roman numerals)."
      },
      {
        "type": "Example",
        "text": "In Washington state, there is no requirement to include a name at all in the case of live birth of known parentage."
      },
      {
        "type": "Procedure",
        "text": "In the case of delayed report of live birth, an individual requesting the delayed report of live birth of an individual under twelve years of age must establish the facts concerning full name, date, and place of live birth."
      },
      {
        "type": "Caveat",
        "text": "No restrictions are imposed on names that can be reported in the case of delayed birth."
      },
      {
        "type": "Analogy",
        "text": "Theoretically, one could attempt to register a child with a complex name like \ud806\udc13\ud806\udc33\ud806\udc22 (in the Dogra script), causing technical problems for the registrar's office."
      },
      {
        "type": "Procedure",
        "text": "If faced with such a complex name, the registrar's office might ask for clarification or insist on a romanization."
      },
      {
        "type": "Implication",
        "text": "Courts would likely consider practical concerns of the registrar when dealing with complex names and may not demand extensive system changes for every unique name."
      },
      {
        "type": "Context",
        "text": "The State Department has regulations regarding names at 8 FAM 403."
      },
      {
        "type": "Elaboration",
        "text": "The regulations address punctuation, special characters, diacritical marks, and non-Latin alphabets in names."
      },
      {
        "type": "Comparison",
        "text": "Passport names comply with the International Civil Aviation Organization standard, while Social Security has different rules prohibiting special characters and imposing length limits on names."
      }
    ]
  },
  {
    "site": "law",
    "question_title": "If a signal person in California tells me to kill someone, do I have to do so?",
    "question_body": "According to California sample written driving test (problem 6):\n\n\nYou see a signal person at a road construction site ahead. You should obey his or her instructions:\n\nA. Only if you see orange cones on the road ahead.\nB. Unless they conflict with existing signs, signals, or laws.\nC. At all times.\n\nC is the correct answer and B is marked as incorrect. So could a signal person tell me to kill someone? If they did and I did so, would I still get in trouble? I am following their instructions, and the test explicitly states that I should follow their instructions even if they conflict with existing laws.\n",
    "answer_body": "You are not reading a law book here and you should not interpret a driving test so literally. It's quite clear that the question implies you should follow all of their instructions regarding how to proceed through traffic. Sometimes those instructions do involve \"breaking laws\" such as driving on the wrong side of the road or proceeding through a traffic signal that was not turned off. The B option clearly does not mean they have the power to disobey all laws in existence, only those concerning traffic as evidenced by the examples given.\nYou are not Sheldon Cooper and you should know how to interpret a vague question correctly. You are also not a gopher, and you can correctly deduce that crashing into another car or driving off the cliff into the water is not in your best interests, and that calling the police to report someone abusing their position is probably a good idea.\nIf you're concerned by the wording, try contacting the California DMV to have them clarify the wording.\n",
    "audience_level": "Expert",
    "tone": "Casual",
    "blocks": [
      {
        "type": "Context",
        "text": "You are not reading a law book here and you should not interpret a driving test so literally."
      },
      {
        "type": "Emphasis",
        "text": "It's quite clear that the question implies you should follow all of their instructions regarding how to proceed through traffic."
      },
      {
        "type": "Example",
        "text": "Sometimes those instructions do involve 'breaking laws' such as driving on the wrong side of the road or proceeding through a traffic signal that was not turned off."
      },
      {
        "type": "Claim",
        "text": "The B option clearly does not mean they have the power to disobey all laws in existence, only those concerning traffic as evidenced by the examples given."
      },
      {
        "type": "Analogy",
        "text": "You are not Sheldon Cooper and you should know how to interpret a vague question correctly."
      },
      {
        "type": "Analogy",
        "text": "You are also not a gopher, and you can correctly deduce that crashing into another car or driving off the cliff into the water is not in your best interests."
      },
      {
        "type": "Implication",
        "text": "Calling the police to report someone abusing their position is probably a good idea."
      },
      {
        "type": "Procedure",
        "text": "If you're concerned by the wording, try contacting the California DMV to have them clarify the wording."
      }
    ]
  },
  {
    "site": "law",
    "question_title": "Can I enter a rental property without giving notice if I'm afraid a tenant may be hurt?",
    "question_body": "I rent a \u201cMother-In-Laws quarters\u201d (guest area) that is on the side of my house. It has a private entrance.\nIf I'm concerned something has happened to the tenant, am I legally allowed to enter the building without giving notice?\nSome background: For the first time, my tenant is late on rent, and hasn't contacted me in any way which seems very unusual for her.\nYesterday I texted once to remind about the rent, and a second time asking if they are okay once I noticed there was still mail from a few days ago in her mail box right by her door with no response.\nI also couldn't hear TV through the wall which I usually can, and her curtains were never opened which I notice she usually does during the day. The air conditioning also wasn't on all day, which is also unusual. She's elderly and I'm afraid she may have fallen or even worse.\nI'm planning on knocking on her door on my lunch break. If she does not answer the door, I'm trying to figure out if I can go ahead and enter the property and make sure everything is okay.\n",
    "answer_body": "Most leases have a provision allowing a landlord to make entry without notice in an emergency, but the better course of action, as noted in a comment by @BlueDogRanch, is to call the police and ask them to make a \"welfare check.\" You would ordinarily be permitted to cooperate with police by unlocking doors in furtherance of their welfare check.\nThe police are trained to do this properly in a way that properly balances the need to aid someone who is sick or ill, the need to preserve evidence if there was a death or crime that needs to be understood legally, and to protect the legitimate privacy interests of the tenant.\nYou are not. You could incur liability for failing to prevent death or aggravating injury,  could be wrongfully implicated if physical evidence from you contaminates the scene or you destroy evidence showing the true cause, and could be sued for invading the tenant's privacy if it was found that you entry was unreasonable and that it wasn't really an emergency, which is always easier to conclude with 20/20 hindsight.\nAs it is, your biggest potential source of liability is delaying in calling the police seeking a welfare check. They often respond quite quickly to these by the way, although it is not the very highest priority for law enforcement.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Most leases have a provision allowing a landlord to make entry without notice in an emergency."
      },
      {
        "type": "Claim",
        "text": "The better course of action is to call the police and ask them to make a 'welfare check.'"
      },
      {
        "type": "Example",
        "text": "You would ordinarily be permitted to cooperate with police by unlocking doors in furtherance of their welfare check."
      },
      {
        "type": "Elaboration",
        "text": "The police are trained to properly balance aiding someone who is sick or ill, preserving evidence in case of death or crime, and protecting the tenant's privacy."
      },
      {
        "type": "Caveat",
        "text": "You could incur liability for failing to prevent death or aggravating injury, wrongfully implicating yourself, or being sued for invading the tenant's privacy."
      },
      {
        "type": "Implication",
        "text": "Delaying in calling the police seeking a welfare check is a potential source of liability."
      },
      {
        "type": "Qualification",
        "text": "It is easier to conclude with hindsight that an entry was unreasonable and not really an emergency."
      },
      {
        "type": "Context",
        "text": "Calling the police for a welfare check is often not the very highest priority for law enforcement."
      }
    ]
  },
  {
    "site": "law",
    "question_title": "How does NC's atheism prohibition fit with the 1st Amendment of the US Constitution?",
    "question_body": "In 1971, the US State of North Carolina rewrote its constitution. According to this source, one of the purposes of this was that \"Ambiguities and sections seemingly in conflict with the U.S. Constitution were either dropped or rewritten.\"\nYet, the NC Constitution Article VI states:\n\nThe following persons shall be disqualified for office:\nFirst, any person who shall deny the being of Almighty God.\n\nThis seems to be somewhat in conflict with the freedom of religion nominally guaranteed by the first amendment to the US constitution, as a barrier to religious practice of community/political representatives. Is it? If not, why not?\n",
    "answer_body": "In Torcaso v. Watkins, 367 U.S. 488 (1961), the US Supreme Court ruled unanimously that a similar provision in Maryland's constitution violated the First Amendment and could not be enforced.  So presumably the North Carolina provision is similarly unconstitutional and unenforceable.\nIt's not clear why it wasn't removed in 1971.\nI found references to a 2009 incident in which an avowed atheist named Cecil Bothwell was elected to the Asheville, NC city council.  Opponents apparently threatened to mount a legal challenge to his eligibility under the Article VI provision.  It's not clear if they actually tried to do so, but in any event, Bothwell served his full four-year term and was then re-elected for another.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Historical Reference",
        "text": "In Torcaso v. Watkins, 367 U.S. 488 (1961), the US Supreme Court ruled unanimously that a similar provision in Maryland's constitution violated the First Amendment and could not be enforced."
      },
      {
        "type": "Claim",
        "text": "So presumably the North Carolina provision is similarly unconstitutional and unenforceable."
      },
      {
        "type": "Qualification",
        "text": "It's not clear why it wasn't removed in 1971."
      },
      {
        "type": "Evidence",
        "text": "I found references to a 2009 incident in which an avowed atheist named Cecil Bothwell was elected to the Asheville, NC city council."
      },
      {
        "type": "Context",
        "text": "Opponents apparently threatened to mount a legal challenge to his eligibility under the Article VI provision."
      },
      {
        "type": "Qualification",
        "text": "It's not clear if they actually tried to do so, but in any event, Bothwell served his full four-year term and was then re-elected for another."
      }
    ]
  },
  {
    "site": "law",
    "question_title": "Can a public school in the USA force a 14yr old to create a Twitter account for a passing grade?",
    "question_body": "My youngest is a freshman & signed up for a program the school refers to as \"co-lab\". At my insistence.\n2yrs ago, I attended a board meeting where the formation of the program was introduced. At the time, I was jumping out of my skin, I was so excited.it incorporates 5-6 subjects into 1 5-6hr block.\nWithout getting into the boring details, the bottom line is what they proposed vaguely resembles what is in play. \nAll the subjects are still being taught, but separately instead of seamlessly together. \nHe has As & Bs in all but 1 subject  - Digital Literacy. The sole reason being that the school expects him to create a public Twitter account, with his real information, in order to promote the program & the results of the program. \nThe results being, if I understand correctly, their personality, family history/tree, how their life experiences have shaped them into who they are today. These details were NOT part of the initial proposal nor part of anything sent home to me, to my knowledge. \nOn 2 separate occasions, during school functions I attended, his reluctance to do this step has come up. He has stated that he does NOT want to create an account on FB or  Twitter. He's not comfortable with that. No questions asked, I turned back to the teacher & said  - you have your answer. I will not force him to do that. \nYesterday, I received a call from the teacher regarding this, after reviewing his progress report which shows him failing that subject. She explained his failing grade was due to his resistance in creating the acct.\nHe tried to compromise with them & created a fake acct using a nickname a teacher had given him in 8th grade. (HE informed me of that, not the teacher). During the call, not knowing he had attempted that already, I suggested that as a compromise. But they won't accept that. Their claim is that it needs to be seen by the public, he needs to create a digital footprint, yadda, yadda. \nWhen asked how this would affect his grades in the other, now separate, subjects, she could not give me an answer. \nThis class is ONLY available for a freshman. It will have nothing to do with his next 3yrs of education. Yet his information will forever be out there even if never utilized or deleted after.\nThere's a parallel story to this which has bugged me, but this is the crux of THIS situation. \nMy question is, is this public school allowed to force him to create a digital footprint on an already worldwide controversial platform, POSSIBLY holding his entire freshman year over his head? At the very least, forcing him to retake the one class, which may end the same way?\nGiven the current environment, I'm one pissed off Mama bear. And he's ready to cave to the bullies, something I drilled into both my boys to never do.\nThoughts?\n",
    "answer_body": "You say:\n\nthe school expects him to create a public Twitter account, with his real information, in order to promote the program & the results of the program. \n\nThis is a cut-and-dried case of compelled speech. Your son is being required to say certain things in public in order to pass this course.\nThe Supreme Court has decided that students do not \"shed their constitutional rights to freedom of speech or expression at the schoolhouse gate\". This is subject to the legitimate interests of the school, but it is hard to see how compelled speech on Twitter can be defended as a legitimate interest.\nYou also say this is to \"promote the program and the results of the program\". It sounds like the students are being required to say certain things about the course. If your son were to create the account and then post only material critical of the school, such as complaints about compelled speech, would that result in a passing grade? It sounds like it might be an issue.\nCompelled speech at school was considered by the Supreme Court in West Virginia State Board of Education v. Barnette (1943), which is the case about students being required to salute the flag. They found that requiring students to salute the flag was unconstitutional.\n\nPublic education, according to the Court, should \u201cnot strangle the free mind at its source [or] teach youth to discount important principles of our government as mere platitudes.\u201d Instead, education should enable students to make informed choices about what to believe. \n\nFrom the judgement itself:\n\nthe refusal of these persons to participate in the ceremony does not interfere with or deny rights of others to do so. Nor is there any question in this case that their behavior is peaceable and orderly. The sole conflict is between authority and rights of the individual.\n\nThat is very much the case here. Your son's refusal to make the required public speech is not causing any difficulty for the school authorities.\nIn conclusion, your son has made a decision not to create a Twitter account under his own name and say certain things within that account. This is entirely his constitutional right.\nYou might consider contacting the American Civil Liberties Union (ACLU), who have a history of engaging in cases like this. The Electronic Frontier Foundation (EFF) might also be interested.\nEdit:\nZack Lipton in comments below makes the point that much student work can be considered a kind of compelled speech (e.g. \"Write an essay on Hamlet\" or \"Submit an entry to this poetry competition\") and asks how this is different. Its a good question, and I have to say it does suggest that there is a difference of degree rather than kind. However I would argue that posting to an international forum widely used by adults is a different matter to a school assembly, or even a national essay competition. It would also depend a great deal on what has to be posted to get a passing grade.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "The school expects him to create a public Twitter account, with his real information, in order to promote the program & the results of the program."
      },
      {
        "type": "Claim",
        "text": "This is a cut-and-dried case of compelled speech. Your son is being required to say certain things in public in order to pass this course."
      },
      {
        "type": "Historical Reference",
        "text": "Compelled speech at school was considered by the Supreme Court in West Virginia State Board of Education v. Barnette (1943), which is the case about students being required to salute the flag."
      },
      {
        "type": "Implication",
        "text": "The Supreme Court has decided that students do not 'shed their constitutional rights to freedom of speech or expression at the schoolhouse gate'."
      },
      {
        "type": "Elaboration",
        "text": "Public education, according to the Court, should 'not strangle the free mind at its source [or] teach youth to discount important principles of our government as mere platitudes.' Instead, education should enable students to make informed choices about what to believe."
      },
      {
        "type": "Evidence",
        "text": "From the judgement itself: the refusal of these persons to participate in the ceremony does not interfere with or deny rights of others to do so. Nor is there any question in this case that their behavior is peaceable and orderly. The sole conflict is between authority and rights of the individual."
      },
      {
        "type": "Qualification",
        "text": "It does suggest that there is a difference of degree rather than kind. However, posting to an international forum widely used by adults is a different matter to a school assembly, or even a national essay competition."
      },
      {
        "type": "Application",
        "text": "You might consider contacting the American Civil Liberties Union (ACLU), who have a history of engaging in cases like this. The Electronic Frontier Foundation (EFF) might also be interested."
      }
    ]
  },
  {
    "site": "history",
    "question_title": "Were there women who were against giving women the right to vote?",
    "question_body": "The TV show Sherlock prompted an interesting question about the historical role of women during the United Kingdom's road to women's suffrage.\nIn the episode \"The Abominable Bride\" (set in Victorian London in 1895), there was a scene where Mrs. Watson says that she is part of a campaign to give women the right to vote:\n\nMRS WATSON: I\u2019m part of a campaign, you know.\nLESTRADE: Oh yeah? Campaign?\nMRS WATSON: Votes for Women.\nLESTRADE: And are you \u2013 are you for or against?\nMRS WATSON (sternly pointing to the stairs): Get out.\n\nThe scene was played for laughs with the joke being that Lestrade should have realized his question was misplaced; being a woman of course she would want the right to vote.\nBut this made me wonder; were there women in this era who historically were against giving women the right to vote? If so, what were their reasons?\nI am interested in either organized campaigns led by women or prominent women who held this view. I'd like to know about UK movements, but similar movements outside of the UK would be interesting supplementary information too.\n",
    "answer_body": "Strange at it may seem, there was a movement called \"anti-suffragism\" in the U.S. and U.K. composed mainly of women. Their numbers were small, since this posture would have been \"counterintuitive.\"\nThe Americans were composed mainly of \"conservative\" women who liked the division of duties and society between \"domestic\" (for women), and \"outside,\" for men. On the other side were radicals like Emma Goldman, who favored \"anarchism,\" instead of working within the system, through suffrage.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "Strange at it may seem, there was a movement called 'anti-suffragism' in the U.S. and U.K. composed mainly of women."
      },
      {
        "type": "Claim",
        "text": "Their numbers were small, since this posture would have been 'counterintuitive.'"
      },
      {
        "type": "Description",
        "text": "The Americans were composed mainly of 'conservative' women who liked the division of duties and society between 'domestic' (for women), and 'outside,' for men."
      },
      {
        "type": "Comparison",
        "text": "On the other side were radicals like Emma Goldman, who favored 'anarchism,' instead of working within the system, through suffrage."
      }
    ]
  },
  {
    "site": "history",
    "question_title": "How did the general population of England convert so smoothly to Protestantism?",
    "question_body": "The rise of Protestantism among English monarchs from Henry VIII is well documented. However, after a bit of light research, I have been unable to find many resources on how the general population felt about the change, or how it came to be a popular majority position in a relatively short space of time.\nAlthough Henry VIII broke with the Pope, the early Church of England remained essentially Catholic in its practices. During Edward's regency, Protestantism was effectively forced upon the country. But there seems to have been no great outbreak of upset or unrest at this even though when Mary gained the throne, they settled easily back into Catholic practice.\nYet during Elizabeth's reign of 45 years, the country gained a Protestant majority, again with little seeming fuss or protest from the general populace. By the time of Charles, a mere 22 years later, it was established enough and with sufficient devotion to be a major cause in the outbreak of civil war.\nI could accept that most of the population simply didn't care enough one way or the other. They just went to church and worshipped as the minister lead them. But if this is the case it would seem to stand in stark contrast with the considerable fervour which Londoners and the urban population in general greeted Charles' vague defences of Catholicism. \nHow were the general public persuaded to change their religion so smoothly, and so totally, in the space of 70 years?\n",
    "answer_body": "Well, it wasn't smooth.\nFirst of all, there was already a minority of \"reform\" viewpoint in England before Henry VIII.  It was centered in the intelligentsia and gentry.\nSo when Henry VIII decided to divorce the Church to marry Anne Boleyn, a significant and influential minority not only was in favor, but wanted to go further, faster.\nAnd, as always, a big group of people just wanted to get on with their lives and hoped it would all go away.\nHenry VIII's extravagance had bankrupted him and the country (there was not much distinction between the two at that time) so his other motive (besides Anne) was the wealth of the monasteries. By plundering them, he restored his checkbook's balance. Key point: He didn't keep the church lands, but sold it for ready cash to the gentry.  Now, the gentry -- whose lifestyle revolved around the land -- had an incentive to support the new regime, since a return to the old would probably mean the disgorgement of all that lovely land.  \"Return to the True Church?  Of course I'm in favor!  Always have been. But those monasteries were pretty bad -- do you know what I saw on Facebook the other day? They were doing terrible things! -- Maybe we should restore the Church, but leave the lands alone.  So that the Church can focus on saving souls, of course.\"\nOnce Queen Elizabeth took over from her activist siblings, she ruled as a kinder, gentler Protestant and -- mostly -- didn't punish people for what they believed, but for what they did. \"Be Catholic if you like. Just be discreet about it, don't support the Spanish or the French, or Mary Queen of Scots, and don't expect royal favor.\"  It wasn't a happy time for Catholics, but it was -- mostly -- tolerable as long as they avoided religious politics.\nBy the time that the Stewarts came in and made things a bit more difficult, this state of affairs had been in place for most people's whole lifetimes.  It could be lived with.  And it was.\nBut Catholicism never died and was a significant minority religion throughout.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": []
  },
  {
    "site": "history",
    "question_title": "How much smaller were medieval farm animals in England than today?",
    "question_body": "According the Medieval Life and Times website,\n\nFarm animals were small, for scientific breeding had not yet begun. A\n  full-grown ox reached a size scarcely larger than a calf of to-day,\n  and the fleece of a sheep often weighed less than two ounces.\n\nThis is more or less echoed by The Finer Times (or maybe one copied the other?):\n\nThe size of a full-grown bull reached the size slightly larger than a\n  calf today, and the fleece of an entire sheep weighed an average of\n  two ounces.\n\nThe assertion that an ox wasn\u2019t much larger than a calf is unsatisfactory (perhaps even dubious): the size of a calf obviously depends on its age and some breeds (e.g. Limousin) are much larger than others (e.g. Jersey) - I grew up on a farm so I know a little about this. Wikipedia has this image of Anglo-Saxon ploughmen but I\u2019m not sure how reliable the scale is (and the animals\u2019 faces are a little odd!).\n\nEven more implausible is 2 ounces for the fleece of a sheep. According to sheep101, a fleece in the US today might weigh anything from 2 to 16 pounds (32 to 288 ounces). Even just taking the lowest figure, the difference between medieval and contemporary fleece weights seems improbable, or is at least in need of more authoritative sources.\nWere the animals cited in the sources really so much smaller (on average) than they are today? Was the same true for other farm animals such as chickens and pigs?\n",
    "answer_body": "I think both sources copied Early European History by Hutton Webster, published about a century ago. The underlying claim is true: Medieval animals were much smaller than today's. However, it is obvious that \"a calf\" is not a meaningful unit of comparison.\nThe historical weight of livestock is mainly determined from archaeological studies as well as records of butchery transactions, and reveal significantly smaller farm animals than today's. See for instance the following figures:\n\n[A]round the year 1000, an adult pig weighed around 70-80 kg, a sheep 20 to 30 kg, and a cow or ox 200 to 250 kg . . . In comparison, at the beginning of the twentieth century, an ox weighed in the region of 650 kg, a sheep from 50-150 kg, and a pig from 100-200 kg.\nComet, Georges. \"Technology and agricultural expansion in the middle ages: the example of France north of the Loire.\" Astill, Grenville G., and John Langdon, eds. Medieval farming and technology: The impact of agricultural change in Northwest Europe. Brill, 1997.\n\nThese are from Charavines in France, but English animals would have been similar in size. Based on remains, cattle at York were estimated to be between 220 - 270 kg, for instance.1\nOf course, the weight of animals did not stay constant throughout the whole of the Middle Ages. They were even smaller during the earliest centuries,2 and seemed to have gradually became larger close to the Early Modern period.3\nIn any case, height differences are much less dramatic than weight. Medieval cattle were half the weight of industrial revolution ones, but only 20% shorter.4 Hence, compared to ~150cm for cattle and ~75cm for sheep, depending on the species, today:\n\nAt Hamwih . . . cattle apparently had a mean shoulder height of 115cm. The sheep were small with a shoulder height of 62 cm.\nSteane, John. The Archaeology of Medieval England and Wales. Vol. 47. Routledge, 2014.\n\nThat said, only two ounces for the fleece of the sheep is quite an understatement.\n\nThe average weight of sheep fleeces per animal on the Winchester manors from 1300 to 1324 was 1.5 lb.\nClark, Gregory. \"Labour productivity in English agriculture, 1300-1860.\" Campbell, Bruce MS, and Mark Overton, eds. Land, Labour, and Livestock: Historical Studies in European Agricultural Productivity. Manchester University Press, 1991.\n\nSeasonal variations aside, differences in fleece weight were mainly region dependent. A particularly poor area was East Anglia, and especially from the pastures of Breckland.5 Yet, even in Breckland the worst yield was still about ~1 lb, or 16 ounces:\n\nBailey, Mark. A Marginal Economy?: East Anglian Breckland in the Later Middle Ages. Cambridge University Press, 1989.\nNotes & Refernces:\n1. O'Connor, Terence Patrick. Bones from Anglo-Scandinavian Levels at 16-22 Coppergate. Council for British Archaeology, London 1989. \"[For] a very lean conformation, an average liveweight in the region of 220kg would seem likely. For a heavier conformation, this average could perhaps be raised to around 270kg.\"\n2. Crabtree, Pam J. \"West Stow, Suffolk: Early Anglo-Saxon Animal Husbandry\". East Anglian Archaeology Report 47. Suffolk County Council, 1989.. \"Based on the measurements of the trochlear breadth of the humerus, it is estimated that the average West Stow cattle would have had an average live weight of only about 150-170 kg, and a fat-free carcass weight of about 100 kg.\n3. Kershaw, Ian. Bolton Priory: the Economy of a Northern Monastery, 1286-1325. Oxford University Press, 1973. \"[T]he average carcass weight [was] about 430 lb. for oxen urhcased for victualling the navy in 1547.\"\n4. Clark, Gregory. \"Labour productivity in English agriculture, 1300-1860.\" Campbell, Bruce MS, and Mark Overton, eds. Land, Labour, and Livestock: Historical Studies in European Agricultural Productivity. Manchester University Press, 1991.\"[N]ote that cattle in this period were about 80 per cent the hiegh tof cattle in the late eighteenth century, which would impaly that they were about 49 per cent of thre weight.\"\n5. Bailey, Mark. A Marginal Economy?: East Anglian Breckland in the Later Middle Ages. Cambridge University Press, 1989. \"Breckland's poor pastures were not conducive to producing heavy, thick fleeces, and its sheep were of the shortwool variety whose fleeces were lightweight and low in quality. This was true of East Anglia in general, but it would appear that Breckland fleeces were poor even by these standards.\"\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "I think both sources copied Early European History by Hutton Webster, published about a century ago."
      },
      {
        "type": "Claim",
        "text": "The underlying claim is true: Medieval animals were much smaller than today's."
      },
      {
        "type": "Caveat",
        "text": "However, it is obvious that 'a calf' is not a meaningful unit of comparison."
      },
      {
        "type": "Evidence",
        "text": "The historical weight of livestock is mainly determined from archaeological studies as well as records of butchery transactions, and reveal significantly smaller farm animals than today's."
      },
      {
        "type": "Example",
        "text": "For instance, around the year 1000, an adult pig weighed around 70-80 kg, a sheep 20 to 30 kg, and a cow or ox 200 to 250 kg."
      },
      {
        "type": "Citation",
        "text": "Comet, Georges. 'Technology and agricultural expansion in the middle ages: the example of France north of the Loire.' Astill, Grenville G., and John Langdon, eds. Medieval farming and technology: The impact of agricultural change in Northwest Europe. Brill, 1997."
      },
      {
        "type": "Comparison",
        "text": "In comparison, at the beginning of the twentieth century, an ox weighed in the region of 650 kg, a sheep from 50-150 kg, and a pig from 100-200 kg."
      },
      {
        "type": "Context",
        "text": "These are from Charavines in France, but English animals would have been similar in size."
      },
      {
        "type": "Evidence",
        "text": "Based on remains, cattle at York were estimated to be between 220 - 270 kg."
      },
      {
        "type": "Elaboration",
        "text": "The weight of animals did not stay constant throughout the whole of the Middle Ages. They were even smaller during the earliest centuries and seemed to have gradually become larger close to the Early Modern period."
      },
      {
        "type": "Comparison",
        "text": "Height differences are much less dramatic than weight. Medieval cattle were half the weight of industrial revolution ones, but only 20% shorter."
      },
      {
        "type": "Citation",
        "text": "Steane, John. The Archaeology of Medieval England and Wales. Vol. 47. Routledge, 2014."
      },
      {
        "type": "Example",
        "text": "At Hamwih, cattle apparently had a mean shoulder height of 115cm. The sheep were small with a shoulder height of 62 cm."
      },
      {
        "type": "Caveat",
        "text": "That said, only two ounces for the fleece of the sheep is quite an understatement."
      },
      {
        "type": "Citation",
        "text": "Clark, Gregory. 'Labour productivity in English agriculture, 1300-1860.' Campbell, Bruce MS, and Mark Overton, eds. Land, Labour, and Livestock: Historical Studies in European Agricultural Productivity. Manchester University Press, 1991."
      },
      {
        "type": "Elaboration",
        "text": "Seasonal variations aside, differences in fleece weight were mainly region dependent."
      },
      {
        "type": "Example",
        "text": "A particularly poor area was East Anglia, and especially from the pastures of Breckland."
      },
      {
        "type": "Citation",
        "text": "Bailey, Mark. A Marginal Economy?: East Anglian Breckland in the Later Middle Ages. Cambridge University Press, 1989."
      },
      {
        "type": "Historical Reference",
        "text": "Notes & References:"
      }
    ]
  },
  {
    "site": "history",
    "question_title": "Why were old fortifications shaped like stars and not like circles?",
    "question_body": "The fortifications of Copenhagen, Fredericia and Acre are star-shaped. (Defense of Copenhagen is shown in the picture below.) Why not have defenses or fortifications in the shape of  a pure square or circle?\n\n",
    "answer_body": "Star forts or bastion forts are designed to enable enfilade (or flanking) fire: shooting on the line of attackers from the side, significantly increasing firing efficiency of the defender.\nFlanking fire allows guns placed in the side wall of the bastion (protected from direct fire from attackers) to safely and effectively shoot at an entire line of attackers (enfilade them) as they are closing to the neighboring bastion. Flanking fire is very effective, because if you miss one attacker, you hit the next one. Check the enfilade link for images.\n\nHere the red arrows show direction of the flanking fire from the guns placed in the protected side walls (\"flanks\", #3) of the bastions (#1 and #2). More details about the placement of guns in bastion's flank side is in this image of bastion with caponier. Guns in \"face\" walls (#4) would be much more exposed to enemy's fire.\nThe star fort has no \"dead zones\", where an attacker can hide from flanking fire, like a round turret allows.\nThe star design for bastions was first introduced in Italy in the 15th century, and later perfected by Vauban - in the Citadell of Lille and others. Wikipedia has more nice images and a history of the approach.\nOther answers mentioned an inability to fire from above, but that is not the main reason for the star design: its goal is to enable flanking fire. And yes, star shaped bastions do not allow to fire on attackers from above, but that is not necessary (and would be dangerous for defenders), because the walls are subject to more effective flanking fire from the side walls (protected from direct fire of attackers) of neighboring bastions.\nOf course Vauban also designed a way how to attack such star fortress, by digging 3 lines of parallel trenches connected by zigzag trenches to avoid enfilading fire down the trench line, with 3rd parallel coming close to the attacked fort's glacis (outer edge of the fortifications).\n\nImages are from Wikimedia/Wikipedia.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Star forts or bastion forts are designed to enable enfilade (or flanking) fire: shooting on the line of attackers from the side, significantly increasing firing efficiency of the defender."
      },
      {
        "type": "Example",
        "text": "Flanking fire allows guns placed in the side wall of the bastion (protected from direct fire from attackers) to safely and effectively shoot at an entire line of attackers (enfilade them) as they are closing to the neighboring bastion."
      },
      {
        "type": "Visual Description",
        "text": "Here the red arrows show direction of the flanking fire from the guns placed in the protected side walls ('flanks', #3) of the bastions (#1 and #2). More details about the placement of guns in bastion's flank side is in this image of bastion with caponier."
      },
      {
        "type": "Comparison",
        "text": "The star fort has no 'dead zones', where an attacker can hide from flanking fire, like a round turret allows."
      },
      {
        "type": "Historical Reference",
        "text": "The star design for bastions was first introduced in Italy in the 15th century, and later perfected by Vauban - in the Citadell of Lille and others."
      },
      {
        "type": "Claim",
        "text": "Other answers mentioned an inability to fire from above, but that is not the main reason for the star design: its goal is to enable flanking fire."
      },
      {
        "type": "Elaboration",
        "text": "And yes, star shaped bastions do not allow to fire on attackers from above, but that is not necessary (and would be dangerous for defenders), because the walls are subject to more effective flanking fire from the side walls (protected from direct fire of attackers) of neighboring bastions."
      },
      {
        "type": "Procedure",
        "text": "Of course Vauban also designed a way how to attack such star fortress, by digging 3 lines of parallel trenches connected by zigzag trenches to avoid enfilading fire down the trench line, with 3rd parallel coming close to the attacked fort's glacis (outer edge of the fortifications)."
      },
      {
        "type": "Citation",
        "text": "Images are from Wikimedia/Wikipedia."
      }
    ]
  },
  {
    "site": "history",
    "question_title": "Why did Canada not join the American Revolution?",
    "question_body": "America declared independence from Britain and fought a war of independence. Several states joined the fight, however, Canada remained British. Why did the American rebellion and war of independence not spread to Canada?\n",
    "answer_body": "Short Answer: The Canadiens were tired of war and content with British rule.\nLong Answer:\nTwenty-some years before the American Revolution (1754), which was just before the Seven Years War, this is what the map of British Colonies looked like:\n\nOnly a few areas of modern-day Canada were British then: Nova-Scotia, Labrador-Newfoundland, and around James' Bay & Hudson's Bay. Quebec extended south to below Niagara falls.\nThe 13 American Colonies were centered around New-York City:\n\n1.) Geographic Separation caused the English speaking British colonies north of Maine to be culturally distinct from the 13 American Colonies. The people of Nova Scotia were half New Englanders and half Germans, Highlanders, Ulstermen and Yorkshiremen. Nova Scotia wished to remain neutral. British Naval power and a British Garrison at Halifax prevented any serious American attempt at invasion. In 1777 Nova-Scotian outposts came under attack from New England privateers seeking plunder. That caused even former New-Englanders to form militias and defend their homes. Soon thereafter the New Light religious movement (Great Awakening) started by Henry Alline of Rhode Island swept through New England and Nova Scotia turning attention away from politics.\n2.) Acceptance of British Rule: When New France fell in 1760, the defeated armies, French officials, some seigneurs, and some merchants returned to France. British credit, currency, and markets such as London was what mattered--not Paris or America. The British successfully implemented representative government in Quebec through respecting the religious freedoms of Catholics and recognizing the political value of the Catholic Church, which was backed by a dutiful French populace that contrasted sharply with the restive 13 American colonies.\n3.) The Quebec Act of 1774 satisfied Quebec and angered the American colonies. It allowed English criminal law to exist in parallel with French civil law and the entrenched seigneurial system. Quebec even had a (legal) mandatory tithe to the Catholic Church, which only concerned Catholics.\nThe Quebec Act also expanded the province of Quebec to include Labrador in the East and extended the Western boundary to the junction of the Ohio and Mississippi rivers all the way north to Rupert's Land. This expansion had the obvious intent of funneling the fur-trading areas serviced through the St. Lawrence into the jurisdiction of Quebec. The land was mainly Indian territory (where the Indians were allied with the French) that was exploitable for the fur trade without endangering Indian land rights and risking war.\nAmerican colonists desired to settle these native lands, and therefore listed the Quebec Act as one of the \"Insufferable Acts.\" Furthermore, the accommodations given to Catholic religious practice angered and frightened the fiercely anti-Catholic Americans, who often viewed the Anglican British as in league with Catholics. In fact, the Dominion of New England ended, and the New England colonies separated administratively again, after the Anglican royal governor was overthrown amid rumors that the governor was planning to take over the area in the name of the Pope.\n4.) Cultural and Religious Isolation: Quebec was the largest British colony in what is now Canada. The language barrier combined with the foreign religion of French Quebec and the history of hostilities from the Seven Years War caused Americans to view the people of Quebec as foes.\n5.) Patriot attacks on Canadiens solidified opposition to the American revolution. American Patriot generals Richard Montgomery and Benedit Arnold Attacked Quebec in an attempt to seize Canada from British control (1775). They took Montreal and laid siege (ultimately unsuccessfully) to Quebec City, where British regulars and a few Canadien militia defended. The Americans were ill-supplied but stayed till spring, when the British navy sailed up the St. Lawrence.\n\nIt also became true that in the wartime alliance reached in 1778\nbetween France and the young American republic, neither partner really\nwanted to see the other established at Quebec, preferring to have it\nleft to Britain rather than that either of the two new \"friends\"\nshould hold it.\n\n6.) Economic Interests: The merchants of British North-America benefited from the influx of British troops (and money) which powered the offense south from Quebec. The Canadians also profited from access to the tariff-protected British markets, which far larger New England competitors had forfeited through the act of war. The fur market in particular began to thrive in Canada. The British Navy on the Atlantic and by British military power in the interior both guarded the fur trade.\n\nBusinessmen came to recognize that their economic stake in the\nimperial system far outweighed any political discontent over the\nQuebec Act -- and that Act, after all had re-attached the valuable\nsouthwest fur domains to Canada. Hence the merchants' sense of\ncommitment increased with the flow of trade on into the 1780s; as they\nsaw that their St. Lawrence commercial realm was tied both to Britain\nand to Canada's own growth westward. Factors of geography and business\ninterest in effect were shaping the prime leaders of Montreal into\nBritish imperialists and Canadian economic nationalists combined.\n\n7.) Many Loyalists moved to Canada to support the British cause. .\nConclusion: pardon the quotes\n\nAs for the mass of French Canadians in the province (of Quebec), they\nbegan to follow their seigneurial and clerical elites into their own\ncommitment to the British side. Naturally the Canadiens still put\ntheir distinct community concerns and heritage first; yet they also\nconcluded that the Americans should not be welcomed, but kept outside.\nThe self-proclaimed republican \"liberators\" had simply turned out to\nbe the same old enemies, les Bostonnais, the Puritans of New England:\nstabling horses in Catholic churches during their invasion, paying in\nworthless paper money for crops and supplies seized from habitant\nfarms. The Canadiens did not learn to love their British conquerors as\na result -- why should they? -- but did grow to believe that they were\nbetter off with them. For the provisions of the Quebec Act had\nguaranteed French Canada's own special rights and character under\nBritish rule: guarantees which the Americans certainly would not have\ngiven. Instead angry American outcries had greeted the Act because of\nthe very grants it had made to the \"French Papists\". Thus for\ndifferent but historically sound reasons, neither the Francophone and\nAnglophone communities of Quebec province took to the American path of\nrevolution. They stayed within the remaining British empire -- above\nall, to avoid being swallowed up in another emerging empire, that of\nthe United States.\n\nAddendum:\nSt. Johns, PEI, and Newfoundland\n\nThe little neighbouring Atlantic province, the Island of St. John, was\nhardly likely to affect the course of empires. It certainly continued\nin British keeping -- although an American privateer raid on\nCharlottetown in 1775 carried the acting governor and two officials\noff to General Washington, who did not want them, and sent them home.\nThe big island of Newfoundland also suffered, and more harshly, from\nAmerican privateering ravages. But here British garrisons and naval\nsquadrons still blocked any real threat to imperial control. In any\ncase, the war years brought the island flourishing times in its\nessential cod fishery, particularly for residents, since many of the\nvisiting overseas fishermen had been drafted into the Royal Navy. Thus\nNewfoundland, too, stayed surely within Britain's American empire.\n\nGreat Lake Indians\n\nAt the other, western end of empire, war spread through the inland\nforests below the Great Lakes, from the Iroquois country to the Ohio\nand Michigan wilderness. In the upper reaches of New York province,\npatriot rebel forces contended fiercely with units raised from\nloyal-minded settlers in the area. But further, the Six Nations\nIroquois and their traditional homelands were heavily involved. The\nTuscaroras and Oneidas largely sided with the Americans. The rest of\nthe Six Nations, and especially the Mohawks, supported the British;\nfor here old bonds of alliance held strong. They had been well forged\nunder Sir William Johnson as Indian Superintendent till his death in\n1774, to be maintained thereafter by his son and heir, Sir John\nJohnson, later to become Superintendent in his own right.\n\nFor the most in-depth discussion of this topic I could find see this Canadian Heritage Book (free), which is the source of the quotes and much of the content in this answer.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "Twenty-some years before the American Revolution (1754), which was just before the Seven Years War, this is what the map of British Colonies looked like:"
      },
      {
        "type": "Comparison",
        "text": "Geographic Separation caused the English speaking British colonies north of Maine to be culturally distinct from the 13 American Colonies."
      },
      {
        "type": "Elaboration",
        "text": "Acceptance of British Rule: When New France fell in 1760, the defeated armies, French officials, some seigneurs, and some merchants returned to France."
      },
      {
        "type": "Implication",
        "text": "The Quebec Act of 1774 satisfied Quebec and angered the American colonies."
      },
      {
        "type": "Evidence",
        "text": "Patriot attacks on Canadiens solidified opposition to the American revolution."
      },
      {
        "type": "Cause/Effect",
        "text": "Economic Interests: The merchants of British North-America benefited from the influx of British troops (and money) which powered the offense south from Quebec."
      },
      {
        "type": "Summary",
        "text": "Conclusion: The Canadiens were tired of war and content with British rule."
      },
      {
        "type": "Context",
        "text": "Addendum: St. Johns, PEI, and Newfoundland"
      },
      {
        "type": "Context",
        "text": "Great Lake Indians"
      }
    ]
  },
  {
    "site": "graphicdesign",
    "question_title": "Why should I ever use Unicode\u2019s special characters for Roman numerals?",
    "question_body": "This is to answer a question which arose in the comments on this question on the Unicode characters for Roman numerals:\n\nWhy is this necessary or preferred over the usual way of typing ai, ai-ai, ai-ai-ai, vee-ai, etc.?\n\nTo start from the beginning, in Unicode\u2019s Number Forms block, there exist code points for Roman Numerals that are at first glance very similar in appearance to standard capital latin letters or combinations thereof (U+2160 \u2013\u00a0U+217F). For example, U+2165 (Roman Numeral Six) looks very much like VI (Latin Capital Letter\u00a0V and Latin Capital Letter\u00a0I).\nThus, the question arises why one should not use the latter to represent those digits and, e.g., type Louis VII instead of Louis \u2166. Obviously, using no special characters avoids compatibility issues with fonts that do not support them. But even if I know that the text will be rendered with a font that does support these characters, why should I bother using them?\n",
    "answer_body": "In many fonts you will indeed find hardly any difference between using the Unicode characters for Roman numerals and just composing them from stardard Latin letters. For example, the following shows Louis VII (top) and Louis \u2166 (bottom, using codepoints for Roman numerals) rendered with FreeSans:\n\nApart from a tiny difference in spacing, which was propably not intentional, the output is identical.\nHere is the same text rendered with DejaVu Sans:\n\nWhile the characters still look identical, there is a considerable difference in spacing. It may be a matter of taste whether the latter is preferrable for Roman numerals, but it certainly wouldn\u2019t be a good choice of kerning for regular all-caps.\nLinux Libertine goes one step further:\n\nHere the Roman numerals are slightly smaller than the capital letters, thus matching the font\u2019s Arabic numerals. Most importantly, they are connected, reproducing a feature often found in hand-drawn Roman numerals.\nNow, some may still argue that there aren\u2019t any improvements in the above or that they aren\u2019t worth the effort. So here is a case, where not using the Unicode characters will produce horrible results:\n\n(Note that the small size of the numerals reflects some actual historic typesetting.) Something similar may occur for script or caligraphic fonts.\nWithout specific Unicode points for Roman numerals, dissolving the latter problem would only be possible with:\n\nUsing a complex OpenType feature (or similar) that tries to detect whether a sequence of capital letters is a roman numeral. This will inevitably cause problems with words that would also be a valid Roman numeral.\nUsing a simple OpenType feature, that needs to be manually activated for every Roman numeral.\nUsing Unicode\u2019s Private-Use Area. Compatibility issues are likely to ensue even when switching between two fonts that both support Roman numerals.\n\nFrom Unicode\u2019s point of view, the huge semantical difference between capital Latin letters and Roman numerals should already have sufficed for a seperate encoding of Roman numerals.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "In many fonts you will indeed find hardly any difference between using the Unicode characters for Roman numerals and just composing them from standard Latin letters."
      },
      {
        "type": "Example",
        "text": "For example, the following shows Louis VII (top) and Louis \u2166 (bottom, using codepoints for Roman numerals) rendered with FreeSans."
      },
      {
        "type": "Comparison",
        "text": "Apart from a tiny difference in spacing, which was probably not intentional, the output is identical."
      },
      {
        "type": "Example",
        "text": "Here is the same text rendered with DejaVu Sans."
      },
      {
        "type": "Comparison",
        "text": "While the characters still look identical, there is a considerable difference in spacing."
      },
      {
        "type": "Elaboration",
        "text": "It may be a matter of taste whether the latter is preferable for Roman numerals, but it certainly wouldn\u2019t be a good choice of kerning for regular all-caps."
      },
      {
        "type": "Example",
        "text": "Linux Libertine goes one step further."
      },
      {
        "type": "Elaboration",
        "text": "Here the Roman numerals are slightly smaller than the capital letters, thus matching the font\u2019s Arabic numerals. Most importantly, they are connected, reproducing a feature often found in hand-drawn Roman numerals."
      },
      {
        "type": "Claim",
        "text": "Now, some may still argue that there aren\u2019t any improvements in the above or that they aren\u2019t worth the effort."
      },
      {
        "type": "Example",
        "text": "So here is a case, where not using the Unicode characters will produce horrible results."
      },
      {
        "type": "Caveat",
        "text": "(Note that the small size of the numerals reflects some actual historic typesetting.) Something similar may occur for script or calligraphic fonts."
      },
      {
        "type": "Procedure",
        "text": "Without specific Unicode points for Roman numerals, dissolving the latter problem would only be possible with:"
      },
      {
        "type": "Elaboration",
        "text": "1. Using a complex OpenType feature (or similar) that tries to detect whether a sequence of capital letters is a Roman numeral. This will inevitably cause problems with words that would also be a valid Roman numeral."
      },
      {
        "type": "Elaboration",
        "text": "2. Using a simple OpenType feature that needs to be manually activated for every Roman numeral."
      },
      {
        "type": "Elaboration",
        "text": "3. Using Unicode\u2019s Private-Use Area. Compatibility issues are likely to ensue even when switching between two fonts that both support Roman numerals."
      },
      {
        "type": "Implication",
        "text": "From Unicode\u2019s point of view, the huge semantical difference between capital Latin letters and Roman numerals should already have sufficed for a separate encoding of Roman numerals."
      }
    ]
  },
  {
    "site": "graphicdesign",
    "question_title": "Q: Gimp vs Photoshop",
    "question_body": "I have been using Photoshop since the beginning of time. Other than the infamous Paintshop Pro, it is the only image editor I have used.\nI am about to embark on an adventure of trying out GIMP as a lot of people have been recommending it.\n\nQ: In What Areas (if Any) Does it Excel Over Photoshop?\nIt would be interesting to know from people who are seasoned with it if it excels over Photoshop in any areas or is it just a good as free alternative image editing application that is best suited to people on a budget?\n",
    "answer_body": "To answer your literal question, there are a few areas where GIMP can be better than Photoshop.  It's not particularly common (usually, at best you can expect them to do the job equally well), but it does happen.  Off the top of my head (and keeping in mind that it's been many years since I last used Photoshop), here are a few examples:\n\nAs a historical example, GIMP had content-aware fill (via a free plugin) for several years before Adobe added an equivalent feature to Photoshop.\nAs far as I know, Photoshop still doesn't have anything quite equivalent to GIMP's Color to Alpha tool, even though it's very useful for things like replacing the background of an image while preserving soft edges.  (Apparently there is an old plugin for it, but it's 32-bit only and no longer maintained.)\nGIMP has built-in support for loading and saving multiresolution icons in the .ico format, e.g. for use as favicons on the web.  For Photoshop, you need to find and install a plugin or use an external converter tool.\nI haven't actually used the Adobe Camera Raw plugin in Photoshop, so I can't really compare it in detail with GIMP's UFRaw plugin.  From what I've seen, Adobe's plugin certainly has a much smoother user interface and includes a bunch of editing options that UFRaw lacks, but on the other hand, UFRaw does have a pretty advanced technical back-end and supports lots of obscure camera formats.  While Camera Raw is probably nicer to use for most people, I'd be surprised if one couldn't find anything that UFRaw does better.\n\nI'm sure one could find quite a few more cases where GIMP does something better than Photoshop or has some feature Photoshop lacks, especially if plugins are included.  (There are a lot of plugins for both GIMP and Photoshop, and not all of them have equivalents either way.)  If you have both, you'll probably find that Photoshop offers more features and/or a better user interface for most common editing tasks.  But every once in a while, there are some places where GIMP manages to shine.\nAnd of course, there are two more advantages of GIMP worth noting:\n\nPhotoshop is proprietary software, and costs money to install (and the pricing is effectively graduated so that if you want more advanced features, you need to pay more).  GIMP is free software (both \"free as in beer\" and \"free as in speech\"), meaning that you don't have to pay anything for it, can freely make as many copies as you want and even share them with others, and always get all the features.\nSince anybody is free to download the source code to GIMP and compile it, GIMP runs on a lot of systems that Photoshop doesn't support.  Sure, if you're using Windows or MacOS then you're probably fine, but even on x86-based Linux systems, you can only run Photoshop using an API emulation layer like WINE.  And just try running Photoshop on a Raspberry Pi...\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "To answer your literal question, there are a few areas where GIMP can be better than Photoshop."
      },
      {
        "type": "Example",
        "text": "As a historical example, GIMP had content-aware fill (via a free plugin) for several years before Adobe added an equivalent feature to Photoshop."
      },
      {
        "type": "Example",
        "text": "As far as I know, Photoshop still doesn't have anything quite equivalent to GIMP's Color to Alpha tool, even though it's very useful for things like replacing the background of an image while preserving soft edges."
      },
      {
        "type": "Example",
        "text": "GIMP has built-in support for loading and saving multiresolution icons in the .ico format, e.g. for use as favicons on the web."
      },
      {
        "type": "Comparison",
        "text": "I haven't actually used the Adobe Camera Raw plugin in Photoshop, so I can't really compare it in detail with GIMP's UFRaw plugin."
      },
      {
        "type": "Elaboration",
        "text": "I'm sure one could find quite a few more cases where GIMP does something better than Photoshop or has some feature Photoshop lacks, especially if plugins are included."
      },
      {
        "type": "Claim",
        "text": "Photoshop offers more features and/or a better user interface for most common editing tasks."
      },
      {
        "type": "Comparison",
        "text": "But every once in a while, there are some places where GIMP manages to shine."
      },
      {
        "type": "Claim",
        "text": "There are two more advantages of GIMP worth noting:"
      },
      {
        "type": "Comparison",
        "text": "Photoshop is proprietary software, and costs money to install, while GIMP is free software."
      },
      {
        "type": "Comparison",
        "text": "Since anybody is free to download the source code to GIMP and compile it, GIMP runs on a lot of systems that Photoshop doesn't support."
      }
    ]
  },
  {
    "site": "graphicdesign",
    "question_title": "What is wrong with Comic Sans?",
    "question_body": "There's a great TEDxExeter talk  by a colleague of mine, Simon Peyton-Jones, about the recent advances in the English lower school 'computer science' curriculum. Like all of his slide decks he uses Comic Sans throughout. Depressingly, though inevitably, one of the YouTube commenters berates him for the font choice, stating that his use of Comic Sans is \"the design equivalent of putting a giant image of a middle finger on the screen; an insult to education\". This prompts a further comment that points to 41:32 in another of Simon's talks where Simon is asked by an audience member why he uses Comic Sans. Here is Simon's reply:\n\nThis is a very funny question, \"Why use Comic Sans?\" So, all my talks\n  use Comic Sans and I frequently see remarks like 'Simon Peyton-Jones,\n  great talk about Haskell but why did he use Comic Sans?' but nobody's\n  ever been able to tell me what is wrong with it. It's a nice legible\n  font, I like it. So until somebody explains to me ... Ah, I understand\n  that it's meant to be a bit naff, but I don't care about naff stuff, I\n  care about being able to read it. So if you have got a sort of ...\n  some rational reasons why I should not then I'll listen to them. But\n  just being unfashionable? I don't care.\n\nSimon is talking off-the-cuff here, so I think by \"rational\" he means affecting legibility, reading speed, comprehension, and things like that. Are there any studies relating fonts across those kind of measures? If so where does Comic Sans come in the ranking?\n(N.B. I cannot help thinking it is a good design choice that he's made. He has deliberately chosen a font that no-one versed in the design of slides would choose. It suggests, in my mind at least, a kind of authenticity; but the argument between brand adherence and authenticity is one I keep losing.)\n",
    "answer_body": "At its core, There isn't really anything wrong with Comic Sans. It was designed for a purpose - comic-book-style speech bubbles primarily. It did a good job at that - if you're going to have Microsoft Bob talk to you on a screen, Comic Sans feels more 'right' than Times New Roman.\nThree things have contributed to Comic Sans' unpopularity, in my view.\nFirst, exposure. It shows up everywhere, and it's distinct enough for people to notice. Your average person doesn't look at Helvetica, Arial, Gotham, and Franklin Gothic and consciously perceive them as all that different, but Comic Sans was comparatively unique and readily available. Sometimes when things become really popular really fast, there is backlash from people who don't like popular things.\nSecond, and this is more of a legitimate critique, is appropriateness. The Declaration of Independence, Magna Carta, etc. - these are formal documents with a high degree of gravitas about them. If Jefferson had passed the quill to the nearest child in the room, the result would have been something that the King wouldn't have taken seriously. \nA Gothic blackletter is fitting for the masthead of The New York Times. Comic Sans would not be. Conversely, having your average superhero talk with a Gothic blackletter in a comic book would also feel inappropriate. \n\nSo why is Comic Sans \"appropriate\" for certain situations? It most closely resembles informal handwriting, and thus conveys informality. If humans didn't write in cursive and we never saw any font other than Comic Sans, we'd never know the difference! But we do and we have, so such a connotation exists.\nWithout knowing anything about this guy, it seems like he'd be the kind of person who'd wear socks under his Crocs because it's comfortable and he doesn't care. And that's fine - Crocs are indeed lightweight and comfortable, and protect you from the heat and irregularities of the road. But if you're running for president or trying to get a job in the C-Suite of a Fortune 500 company, you put on dress shoes because that's what people in that setting do.\nA third reason will sound snobby, but I think would explain a lot about how designers tend to think about this sort of thing. Imagine that you're a wine connoisseur and, everywhere you go, you see people not only buying boxed wine, but saying it's great wine and that they know something about wine because they found this box in the state store. \nThe idea is that trained designers don't really like people using Publisher anyways, so they're more inclined to hate on designs that are done by amateurs who pick Comic Sans because \"it looks fun\" or whatever. Then, once you've established that it's cool to pick on Comic Sans, everyone gets lumped into that group.\n",
    "audience_level": "Expert",
    "tone": "Formal",
    "blocks": [
      {
        "type": "Context",
        "text": "At its core, There isn't really anything wrong with Comic Sans. It was designed for a purpose - comic-book-style speech bubbles primarily."
      },
      {
        "type": "Example",
        "text": "It did a good job at that - if you're going to have Microsoft Bob talk to you on a screen, Comic Sans feels more 'right' than Times New Roman."
      },
      {
        "type": "Claim",
        "text": "Three things have contributed to Comic Sans' unpopularity, in my view."
      },
      {
        "type": "Elaboration",
        "text": "First, exposure. It shows up everywhere, and it's distinct enough for people to notice."
      },
      {
        "type": "Elaboration",
        "text": "Your average person doesn't look at Helvetica, Arial, Gotham, and Franklin Gothic and consciously perceive them as all that different, but Comic Sans was comparatively unique and readily available."
      },
      {
        "type": "Claim",
        "text": "Second, and this is more of a legitimate critique, is appropriateness."
      },
      {
        "type": "Example",
        "text": "The Declaration of Independence, Magna Carta, etc. - these are formal documents with a high degree of gravitas about them."
      },
      {
        "type": "Analogy",
        "text": "If Jefferson had passed the quill to the nearest child in the room, the result would have been something that the King wouldn't have taken seriously."
      },
      {
        "type": "Comparison",
        "text": "A Gothic blackletter is fitting for the masthead of The New York Times. Comic Sans would not be."
      },
      {
        "type": "Elaboration",
        "text": "Conversely, having your average superhero talk with a Gothic blackletter in a comic book would also feel inappropriate."
      },
      {
        "type": "Implication",
        "text": "So why is Comic Sans 'appropriate' for certain situations? It most closely resembles informal handwriting, and thus conveys informality."
      },
      {
        "type": "Analogy",
        "text": "If humans didn't write in cursive and we never saw any font other than Comic Sans, we'd never know the difference!"
      },
      {
        "type": "Elaboration",
        "text": "But we do and we have, so such a connotation exists."
      },
      {
        "type": "Analogy",
        "text": "Without knowing anything about this guy, it seems like he'd be the kind of person who'd wear socks under his Crocs because it's comfortable and he doesn't care."
      },
      {
        "type": "Example",
        "text": "Crocs are indeed lightweight and comfortable, and protect you from the heat and irregularities of the road."
      },
      {
        "type": "Analogy",
        "text": "But if you're running for president or trying to get a job in the C-Suite of a Fortune 500 company, you put on dress shoes because that's what people in that setting do."
      },
      {
        "type": "Elaboration",
        "text": "A third reason will sound snobby, but I think would explain a lot about how designers tend to think about this sort of thing."
      },
      {
        "type": "Analogy",
        "text": "Imagine that you're a wine connoisseur and, everywhere you go, you see people not only buying boxed wine, but saying it's great wine and that they know something about wine because they found this box in the state store."
      },
      {
        "type": "Implication",
        "text": "The idea is that trained designers don't really like people using Publisher anyways, so they're more inclined to hate on designs that are done by amateurs who pick Comic Sans because 'it looks fun' or whatever."
      },
      {
        "type": "Elaboration",
        "text": "Then, once you've established that it's cool to pick on Comic Sans, everyone gets lumped into that group."
      }
    ]
  },
  {
    "site": "graphicdesign",
    "question_title": "What does the interlaced option in Photoshop do?",
    "question_body": "There is this options box shown when saving a PNG image from Adobe Photoshop. I always choose  'None'.\nWhat does the 'Interlaced' option do?\n\n",
    "answer_body": "Interlaced image loads an early degraded version of the whole image as soon as possible and then progressively renders the image to clear state. Interlaced will almost always be a bit bigger in filesize.\nNon-interlaced image will load up in tiles showing clear image in each tile as it progresses to load in the image. \n\n\n.gif format follows the same idea.\nIn .jpgformat \n\n\nprogressiveis the same as interlaced\nbaseline is the same as not interlaced\n\n\n\nGIF Simulation of loading an interlaced png and a non interlaced png using Firefox.\n\n\nClick to view full size \u00bb\n\nThe point of the simulation is to show how these two methods look visually when loading the image and not to compare their load times. Interlaced almost always adds a little to the filesize and therefore loads a little slower. There's also the perceived speed that is somewhat subjective. In this simulation, I used GPRS speeds (~7KB/s) and interlaced loaded 3 seconds later. Some people might say it looked like the Interlaced was faster. Some might say it's true, but it looked terrible when the image first started loading in. My personal preference is to not use interlacing.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "Interlaced image loads an early degraded version of the whole image as soon as possible and then progressively renders the image to clear state."
      },
      {
        "type": "Definition",
        "text": "Non-interlaced image will load up in tiles showing clear image in each tile as it progresses to load in the image."
      },
      {
        "type": "Comparison",
        "text": ".gif format follows the same idea. In .jpg format, progressive is the same as interlaced, and baseline is the same as not interlaced."
      },
      {
        "type": "Visual Description",
        "text": "GIF Simulation of loading an interlaced png and a non-interlaced png using Firefox."
      },
      {
        "type": "Elaboration",
        "text": "The point of the simulation is to show how these two methods look visually when loading the image and not to compare their load times."
      },
      {
        "type": "Claim",
        "text": "Interlaced almost always adds a little to the filesize and therefore loads a little slower."
      },
      {
        "type": "Qualification",
        "text": "There's also the perceived speed that is somewhat subjective."
      },
      {
        "type": "Example",
        "text": "In this simulation, I used GPRS speeds (~7KB/s) and interlaced loaded 3 seconds later."
      },
      {
        "type": "Implication",
        "text": "Some people might say it looked like the Interlaced was faster. Some might say it's true, but it looked terrible when the image first started loading in."
      },
      {
        "type": "Personal Preference",
        "text": "My personal preference is to not use interlacing."
      }
    ]
  },
  {
    "site": "graphicdesign",
    "question_title": "How do I stop Illustrator from snapping to pixel increments?",
    "question_body": "I am working in Illustrator (15.0.2 (CS5) or 16.0.0 (CS6)) and when I try to move objects small distances, they snap to the nearest two pixels on some invisible grid. I can't find anything in the preferences. Here are some screen shots:\n\n",
    "answer_body": "For new objects, turn off \"Align to Pixel Grid\" on the Transform panel.\n\nFor existing objects, select them first, then turn off Align to Pixel Grid using the checkbox at bottom of Transform panel.\n",
    "audience_level": "Beginner / Intermediate",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Procedure",
        "text": "For new objects, turn off 'Align to Pixel Grid' on the Transform panel."
      },
      {
        "type": "Procedure",
        "text": "For existing objects, select them first, then turn off Align to Pixel Grid using the checkbox at the bottom of the Transform panel."
      }
    ]
  },
  {
    "site": "cooking",
    "question_title": "What's the ideal coffee to water ratio for a French press?",
    "question_body": "What's the ideal amount of coffee to use per cup of water in a French press? \nI've looked at various online resources, which all gave me a different amount. Some call for 2 tablespoons per 6 oz of water, others per 4 oz. Others said to use one or the other, and add water after brewing if it's too strong.\nSo what's the ideal blend for extracting the best flavor?\n",
    "answer_body": "There's a whole, whole lot than can go into it, but a great starting place is a ratio of 17.21:1 water to coffee.  That is, if you have 500 g of water, you should use 29 g of ground coffee.  What the ratio really does is specify how concentrated the coffee should be.  Once you've set your ratio, there are many other variables to play with to make the coffee taste wonderful including grind size and consistency, bloom time, brew time, filter type, pour method, etc.\nI wish I could post hyperlinks to all the individual videos of different french press techniques, but you can find them all at the wonderful site http://brewmethods.com   Specifically, please check out:\n\nJames Hoffmann's French Press technique\nMark Prince of CoffeeGeek demonstrates the Press Pot\n\nLastly, and please tell me if this isn't kosher, but I've made an iPhone app that will handle the coffee and water weight conversions automatically, as well as timing the brew and detailing a couple guidelines for different brew methods.  It's available on the app store as Brew Control.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "A great starting place for making coffee is a ratio of 17.21:1 water to coffee, which specifies the concentration of the coffee."
      },
      {
        "type": "Example",
        "text": "For 500 g of water, you should use 29 g of ground coffee based on the 17.21:1 ratio."
      },
      {
        "type": "Elaboration",
        "text": "After setting the ratio, other variables like grind size, bloom time, brew time, filter type, and pour method can be adjusted to enhance the coffee's taste."
      },
      {
        "type": "Citation",
        "text": "For more information on French press techniques, videos can be found at http://brewmethods.com, including demonstrations by James Hoffmann and Mark Prince."
      },
      {
        "type": "Application",
        "text": "An iPhone app called Brew Control is available on the app store to automate coffee and water weight conversions, time the brew, and provide guidelines for different brew methods."
      }
    ]
  },
  {
    "site": "cooking",
    "question_title": "Why drain soaked rice?",
    "question_body": "When cooking black rice, I\u2019ve been told I need to soak it for a few hours, then drain and finally cook with 1:3 parts regular rice (i.e., topped up with fresh water).\nBlack rice is a type of glutinous rice, so I can understand the soaking stage. What I don\u2019t get is the need for draining; especially given black rice\u2019s nutritional content, which presumably partly ends up in that soaking water. Why is this done? Does it need to be done?\nMy best guess, if it does need to be done, is that it\u2019s for some hygiene reason.\n",
    "answer_body": "Apart from water, rice is mainly made from starch. Starch is initially packed in a crystalline structure that is not soluble. However if you soak it for long enough or expose it to heat, the starch slowly 'unpacks' and binds with water, resulting in a soluble compound. This is called starch gelatinization, and is what you are aiming for when you soak your rice in water (note it gains volume!). But it works in two ways: now soluble starch molecules detach from the rice and go into the water. This is why it gets cloudy. \nThe upshot is that this water is full of rice starch, and if you cook it will behave similar to when you add corn starch to water: thicken and form a glue. If you cook your rice in this water you should thus expect a much stickier result, with big lumps of rice 'glued' together. This is not always undesirable! Risotto is an example of cooked rice where we deliberately use this effect (so don't wash or soak your risotto rice). \n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Apart from water, rice is mainly made from starch."
      },
      {
        "type": "Definition",
        "text": "Starch is initially packed in a crystalline structure that is not soluble."
      },
      {
        "type": "Procedure",
        "text": "If you soak starch for long enough or expose it to heat, the starch slowly 'unpacks' and binds with water, resulting in a soluble compound. This process is called starch gelatinization."
      },
      {
        "type": "Implication",
        "text": "When you soak rice in water, you aim for starch gelatinization, which causes the rice to gain volume."
      },
      {
        "type": "Elaboration",
        "text": "Soluble starch molecules detach from the rice and go into the water, causing it to become cloudy."
      },
      {
        "type": "Implication",
        "text": "The water used to soak rice becomes full of rice starch, which can thicken and form a glue when cooked."
      },
      {
        "type": "Application",
        "text": "Cooking rice in this starchy water can result in a stickier texture, with the rice forming big lumps due to the starch 'gluing' them together."
      },
      {
        "type": "Qualification",
        "text": "While stickier rice may not always be desired, dishes like risotto intentionally utilize this effect, so it's recommended not to wash or soak risotto rice."
      }
    ]
  },
  {
    "site": "cooking",
    "question_title": "Why is some metal safe to use in a microwave, but others not?",
    "question_body": "This might be a more scientific question, but it relates to cooking and I thought it was interesting.\nI just made my lunch which was a microwavable bowl of chunky soup.\nThe directions said:\n\nRemove metal lid, remaining metal rim is microwavable. \n\nHow can this be?\n",
    "answer_body": "Metal on its own doesn't necessarily cause electric discharge in a microwave. \nWhat causes the sparking that you see when you put a fork in a microwave is due to the \"sharp\" edges of the fork. These edges concentrate the voltage at their tips which will cause a spark when it exceeds the dielectric breakdown of air.\nThings like sheet pans (with rounded edges), or rounded metal racks are used frequently in microwaves with no ill effect. The absence of any pointed edges allows this. The rim of your bowl fits this requirement.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Claim",
        "text": "Metal on its own doesn't necessarily cause electric discharge in a microwave."
      },
      {
        "type": "Explanation",
        "text": "What causes the sparking that you see when you put a fork in a microwave is due to the 'sharp' edges of the fork."
      },
      {
        "type": "Cause/Effect",
        "text": "These edges concentrate the voltage at their tips which will cause a spark when it exceeds the dielectric breakdown of air."
      },
      {
        "type": "Example",
        "text": "Things like sheet pans (with rounded edges), or rounded metal racks are used frequently in microwaves with no ill effect."
      },
      {
        "type": "Explanation",
        "text": "The absence of any pointed edges allows this."
      },
      {
        "type": "Comparison",
        "text": "The rim of your bowl fits this requirement."
      }
    ]
  },
  {
    "site": "cooking",
    "question_title": "Why can applesauce be used in place of oil?",
    "question_body": "In many recipes, mostly desserts, it is suggested that you can substitute oil with applesauce to reduce the fat content. But why applesauce?\nIs there something special about applesauce or are there other ingredients than can be used?\n",
    "answer_body": "Fundamentally, the reason for this substitution is that applesauce contains pectin.\nIn baking, the role of oil is to coat the flour, preventing it from combining with the water (or other wet ingredients) and developing gluten.  Gluten is what causes dough to rise, and also gives elasticity to the final product - what most people think of as \"chewiness.\"\nWhen you're baking, for example, a cake, or even a pie crust, you want to limit the amount of gluten that develops.  A cake or pie crust should be moist, light, and fluffy, not tough and chewy.  When you bite into a cake and find it very dry and bread-like, that is because it has developed a lot of gluten.  A good amount of oil or other fat leads to a lighter, moister, less-glutinous result.\nThe pectin in applesauce can also, to a certain extent, help to inhibit gluten formation in a dough, but the similarities end there.  I cannot stress this point enough, and I've seen many other online resources get this dead wrong: Pectin (applesauce) is not a straightforward or foolproof substitute for fat.\nThe mechanisms by which pectin and oil work in this context are completely different:\n\nOil is a lipid.  Lipids bind to starch (including the ~75% starch in flour) and are hydrophobic - the traditional example of this is dew drops forming on the surface of grass or plants (the latter being the hydrophobe).  In a sense, oil forms a protective \"shield\" around the flour molecules.\nPectin, on the other hand, is a gelling agent, and specifically a polysaccharide.  Pectin is not hydrophobic and does not actually protect the flour molecules.  In fact, pectin is of the same family as starches, which are also polysaccharides.  What's really happening here is that the pectin competes with the flour for water.  That means less water overall reaches the starch and gluten-forming proteins (giladin and glutenin) in the flour, and because of this, it is not able to develop as much gluten or gelatinize much of the other starch.\n\nWhat does all of this actually mean for you, the baker?  Quite simply, it means you have to be very careful with this substitution:\n\nToo much pectin can turn your recipe into a jelly-like consistency.\nToo little pectin will fail to prevent glutenization (in other words, you'll get bread).\nPectin has the property of syneresis - meaning that once it starts to gel, it also starts to expel liquid, and your dessert will dry out or deflate over time.\nPectin is actually water-soluble at high temperatures (technically, it forms a colloid), it just happens to absorb a lot of water along the way.  Baking for too long, or at too high a temperature, will cause the pectin to break down and dissolve completely, making it useless.\n\nThere are also several other problems (or at least \"gotchas\") when making the apple sauce substitution:\n\nApplesauce is not just pectin.  It has a good deal of water and various proteins and acids, and even a certain amount of lipids.  The exact quantities, however, depend on how the applesauce was made, so it is very hard to get precise control over the amount of pectin, and the textbook 1:1 substitution ratio is almost never correct.\nAnother thing that applesauce contains is sugar - even unsweetened applesauce.  You will almost certainly need to reduce the amount of sugar elsewhere in your recipe.  This may be difficult, especially if the bulk of the sugar is used as a dry ingredient.\nApplesauce behaves somewhat similarly to oil, but do not even try using it as a substitute for any other fat.  Butter, in particular, contains milk proteins which act as natural emulsifiers; pectin does have certain stabilizing properties but is a rather poor emulsifier compared to butter[citation needed].  And it goes without saying that the flavour is substantially different from that of applesauce; oil is \"OK\" to substitute for because it has very little flavour of its own.\n\nIf you plan on using applesauce as a substitute for oil in a recipe that does not specifically explain how to use it as a substitute, then I strongly suggest you do two things:\n\nDon't substitute the entire quantity.  Use 1/2 oil, 1/2 applesauce, or maybe 1/4 oil and 3/4 applesauce.  You will likely also have to lower the total quantity of oil/applesauce to approximately half of what it originally was (give or take 1/4).\nIf you can, try this substitution on a small scale first, and experiment with the quantities of oil, sugar, and applesauce, before going all-out and putting it into the oven while your guests arrive.  It's very likely that on the first few attempts, you'll end up with something that's palatable, but nowhere near the quality of the oil-based recipe.\n\nIgnore these disclaimers at your own risk!\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "Fundamentally, the reason for this substitution is that applesauce contains pectin."
      },
      {
        "type": "Explanation",
        "text": "In baking, the role of oil is to coat the flour, preventing it from combining with the water (or other wet ingredients) and developing gluten. Gluten is what causes dough to rise, and also gives elasticity to the final product - what most people think of as 'chewiness.'"
      },
      {
        "type": "Example",
        "text": "When you're baking, for example, a cake, or even a pie crust, you want to limit the amount of gluten that develops."
      },
      {
        "type": "Claim",
        "text": "A good amount of oil or other fat leads to a lighter, moister, less-glutinous result."
      },
      {
        "type": "Comparison",
        "text": "The mechanisms by which pectin and oil work in this context are completely different."
      },
      {
        "type": "Elaboration",
        "text": "Oil is a lipid. Lipids bind to starch and are hydrophobic, forming a protective 'shield' around the flour molecules. Pectin, on the other hand, is a gelling agent and competes with flour for water."
      },
      {
        "type": "Implication",
        "text": "Too much pectin can turn your recipe into a jelly-like consistency, while too little pectin will fail to prevent glutenization."
      },
      {
        "type": "Caveat",
        "text": "Pectin has the property of syneresis, expelling liquid once it starts to gel, which can lead to drying out or deflating of the dessert over time."
      },
      {
        "type": "Qualification",
        "text": "Applesauce is not just pectin; it contains water, proteins, acids, and even lipids. The exact quantities vary, making precise control challenging."
      },
      {
        "type": "Procedure",
        "text": "If using applesauce as a substitute for oil, it's recommended to experiment with different ratios and quantities before committing to a full recipe."
      },
      {
        "type": "Emphasis",
        "text": "Ignore these disclaimers at your own risk!"
      }
    ]
  },
  {
    "site": "cooking",
    "question_title": "Should I rinse canned beans before using them?",
    "question_body": "Certainly, if I were making a salad with canned beans, I would thoroughly rinse them first. But if I'm making soup or chili with canned black beans or kidney beans, should I drain and rinse them first? Goya brand beans have recipes on the side of the can that call for undrained beans. \nOn the one hand, I've heard claims that using the liquid in the can will increase gassiness, and that in some brands it can contain a lot of sodium. But I've also heard that it contains lots of soluble fiber that is lost if drained. Is there merit to either of these claims? Are there other nutrients that get lost if I drain and rinse? I always feel bad throwing out anything edible.\n",
    "answer_body": "I finally found what I'm looking for, from the University of Michigan \u2014 some actual data on the subject!\nThey say that rinsing canned beans can reduce the amount of sodium by almost half.  They add that it also reduces the amount of gas-causing raffinose.\nDraining the fluid is likely to improve the flavor and texture of the resulting food by concentrating the flavor of the beans. An exception would be if you're following a soup recipe that specifically suggests retaining the liquid to thicken the soup.\nI still haven't found any information about what healthy nutrients might be lost by rinsing the beans. But the general consensus seems to be that rinsing will do more good than harm in almost all cases.\n",
    "audience_level": "Intermediate",
    "tone": "Casual",
    "blocks": [
      {
        "type": "Context",
        "text": "I finally found what I'm looking for, from the University of Michigan \u2014 some actual data on the subject!"
      },
      {
        "type": "Claim",
        "text": "They say that rinsing canned beans can reduce the amount of sodium by almost half. They add that it also reduces the amount of gas-causing raffinose."
      },
      {
        "type": "Implication",
        "text": "Draining the fluid is likely to improve the flavor and texture of the resulting food by concentrating the flavor of the beans."
      },
      {
        "type": "Exception",
        "text": "An exception would be if you're following a soup recipe that specifically suggests retaining the liquid to thicken the soup."
      },
      {
        "type": "Qualification",
        "text": "I still haven't found any information about what healthy nutrients might be lost by rinsing the beans. But the general consensus seems to be that rinsing will do more good than harm in almost all cases."
      }
    ]
  },
  {
    "site": "psychology",
    "question_title": "What is an effective metric of complexity for an Artificial Neural Network?",
    "question_body": "After asking the question What is the most complex neural network... I realized I don't really have a good metric of \"complexity\" in a general sense. The simplest measure would likely be count of neurons or number of synapses, but that fails to take into account the structure of the network.\nA couple measures of complexity are discussed in the paper Complexity of Predictive Neural Networks\n but they are very specific to a single task. One is the amount of work needed to learn a certain thing, and the other is how many neurons are needed to approximate a certain function.\nRough, animal based measures are often employees for the sake of grabbing headlines; such as the incorrect claims that The Blue Brain Project had emulated a neural network \"as complex as\" a cat's brain. C. elegans is a common and seemingly attainable level of complexity for an artificial neural network. \nAnimal based measures are relateable to the layman but seem questionable, especially when comparing a neural network to that of an animal who's neural network has not been completely mapped (as C. elegans has).\nWhat is a meaningful measure by which artificial neural networks can be measured? How are such networks currently compared? Can any such metric appropriately measure complexity of such a system?\n",
    "answer_body": "The standard complexity metric in theoretical computer science and machine learning, in particular in statistical learning theory, is the Vapnik\u2013Chervonenkis (VC) dimension. It is of interest because it gives us a very good tool to measure the learning ability of a neural network (or any other statistical learner, in general).\nA good introduction to the use of VC dimension for studying neural nets is:\nEduardo D. Sontag [1998] \"VC dimension of neural networks\" [pdf].\nThere, the author shows (for instance) that a network with one hidden layer, $n$ inputs, and $\\tanh$ neurons has VC dimension of $n + 1$. He also explain some basic technique for how to upper-bound the VC dimension, and for how to use it for dynamic neural nets.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "The standard complexity metric in theoretical computer science and machine learning, in particular in statistical learning theory, is the Vapnik\u2013Chervonenkis (VC) dimension."
      },
      {
        "type": "Implication",
        "text": "It is of interest because it gives us a very good tool to measure the learning ability of a neural network (or any other statistical learner, in general)."
      },
      {
        "type": "Citation",
        "text": "A good introduction to the use of VC dimension for studying neural nets is: Eduardo D. Sontag [1998] 'VC dimension of neural networks' [pdf]."
      },
      {
        "type": "Example",
        "text": "There, the author shows (for instance) that a network with one hidden layer, $n$ inputs, and $\\tanh$ neurons has VC dimension of $n + 1."
      },
      {
        "type": "Elaboration",
        "text": "He also explains some basic techniques for how to upper-bound the VC dimension, and for how to use it for dynamic neural nets."
      }
    ]
  },
  {
    "site": "psychology",
    "question_title": "If babies were isolated, would they develop their own language?",
    "question_body": "Let\u2019s say that two or more babies/infants from the age of 0 were put in an environment without the affection of adults to teach them how to speak. Would the babies after 13 or so years develop their own language? \nLet's assume that someone or something would take care of the babies, giving them food and water and so on, but never speak to them.\n",
    "answer_body": "This question would require an experiment that cannot ethically be conducted, but it is interesting.\nWikipedia has an article on historical attempts at language deprivation experiments:\n\nAn experiment allegedly carried out by Holy Roman Emperor Frederick II\n  in the 13th century saw young infants raised without human interaction\n  in an attempt to determine if there was a natural language ...\n  \"foster-mothers and nurses to suckle and bathe and wash the children,\n  but in no ways to prattle or speak with them ... But he laboured in\n  vain, for the children could not live without clappings of the hands,\n  and gestures, and gladness of countenance, and blandishments.\" ... \n  James IV of Scotland was said to have sent two children to be raised\n  by a mute woman isolated on the island of Inchkeith, to determine if\n  language was learned or innate. The children were reported to have\n  spoken good Hebrew ...\n\nWhile these experiments had the advantage of a lax ethical environment, they also had a major disadvantage of poor methodology, and the accuracy of their documentation is very questionable.\nThere are quite a few documented cases of feral children, but none that I'm aware of who were raised together so as to have had an opportunity to develop language.\nThe most compelling case for the innateness of language development that I'm aware of is the Nicaraguan sign language (ISN):\n\n... a sign language that was largely spontaneously developed by deaf\n  children in a number of schools in western Nicaragua in the 1970s and\n  1980s. It is of particular interest to the linguists who study it,\n  because it offers a unique opportunity to study what they believe to\n  be the birth of a new language.\n\nThese children were in fact raised as normal children, but were language isolated, with no contact with other deaf children until school age.  Some variables not well controlled for here include that they were exposed to the concept of language, were aware that other people communicated with each other, and already had a proto-language (mimicas).  Nonetheless, they appeared to develop a proper structured language fairly quickly upon being introduced to each other, without any apparent support or encouragement from adults, who were entirely unfamiliar with any sign language.\nThere are similar examples of isolated sign-language development, such as ABSL and MVSL, but the problem with all of them is that the actual level of language isolation is not well controlled for, and hence not clear.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": []
  },
  {
    "site": "psychology",
    "question_title": "What is the current \"accepted\" science behind dream interpretation?",
    "question_body": "I'm doing some casual reading about dream interpretation (meaning I'm reading the wikipedia entry) and the article mentions that there are several ways of thinking about dreams from a psychological standpoint:\n\nIn modern times, various schools of psychology have offered theories about the meaning of dreams.\n\nBut the text offers no judgement of which of these approaches is currently believed to be most \"accurate\". It simply lists various scholars and their theories.\nWhat I want to know is: which approach is considered the most \"correct\" by the scientific community and why? Is there a reason to take one of the listed methods of interpretation more seriously than another as a casual layman?\nNote that I'm not asking for your subjective answer. I'm looking for any evidence as to whether a specific method is more successful or more pursued by the profession and community in general.\n",
    "answer_body": "As far as I know, there is no accepted science to dream interpretation. In fact, there's no science to it at all. Evidence has shown that indeed, dreaming draws material from people, places, and things in our lives, but there's absolutely no scientific data out there (that I'm familiar with) that links dreams to anything meaningful in our actual daily lives.\nIf anything, dreaming and sleep is an opportunity to \"replay\" patterns of neural activation that occurred during the day, leading to consolidation of long-term memories. I wouldn't take anything on that Wikipedia page seriously.\nOther arguments for why we sleep and dream:\n(1) We may sleep to repair our brain or maintain homeostasis within the system. This would suggest that there's a substance in our body which requires sleep that increases its need over time. This possibility is evidenced by the fact that you can die of sleep deprivation -- but evidence against is that some amount of deprivation doesn't set you back all too far. (Also, why does sleep feel restorative, if it's only maintaining homeostasis?) There's increasing amount of evidence to suggest that improved circulation of cerebrospinal fluid in the brain during sleep serves to clean the brain from toxic metabolic byproducts of daily activity.\n(2) It's also possible that sleep and dreaming serves the body's overall safety. This argument stems from value long-ago in sleeping during the dangerous nighttime, when predators and other dangerous animals generally have the upper hand over man. This seems reasonably vestigial in humans, though...\n(3) It also may be the case that sleep and dreaming is effective for conserving our neurak energy over the course of the day. Indeed, slow-wave sleep is associated with reduced cortical activity -- but then again, REM sleep features high activity.\n(4) An even more powerful possibility is that neural reprogramming is dependent on sleep. This is evidenced by consolidation effects that occur while we rest, and the fact that less interference occurs during the sleeping hours (see Jenkins & Dallenbach, 1924, if you want to read a real classic). There are many minimal sleepers out there, though, that are high functioning, so what does that mean for this theory?\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "As far as I know, there is no accepted science to dream interpretation. In fact, there's no science to it at all."
      },
      {
        "type": "Evidence",
        "text": "Evidence has shown that indeed, dreaming draws material from people, places, and things in our lives, but there's absolutely no scientific data out there (that I'm familiar with) that links dreams to anything meaningful in our actual daily lives."
      },
      {
        "type": "Claim",
        "text": "If anything, dreaming and sleep is an opportunity to 'replay' patterns of neural activation that occurred during the day, leading to consolidation of long-term memories."
      },
      {
        "type": "Caveat",
        "text": "I wouldn't take anything on that Wikipedia page seriously."
      },
      {
        "type": "Claim",
        "text": "Other arguments for why we sleep and dream:"
      },
      {
        "type": "Procedure",
        "text": "(1) We may sleep to repair our brain or maintain homeostasis within the system."
      },
      {
        "type": "Evidence",
        "text": "This possibility is evidenced by the fact that you can die of sleep deprivation -- but evidence against is that some amount of deprivation doesn't set you back all too far."
      },
      {
        "type": "Question",
        "text": "Also, why does sleep feel restorative, if it's only maintaining homeostasis?"
      },
      {
        "type": "Evidence",
        "text": "There's increasing amount of evidence to suggest that improved circulation of cerebrospinal fluid in the brain during sleep serves to clean the brain from toxic metabolic byproducts of daily activity."
      },
      {
        "type": "Procedure",
        "text": "(2) It's also possible that sleep and dreaming serves the body's overall safety."
      },
      {
        "type": "Analogy",
        "text": "This argument stems from value long-ago in sleeping during the dangerous nighttime, when predators and other dangerous animals generally have the upper hand over man."
      },
      {
        "type": "Qualification",
        "text": "This seems reasonably vestigial in humans, though..."
      },
      {
        "type": "Procedure",
        "text": "(3) It also may be the case that sleep and dreaming is effective for conserving our neural energy over the course of the day."
      },
      {
        "type": "Evidence",
        "text": "Indeed, slow-wave sleep is associated with reduced cortical activity -- but then again, REM sleep features high activity."
      },
      {
        "type": "Procedure",
        "text": "(4) An even more powerful possibility is that neural reprogramming is dependent on sleep."
      },
      {
        "type": "Evidence",
        "text": "This is evidenced by consolidation effects that occur while we rest, and the fact that less interference occurs during the sleeping hours (see Jenkins & Dallenbach, 1924, if you want to read a real classic)."
      },
      {
        "type": "Counterexample",
        "text": "There are many minimal sleepers out there, though, that are high functioning, so what does that mean for this theory?"
      }
    ]
  },
  {
    "site": "psychology",
    "question_title": "Open source software for running Internet psychological experiments that collect reaction time data",
    "question_body": "I've often used Inquisit to run psychological experiments online. The software enables delivery of stimuli (e.g., text, images, etc.) and collection of reaction times. \nObviously general purpose programming languages provide one avenue for delivery of online experiments. There's also a good listing of software for psychological experiments here. I've also seen PsyToolkit which is GNU licensed software for programming psychological experiments, but I'm not aware of any online option.\nAre there any open source options for online delivery of psychological experiments, particularly ones that enable reaction time measurement?\n",
    "answer_body": "For an open source JavaScript/HTML/CSS solution, check out jsPsych. It  can be used for reaction time measurement and interactive designs. An article describing the library was recently published in Behavior Research Methods.\nA subsequent article investigated the properties of reaction time distributions collected with JavaScript compared to those collected with MATLAB and Psychtoolbox. The main result is that JavaScript was 10-40ms slower, but had equivalent variance across different experimental conditions and equal sensitivity to the experimental manipulation of set size in a visual search task.\nThere are several good answers to this related question about the validity and accuracy of response time measurements online. Many of those answers discuss findings that are relevant to JavaScript libraries like jsPsych.\nde Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. Behavior Research Methods, 47(1), 1-12.\nde Leeuw, J. R., & Motz, B. A. (2015). Psychophysics in a Web browser? Comparing response times collected with JavaScript and Psychophysics Toolbox in a visual search task. Behavior Research Methods, advance online publication.\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Context",
        "text": "For an open source JavaScript/HTML/CSS solution, check out jsPsych. It can be used for reaction time measurement and interactive designs."
      },
      {
        "type": "Citation",
        "text": "An article describing the library was recently published in Behavior Research Methods."
      },
      {
        "type": "Summary",
        "text": "A subsequent article investigated the properties of reaction time distributions collected with JavaScript compared to those collected with MATLAB and Psychtoolbox."
      },
      {
        "type": "Claim",
        "text": "The main result is that JavaScript was 10-40ms slower, but had equivalent variance across different experimental conditions and equal sensitivity to the experimental manipulation of set size in a visual search task."
      },
      {
        "type": "Evidence",
        "text": "There are several good answers to this related question about the validity and accuracy of response time measurements online. Many of those answers discuss findings that are relevant to JavaScript libraries like jsPsych."
      },
      {
        "type": "Citation",
        "text": "de Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a Web browser. Behavior Research Methods, 47(1), 1-12."
      },
      {
        "type": "Citation",
        "text": "de Leeuw, J. R., & Motz, B. A. (2015). Psychophysics in a Web browser? Comparing response times collected with JavaScript and Psychophysics Toolbox in a visual search task. Behavior Research Methods, advance online publication."
      }
    ]
  },
  {
    "site": "psychology",
    "question_title": "Is there evidence that listening to music can aid/hinder concentration or performance?",
    "question_body": "I, like many computer programmers, love to listen to music while I work. I have always believed that music helps me stay focused and motivated, and improves my performance on many types of tasks, espescially \"busywork\". However my company's CEO disagrees with me, and believes that music is a distraction and leads to reduced productivity. Have any studies been done on whether listening to music while performing a task improves or hinders one's ability to perform that task? Is there a consensus in the cognitive science community regarding this?\n",
    "answer_body": "It basically depends on how the particular musical performance is perceived by the listener. Cognitive process of listening seems to be comprise several layers, which follows a bottom-up direction.\nFirst step is to decode relevant signal(s), among a complex package of sound. This is where the irrelevant noise is eliminated. Can music be eliminated in this level? Highly unlikely, but still possible. I do not know of a particular experiment but when the music is being played in a far destination, or with a low volume, or if the participant is highly concentrated to the task; then it may be eliminated in this step. But the key point in this step is that the term \"noise\" refers to aperiodic background sounds. Therefore, my first impression is that music, being periodical, must decrease the task performance. Cutler and Clifton (1999) \ngives an overview on the entire listening process. Second step is the grouping of different sound sources. There is also modelling studies that aims to explain this phenomenon (Bregman, 1990). Steps in listening continues further, but those steps are beyond the scope of this question.\nBut there are other studies also. Ylias and Heaven (2003) showed that the background noise negatively effects reading comprehension. So far so good. Cassidy and MacDonald (2007) showed that task performance on silence is greater than in low arousal music, and that is greater than noise, and that is even greater than high arousal music conditions. This is interesting, because it now introduces the affective state of the listener into the equation, which makes it a lot more difficult to handle. Another result is that the effect of the noise here is comparable to the effect of background music. But we have to note that the details of the noise in this experiment is not given in detail, only commented as \"the everyday noise\". It would be more conclusive if we just know whether it is the background sound of a television (periodic) or a traffic noise (aperiodic).\nCombining these references, I cannot easily conclude that music is taken as a \"noise\". It seems that music reduces the task performance, by negatively affecting a later step in the listening process.\nEnding note: There are several semiformal-informal studies on the web also. They study directly the \"work/office performance\", therefore I must say that they lack a little bit of a controlled environment. In such environments we can even confidently say that music improves our performance in particular situations. But what we miss is that office environments comprise several unhandled parameters that makes it hard for scientific experimental setup (i.e. listening music may improve the performance if your office mates chit chat next to you).\nEnding note 2: I was interested in this topic a time ago. So I welcome more recent references or comments.\n\nCutler, A., & Clifton, C. (1999). Comprehending spoken language: A blueprint of the listener. The neurocognition of language, 123-166.\nBregman, A. S. (1984, July). Auditory scene analysis. In Proceedings of the 7th International Conference on Pattern Recognition (pp. 168-175).\nYlias, G., & Heaven, P. C. (2003). The influence of distraction on reading comprehension: a Big Five analysis. Personality and individual differences, 34(6), 1069-1079.\nCassidy, G., & MacDonald, R. A. (2007). The effect of background music and background noise on the task performance of introverts and extraverts. Psychology of Music, 35(3), 517-537.\n\n",
    "audience_level": "Expert",
    "tone": "Technical",
    "blocks": [
      {
        "type": "Definition",
        "text": "It basically depends on how the particular musical performance is perceived by the listener. Cognitive process of listening seems to be comprise several layers, which follows a bottom-up direction."
      },
      {
        "type": "Procedure",
        "text": "First step is to decode relevant signal(s), among a complex package of sound. This is where the irrelevant noise is eliminated."
      },
      {
        "type": "Claim",
        "text": "The term 'noise' refers to aperiodic background sounds. Therefore, music, being periodical, may decrease task performance."
      },
      {
        "type": "Citation",
        "text": "Cutler and Clifton (1999) gives an overview on the entire listening process."
      },
      {
        "type": "Citation",
        "text": "Bregman (1990) conducted modelling studies to explain the grouping of different sound sources."
      },
      {
        "type": "Claim",
        "text": "Ylias and Heaven (2003) showed that background noise negatively affects reading comprehension."
      },
      {
        "type": "Claim",
        "text": "Cassidy and MacDonald (2007) found that task performance on silence is greater than in low arousal music, which is greater than noise, and that is even greater than high arousal music conditions."
      },
      {
        "type": "Implication",
        "text": "The affective state of the listener plays a significant role in the listening process, making it more complex to analyze."
      },
      {
        "type": "Qualification",
        "text": "The details of the noise in experiments, such as whether it is periodic or aperiodic, are crucial for drawing conclusive results."
      },
      {
        "type": "Elaboration",
        "text": "Music may reduce task performance by negatively affecting a later step in the listening process."
      },
      {
        "type": "Context",
        "text": "There are several semiformal-informal studies on the web studying 'work/office performance', but they may lack a controlled environment."
      },
      {
        "type": "Qualification",
        "text": "Office environments have unhandled parameters that can impact the effects of music on performance."
      },
      {
        "type": "Summary",
        "text": "The impact of music on task performance during the listening process is influenced by various factors, including the affective state of the listener and the nature of the background noise."
      },
      {
        "type": "Transition",
        "text": "Ending note: There are several semiformal-informal studies on the web also."
      },
      {
        "type": "Transition",
        "text": "Ending note 2: I was interested in this topic a time ago. So I welcome more recent references or comments."
      }
    ]
  }
]